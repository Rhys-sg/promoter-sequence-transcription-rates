{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "For the best results, one should optimize the LSTM with the following hyperparameters: the neurons, layers, learning rate, batch size, and epochs. However, even randomized-search hyperparameter tuning is too taxing and time-intensive for my computer. In the future, I will perform hyperparameter tuning to better optimize the model. To reach robust performance results with non-parametric models, their respective hyperparameters must be optimized. Default hyperparameter settings cannot guarantee an optimal performance of machine-learning techniques, and additional attention should be directed to this critical step (Schratz et al., 2019). To do this, one should design their model to objectively search different values for model hyperparameters and choose a subset that results in a model that achieves the best performance on a given dataset (Brownlee 2020). Two common implementations for hyperparameter tuning are \"randomized search\" and \"grid search.\" While grid search finds the more optimal hyperparameter, randomized search is far less taxing regarding time and processing power (Brownlee 2020). For LSTM, there are several hyperparameters to optimize.\n",
    "\n",
    "**Number of LSTM Neurons:** Increasing the number of neurons per hidden layer will increase the complexity of the model. This will, in turn, improve accuracy, so long as it does not over-fit the data.\n",
    "\n",
    "**Number of Layers:** Increasing the hidden layers will similarly increase how \"deep\" the model is. Past a few hidden layers, the model will become \"blackboxed,\" and one will not be able to conclude meaningful relationships from it. However, this will again increase accuracy so long as it does not overfit the data.\n",
    "\n",
    "**Learning Rate (alpha):** The learning rate is a tuning parameter in the gradient descent optimization algorithm. It determines the step size at each iteration while moving toward a minimum loss function. Increasing the learning rate will increase the model's training speed and convergence, but if it is too high, the algorithm will overstep and not converge.\n",
    "\n",
    "**Batch Size:** Batches are the samples used during each epoch to update the model's weights. Smaller batch sizes can lead to faster convergence but might be less stable. Larger batch sizes may provide more stable updates but require more memory. Experiment with different batch sizes to find the optimal balance.\n",
    "\n",
    "**Number of Epochs (Not Tuned):** The number of epochs determines how many times the model is trained with one forward and one backward propagation. Too few epochs lead to underfitting; likewise, too many lead to overfitting. These models are trained with the number of epochs only set as the upper bound. Ideally, the model stops itself when it reaches a minimum MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dict['upstream_padding'], y, test_size=0.2, random_state=1, shuffle=True)\n",
    "train_test['upstream_padding_tuned'] = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the random search parameters\n",
    "param_distributions = {\n",
    "    'neurons': [32, 64, 128],\n",
    "    'layers': [1, 2, 3],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100, 150]\n",
    "}\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lowest_loss = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get('loss')\n",
    "        if current_loss < self.lowest_loss:\n",
    "            self.lowest_loss = current_loss\n",
    "        print(f\"Epoch {epoch+1}: Current Loss = {current_loss}, Lowest Loss = {self.lowest_loss}\")\n",
    "\n",
    "# Function to create model\n",
    "def create_model(neurons=64, layers=1, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    for _ in range(layers):\n",
    "        model.add(LSTM(neurons, input_shape=X_dict['upstream_padding_tuned'].shape[1:], return_sequences=True if layers > 1 else False))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define EarlyStopping and LossHistory\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "loss_history = LossHistory()\n",
    "\n",
    "# Create KerasRegressor for RandomizedSearchCV\n",
    "models['upstream_padding_tuned'] = KerasRegressor(layers=1, learning_rate=0.001, neurons=32, build_fn=create_model, epochs=epochs, batch_size=32, verbose=0)\n",
    "\n",
    "# Randomized search\n",
    "grid = RandomizedSearchCV(estimator=models['upstream_padding_tuned'], param_distributions=param_distributions, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(train_test['upstream_padding_tuned']['X_train'], train_test['upstream_padding_tuned']['y_train'], callbacks=[early_stopping, loss_history])\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
