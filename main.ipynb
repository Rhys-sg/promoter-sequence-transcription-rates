{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison \n",
    "\n",
    "The primary model I am using is the Long Short-Term Memory (LSTM) network because the input data (DNA sequences) are sequential. LSTMs are a recurrent neural network (RNN), which have improved the base model by remidying the \"vanishing gradient problem\" present in traditional RNNs (Hu et al., 2018). It aims to provide a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\".\n",
    "\n",
    "In this research, I will compare the LSTM models to two others: a Convolutional Neural Network (CNN) and LaFleur et al.'s Multiple Linear Regression (MLR). CNNs are not as well suited for this dataset compared to a LSTM because CNNs are primarily used for grid-structured data like images. Although they can also be applied to sequential data by treating the data as a one-dimensional grid (e.g., for text classification tasks), CNNs do not inherently capture sequential dependencies as effectively as LSTMs (O'Shea and Nash, 2015). Lastly, I will also compare the models to LaFleur et al.'s MLR, which encodes the presence of each possible 3 nucleotide sequences within each hexamer (12).\n",
    "\n",
    "I was was initially going to compare my LSTM to a MLR that one-hot encodes each of the six inputs as a different classification. However, this approach treats every unique sequence in each column as a separate categorical input. With 5 rows and 11942 columns, fitting a MLR model to this data would take an unrealistic amount of time and be very inaccurate. For this code, see \"MLR.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DNA1 [counts]</th>\n",
       "      <th>DNA2 [counts]</th>\n",
       "      <th>DNA3 [counts]</th>\n",
       "      <th>RNA1 [counts]</th>\n",
       "      <th>RNA2 [counts]</th>\n",
       "      <th>RNA3 [counts]</th>\n",
       "      <th>TX1 [au]</th>\n",
       "      <th>TX2 [au]</th>\n",
       "      <th>TX3 [au]</th>\n",
       "      <th>...</th>\n",
       "      <th>high quality</th>\n",
       "      <th>Observed log(TX/Txref)</th>\n",
       "      <th>Predicted log(TX/Txref)</th>\n",
       "      <th>dG10</th>\n",
       "      <th>dG35</th>\n",
       "      <th>dGDisc</th>\n",
       "      <th>dGITR</th>\n",
       "      <th>dGEXT10</th>\n",
       "      <th>dGSPAC</th>\n",
       "      <th>dGUP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8263</td>\n",
       "      <td>7261</td>\n",
       "      <td>5173</td>\n",
       "      <td>16341</td>\n",
       "      <td>10320</td>\n",
       "      <td>13506</td>\n",
       "      <td>2.258071</td>\n",
       "      <td>1.523795</td>\n",
       "      <td>1.545541</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.386326</td>\n",
       "      <td>-3.844827</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.106428</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5600</td>\n",
       "      <td>4886</td>\n",
       "      <td>3264</td>\n",
       "      <td>10986</td>\n",
       "      <td>7250</td>\n",
       "      <td>10800</td>\n",
       "      <td>2.240001</td>\n",
       "      <td>1.590845</td>\n",
       "      <td>1.958709</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.503140</td>\n",
       "      <td>-3.905283</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.166884</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7898</td>\n",
       "      <td>6790</td>\n",
       "      <td>4752</td>\n",
       "      <td>19572</td>\n",
       "      <td>32204</td>\n",
       "      <td>30585</td>\n",
       "      <td>2.829533</td>\n",
       "      <td>5.084911</td>\n",
       "      <td>3.810029</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-4.207206</td>\n",
       "      <td>-3.905283</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.166884</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10651</td>\n",
       "      <td>9875</td>\n",
       "      <td>6466</td>\n",
       "      <td>15734</td>\n",
       "      <td>16246</td>\n",
       "      <td>18908</td>\n",
       "      <td>1.686729</td>\n",
       "      <td>1.763814</td>\n",
       "      <td>1.731036</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.392439</td>\n",
       "      <td>-3.877808</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.139409</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12188</td>\n",
       "      <td>10793</td>\n",
       "      <td>6965</td>\n",
       "      <td>28609</td>\n",
       "      <td>21796</td>\n",
       "      <td>26803</td>\n",
       "      <td>2.680198</td>\n",
       "      <td>2.165100</td>\n",
       "      <td>2.278025</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.698903</td>\n",
       "      <td>-3.672384</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>0.066015</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  DNA1 [counts]  DNA2 [counts]  DNA3 [counts]  RNA1 [counts]  \\\n",
       "0   0           8263           7261           5173          16341   \n",
       "1   1           5600           4886           3264          10986   \n",
       "2   2           7898           6790           4752          19572   \n",
       "3   3          10651           9875           6466          15734   \n",
       "4   4          12188          10793           6965          28609   \n",
       "\n",
       "   RNA2 [counts]  RNA3 [counts]  TX1 [au]  TX2 [au]  TX3 [au]  ...  \\\n",
       "0          10320          13506  2.258071  1.523795  1.545541  ...   \n",
       "1           7250          10800  2.240001  1.590845  1.958709  ...   \n",
       "2          32204          30585  2.829533  5.084911  3.810029  ...   \n",
       "3          16246          18908  1.686729  1.763814  1.731036  ...   \n",
       "4          21796          26803  2.680198  2.165100  2.278025  ...   \n",
       "\n",
       "   high quality  Observed log(TX/Txref) Predicted log(TX/Txref)      dG10  \\\n",
       "0           Yes               -3.386326               -3.844827 -1.781524   \n",
       "1           Yes               -3.503140               -3.905283 -1.781524   \n",
       "2           Yes               -4.207206               -3.905283 -1.781524   \n",
       "3           Yes               -3.392439               -3.877808 -1.781524   \n",
       "4           Yes               -3.698903               -3.672384 -1.781524   \n",
       "\n",
       "       dG35    dGDisc     dGITR   dGEXT10  dGSPAC      dGUP  \n",
       "0 -1.477218 -0.106428 -0.021112  0.191352 -0.0924  0.400862  \n",
       "1 -1.477218 -0.166884 -0.021112  0.191352 -0.0924  0.400862  \n",
       "2 -1.477218 -0.166884 -0.021112  0.191352 -0.0924  0.400862  \n",
       "3 -1.477218 -0.139409 -0.021112  0.191352 -0.0924  0.400862  \n",
       "4 -1.477218  0.066015 -0.021112  0.191352 -0.0924  0.400862  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset into a pandas data frame\n",
    "\n",
    "df = pd.read_csv('41467_2022_32829_MOESM5_ESM.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UP</th>\n",
       "      <th>h35</th>\n",
       "      <th>spacs</th>\n",
       "      <th>h10</th>\n",
       "      <th>disc</th>\n",
       "      <th>ITR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>CCCCGCGG</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>CGCGGCGG</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>CGCGCCCG</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>GCGGCGGC</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>CGGGGGGC</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 UP     h35              spacs     h10      disc  \\\n",
       "0  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  CCCCGCGG   \n",
       "1  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  CGCGGCGG   \n",
       "2  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  CGCGCCCG   \n",
       "3  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  GCGGCGGC   \n",
       "4  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  CGGGGGGC   \n",
       "\n",
       "                     ITR  \n",
       "0  CTCTACCTTAGTTTGTACGTT  \n",
       "1  CTCTACCTTAGTTTGTACGTT  \n",
       "2  CTCTACCTTAGTTTGTACGTT  \n",
       "3  CTCTACCTTAGTTTGTACGTT  \n",
       "4  CTCTACCTTAGTTTGTACGTT  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All input and output data\n",
    "\n",
    "X = df[['UP', 'h35', 'spacs', 'h10', 'disc', 'ITR']]\n",
    "y = df['Observed log(TX/Txref)']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one-hot encode DNA sequences\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    mapping = {'A': [1,0,0,0], 'C': [0,1,0,0], 'G': [0,0,1,0], 'T': [0,0,0,1]}\n",
    "    encoding = []\n",
    "    for nucleotide in sequence:\n",
    "         encoding += [mapping[nucleotide]]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to Handling Variable-Length Inputs\n",
    "There are six input sequences:\n",
    "1. upstream 6-nucleotide site called the −35 motif\n",
    "2. downstream 6-nucleotide site called the −10 motif\n",
    "3. the 20-nucleotide region that appears upstream of the −35 motif, called the UP element\n",
    "4. the spacer region that separates the −10 and −35 motifs\n",
    "5. the typically 6-nucleotide region in between the −10 motif and TSS, called the discriminator (Disc)\n",
    "6. the first 20 transcribed nucleotides, called the initial transcribed region (ITR)23\n",
    "\n",
    "The upstream −35 motif and the downstream −10 motif are both 6 base pairs long. Every other sequence varies in length. There are no simple ways to train convolutional neural networks on multiple varying-length inputs. Here, I try two approaches to optimize MSE:\n",
    "1. Only train the model on the upstream −35 motif and the downstream −10 motif.\n",
    "2. Apply padding to some or all of the variable inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the various input approaches\n",
    "X_dict = {}\n",
    "\n",
    "# stores split training/testing\n",
    "train_test = {}\n",
    "\n",
    "# stores the results\n",
    "results = {}\n",
    "\n",
    "# stores the models\n",
    "models = {}\n",
    "\n",
    "# stores the model history\n",
    "model_history = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Only Including Fixed-Length Sequences\n",
    "\n",
    "The upstream −35 motif and the downstream −10 motif are both 6 base pairs long. Concatenating them together gives us an input with non-variable length. This is the simpler approach, and ignores the complexity of including variable-length inputs. However, the upstream −35 motif and the downstream −10 motif are the promoter sequences, and should have a large effect on the transcription rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the one-hot encoded h35 + h10 sequence motifs\n",
    "X_dict['h35_h10'] = np.array([one_hot_encode(h35 + h10) for h35, h10 in zip(df['h35'], df['h10'])])\n",
    "\n",
    "# The first entry for this approach, one-hot encoded from 'TTGACATATAAT'\n",
    "X_dict['h35_h10'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dict['h35_h10'], y, test_size=0.2, random_state=1, shuffle=True)\n",
    "train_test['h35_h10'] = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(current_model):\n",
    "    # Define RNN model architecture\n",
    "    models[current_model] = Sequential()\n",
    "    models[current_model].add(LSTM(64, input_shape=X_dict[current_model].shape[1:])) # dynamically generated input shape based on X data\n",
    "    models[current_model].add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    models[current_model].compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = models[current_model].fit(train_test[current_model]['X_train'],\n",
    "                                    train_test[current_model]['y_train'],\n",
    "                                    epochs=150,\n",
    "                                    batch_size=32,\n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = models[current_model].evaluate(train_test[current_model]['X_test'], train_test[current_model]['y_test'])\n",
    "\n",
    "    return models[current_model], loss, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.2557 - val_loss: 0.3109\n",
      "Epoch 2/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3323 - val_loss: 0.2493\n",
      "Epoch 3/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2427 - val_loss: 0.2104\n",
      "Epoch 4/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2230 - val_loss: 0.2417\n",
      "Epoch 5/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2240 - val_loss: 0.2015\n",
      "Epoch 6/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2163 - val_loss: 0.2033\n",
      "Epoch 7/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2192 - val_loss: 0.2158\n",
      "Epoch 8/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2184 - val_loss: 0.1988\n",
      "Epoch 9/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2178 - val_loss: 0.2015\n",
      "Epoch 10/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2166 - val_loss: 0.2023\n",
      "Epoch 11/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2137 - val_loss: 0.2007\n",
      "Epoch 12/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2075 - val_loss: 0.1985\n",
      "Epoch 13/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2055 - val_loss: 0.1945\n",
      "Epoch 14/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2044 - val_loss: 0.2042\n",
      "Epoch 15/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2115 - val_loss: 0.1917\n",
      "Epoch 16/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2071 - val_loss: 0.1971\n",
      "Epoch 17/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2072 - val_loss: 0.1885\n",
      "Epoch 18/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1972 - val_loss: 0.2055\n",
      "Epoch 19/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2104 - val_loss: 0.1891\n",
      "Epoch 20/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1986 - val_loss: 0.1856\n",
      "Epoch 21/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2011 - val_loss: 0.1825\n",
      "Epoch 22/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1966 - val_loss: 0.1815\n",
      "Epoch 23/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1979 - val_loss: 0.1875\n",
      "Epoch 24/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1955 - val_loss: 0.2050\n",
      "Epoch 25/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1933 - val_loss: 0.1862\n",
      "Epoch 26/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1986 - val_loss: 0.1810\n",
      "Epoch 27/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1873 - val_loss: 0.1783\n",
      "Epoch 28/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1934 - val_loss: 0.1853\n",
      "Epoch 29/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1981 - val_loss: 0.1785\n",
      "Epoch 30/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1798 - val_loss: 0.1776\n",
      "Epoch 31/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1828 - val_loss: 0.1754\n",
      "Epoch 32/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1833 - val_loss: 0.1822\n",
      "Epoch 33/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1814 - val_loss: 0.1793\n",
      "Epoch 34/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1836 - val_loss: 0.1728\n",
      "Epoch 35/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1875 - val_loss: 0.1756\n",
      "Epoch 36/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1779 - val_loss: 0.1770\n",
      "Epoch 37/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1774 - val_loss: 0.1726\n",
      "Epoch 38/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1759 - val_loss: 0.1766\n",
      "Epoch 39/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1761 - val_loss: 0.1728\n",
      "Epoch 40/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1750 - val_loss: 0.1765\n",
      "Epoch 41/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1691 - val_loss: 0.1713\n",
      "Epoch 42/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1750 - val_loss: 0.1717\n",
      "Epoch 43/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1775 - val_loss: 0.1732\n",
      "Epoch 44/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1731 - val_loss: 0.1737\n",
      "Epoch 45/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1710 - val_loss: 0.1704\n",
      "Epoch 46/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1692 - val_loss: 0.1709\n",
      "Epoch 47/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1731 - val_loss: 0.1716\n",
      "Epoch 48/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1628 - val_loss: 0.1748\n",
      "Epoch 49/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1693 - val_loss: 0.1733\n",
      "Epoch 50/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1633 - val_loss: 0.1699\n",
      "Epoch 51/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1793 - val_loss: 0.1719\n",
      "Epoch 52/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1690 - val_loss: 0.1701\n",
      "Epoch 53/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1710 - val_loss: 0.1689\n",
      "Epoch 54/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1694 - val_loss: 0.1713\n",
      "Epoch 55/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1622 - val_loss: 0.1732\n",
      "Epoch 56/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1643 - val_loss: 0.1747\n",
      "Epoch 57/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1719 - val_loss: 0.1693\n",
      "Epoch 58/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1668 - val_loss: 0.1693\n",
      "Epoch 59/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1708 - val_loss: 0.1680\n",
      "Epoch 60/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1587 - val_loss: 0.1696\n",
      "Epoch 61/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1663 - val_loss: 0.1664\n",
      "Epoch 62/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1601 - val_loss: 0.1698\n",
      "Epoch 63/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1640 - val_loss: 0.1677\n",
      "Epoch 64/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1581 - val_loss: 0.1680\n",
      "Epoch 65/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1582 - val_loss: 0.1700\n",
      "Epoch 66/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1590 - val_loss: 0.1694\n",
      "Epoch 67/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1560 - val_loss: 0.1685\n",
      "Epoch 68/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1606 - val_loss: 0.1696\n",
      "Epoch 69/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1559 - val_loss: 0.1694\n",
      "Epoch 70/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1627 - val_loss: 0.1685\n",
      "Epoch 71/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1559 - val_loss: 0.1684\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1746\n"
     ]
    }
   ],
   "source": [
    "# Call the function to build the model, save the model, and store the results\n",
    "\n",
    "m = 'h35_h10'\n",
    "\n",
    "models[m], results[m], model_history[m] = build_model(m)\n",
    "models[m].save(m + '.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implimenting a Convolutional Neural Network\n",
    "\n",
    "Next, we can train a CNN on the same data as before. We should see that the CNN performs marginally worse as it is not as well suited for this dataset compared to an LSTM. This is because CNNs are primarily used for grid-structured data like images instead of sequential inputs. Although they can also be applied to sequential data by treating the data as a one-dimensional grid (e.g., for text classification tasks), CNNs do not inherently capture sequential dependencies as effectively as LSTMs (O'Shea and Nash, 2015). \n",
    "\n",
    "I am extrapolating these results to the next LSTM, which is far more taxing and takes longer to train. Although the CNN will should improve given more data, it will not improve more than the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8390 - val_loss: 0.2189\n",
      "Epoch 2/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2189 - val_loss: 0.2000\n",
      "Epoch 3/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2044 - val_loss: 0.1907\n",
      "Epoch 4/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2033 - val_loss: 0.1894\n",
      "Epoch 5/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2033 - val_loss: 0.1891\n",
      "Epoch 6/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1989 - val_loss: 0.1915\n",
      "Epoch 7/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1971 - val_loss: 0.1964\n",
      "Epoch 8/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1968 - val_loss: 0.1938\n",
      "Epoch 9/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2008 - val_loss: 0.1945\n",
      "Epoch 10/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1946 - val_loss: 0.1948\n",
      "Epoch 11/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1926 - val_loss: 0.1948\n",
      "Epoch 12/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1894 - val_loss: 0.1897\n",
      "Epoch 13/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1930 - val_loss: 0.1896\n",
      "Epoch 14/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1934 - val_loss: 0.1903\n",
      "Epoch 15/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1883 - val_loss: 0.2018\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 0.1960\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define CNN model architecture\n",
    "models['CNN'] = Sequential()\n",
    "models['CNN'].add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=X_dict['h35_h10'].shape[1:]))\n",
    "models['CNN'].add(MaxPooling1D(pool_size=2))\n",
    "models['CNN'].add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "models['CNN'].add(MaxPooling1D(pool_size=2))\n",
    "models['CNN'].add(Flatten())\n",
    "models['CNN'].add(Dense(64, activation='relu'))\n",
    "models['CNN'].add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "models['CNN'].compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = models['CNN'].fit(train_test['h35_h10']['X_train'],\n",
    "                                    train_test['h35_h10']['y_train'],\n",
    "                                    epochs=150,\n",
    "                                    batch_size=32,\n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss = models['CNN'].evaluate(train_test['h35_h10']['X_test'], train_test['h35_h10']['y_test'])\n",
    "\n",
    "results['CNN'] = loss\n",
    "model_history['CNN'] = history\n",
    "models['CNN'].save('CNN.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Padding\n",
    "\n",
    "Padding makes all the inputs equal in length by adding layers of zeros or other \"filler\" data outside the actual data in an input matrix. The primary purpose of padding is to preserve the spatial size of the input so that the output, after applying filters (kernels), remains the same size or adjusts according to the desired output dimensions (deepai.org, n.d.).\n",
    "\n",
    "Padding is a preprocessing step applied to the LSTM or CNN before performing the convolution operation. During backtracking, the weight of padding data decreases accordingly, allowing the network to be able to read any necessary information from the input border areas.\n",
    "According to Reddy and Reddy (2019), there is little difference in performance between pre-and post-padding in LSTMs, unlike with CNNs. However, they did find that LSTM pre-padding was marginally more accurate (5), so the padding will go upstream (before) the data. Additionally, I excluded the spacer sequence ('spacs' column) with lengths other than 16, 17, or 18. The other sequences have been synthetically developed and vary with a length from 0 to 31. This large standard deviation does not help produce more accurate results and only increases runtime.\n",
    "\n",
    "According to Reddy and Reddy (2019), there is little difference in performance between pre-and post-padding in LSTMs, unlike with CNNs. However, they did find that LSTM pre-padding was marginally more accurate (5), so the padding will go upstream (before) the data. For a more comprehensive comparison, see \"padding_comparison.ipynb\". Additionally, I excluded the spacer sequence ('spacs' column) with lengths other than 16, 17, or 18. The other sequences have been synthetically developed and vary with a length from 0 to 31. This large standard deviation does not help produce more accurate results and only increases runtime.\n",
    "\n",
    "My worry for using padding is that the model will fit trends to padding and that with such large inputs the model will have a difficult time identifying the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UP {16, 20, 22}\n",
      "h35 {6}\n",
      "spacs {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31}\n",
      "h10 {6}\n",
      "disc {8, 6, 7}\n",
      "ITR {20, 21}\n"
     ]
    }
   ],
   "source": [
    "# lengths the values in each X column\n",
    "\n",
    "for col in X.columns:\n",
    "    seen = set()\n",
    "    for each in X[col]:\n",
    "        seen.add(len(str(each)))\n",
    "    print(col, seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph the distribution of Spacer sequence lengths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(X['spacs'].apply(len), bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Spacer Sequence Lengths')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 508 rows\n"
     ]
    }
   ],
   "source": [
    "# remove all rows with spacer sequences that are not 16-18 nucleotides long\n",
    "\n",
    "\n",
    "_df = df[(df['spacs'].str.len() >= 16) & (df['spacs'].str.len() <= 18)]\n",
    "\n",
    "\n",
    "X = _df[['UP', 'h35', 'spacs', 'h10', 'disc', 'ITR']]\n",
    "y = _df['Observed log(TX/Txref)']\n",
    "\n",
    "print(f'Removed {df.shape[0] - _df.shape[0]} rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one-hot encode DNA sequences, including padding 0's\n",
    "\n",
    "def padded_one_hot_encode(sequence):\n",
    "    mapping = {'A': [1,0,0,0,0], 'C': [0,1,0,0,0], 'G': [0,0,1,0,0], 'T': [0,0,0,1,0], '0': [0,0,0,0,1]}\n",
    "    encoding = []\n",
    "    for nucleotide in sequence:\n",
    "         encoding += [mapping[nucleotide]]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13481, 94, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upstream_padding = {}\n",
    "\n",
    "for col in X.columns:\n",
    "    max_len = X[col].apply(len).max()\n",
    "    upstream_padding[col] = np.array([padded_one_hot_encode('0' * (max_len - len(seq)) + seq) for seq in X[col]])\n",
    "\n",
    "# Concatenate the one-hot encoded, upstream-padded sequences\n",
    "X_dict['upstream_padding'] = np.concatenate([upstream_padding[col] for col in X.columns], axis=1)\n",
    "X_dict['upstream_padding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dict['upstream_padding'], y, test_size=0.2, random_state=1, shuffle=True)\n",
    "train_test['upstream_padding'] = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 1.3312 - val_loss: 0.7525\n",
      "Epoch 2/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.4052 - val_loss: 0.2461\n",
      "Epoch 3/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2618 - val_loss: 0.2684\n",
      "Epoch 4/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2633 - val_loss: 0.2354\n",
      "Epoch 5/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2353 - val_loss: 0.2302\n",
      "Epoch 6/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2284 - val_loss: 0.2289\n",
      "Epoch 7/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2375 - val_loss: 0.2414\n",
      "Epoch 8/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2302 - val_loss: 0.2292\n",
      "Epoch 9/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2217 - val_loss: 0.2203\n",
      "Epoch 10/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2224 - val_loss: 0.2225\n",
      "Epoch 11/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2279 - val_loss: 0.2198\n",
      "Epoch 12/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2177 - val_loss: 0.2155\n",
      "Epoch 13/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2275 - val_loss: 0.2208\n",
      "Epoch 14/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2175 - val_loss: 0.2126\n",
      "Epoch 15/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2127 - val_loss: 0.2200\n",
      "Epoch 16/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2076 - val_loss: 0.2275\n",
      "Epoch 17/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2047 - val_loss: 0.2283\n",
      "Epoch 18/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2095 - val_loss: 0.2190\n",
      "Epoch 19/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2152 - val_loss: 0.2216\n",
      "Epoch 20/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2070 - val_loss: 0.2241\n",
      "Epoch 21/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.2092 - val_loss: 0.1959\n",
      "Epoch 22/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.2065 - val_loss: 0.2042\n",
      "Epoch 23/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.2023 - val_loss: 0.1964\n",
      "Epoch 24/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1943 - val_loss: 0.1979\n",
      "Epoch 25/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1936 - val_loss: 0.1872\n",
      "Epoch 26/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1869 - val_loss: 0.1852\n",
      "Epoch 27/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1871 - val_loss: 0.1907\n",
      "Epoch 28/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1857 - val_loss: 0.1822\n",
      "Epoch 29/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1811 - val_loss: 0.1929\n",
      "Epoch 30/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.1786 - val_loss: 0.1881\n",
      "Epoch 31/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1740 - val_loss: 0.1742\n",
      "Epoch 32/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1693 - val_loss: 0.1727\n",
      "Epoch 33/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1665 - val_loss: 0.1688\n",
      "Epoch 34/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1685 - val_loss: 0.1732\n",
      "Epoch 35/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1740 - val_loss: 0.1683\n",
      "Epoch 36/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1655 - val_loss: 0.1620\n",
      "Epoch 37/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1735 - val_loss: 0.1640\n",
      "Epoch 38/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1648 - val_loss: 0.1642\n",
      "Epoch 39/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1640 - val_loss: 0.1636\n",
      "Epoch 40/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1555 - val_loss: 0.1679\n",
      "Epoch 41/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1610 - val_loss: 0.1620\n",
      "Epoch 42/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1575 - val_loss: 0.1579\n",
      "Epoch 43/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1564 - val_loss: 0.1569\n",
      "Epoch 44/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1544 - val_loss: 0.1567\n",
      "Epoch 45/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1552 - val_loss: 0.1555\n",
      "Epoch 46/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1537 - val_loss: 0.1559\n",
      "Epoch 47/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1556 - val_loss: 0.1507\n",
      "Epoch 48/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1545 - val_loss: 0.1497\n",
      "Epoch 49/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1493 - val_loss: 0.1555\n",
      "Epoch 50/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1508 - val_loss: 0.1527\n",
      "Epoch 51/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1473 - val_loss: 0.1470\n",
      "Epoch 52/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1460 - val_loss: 0.1501\n",
      "Epoch 53/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1510 - val_loss: 0.1466\n",
      "Epoch 54/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1511 - val_loss: 0.1482\n",
      "Epoch 55/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1466 - val_loss: 0.1430\n",
      "Epoch 56/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 0.1467 - val_loss: 0.1455\n",
      "Epoch 57/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1418 - val_loss: 0.1466\n",
      "Epoch 58/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1486 - val_loss: 0.1450\n",
      "Epoch 59/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1416 - val_loss: 0.1542\n",
      "Epoch 60/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1495 - val_loss: 0.1441\n",
      "Epoch 61/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1436 - val_loss: 0.1430\n",
      "Epoch 62/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1363 - val_loss: 0.1442\n",
      "Epoch 63/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1463 - val_loss: 0.1475\n",
      "Epoch 64/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.1395 - val_loss: 0.1480\n",
      "Epoch 65/150\n",
      "\u001b[1m325/325\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.1400 - val_loss: 0.1464\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1428\n"
     ]
    }
   ],
   "source": [
    "# Call the function to build the model, save the model, and store the results\n",
    "\n",
    "m = 'upstream_padding'\n",
    "\n",
    "models[m], results[m], model_history[m] = build_model(m)\n",
    "models[m].save(m + '.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating LaFleur et al.'s Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Observed log(TX/Txref)</th>\n",
       "      <th>Predicted log(TX/Txref)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.386326</td>\n",
       "      <td>-3.844827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.503140</td>\n",
       "      <td>-3.905283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.207206</td>\n",
       "      <td>-3.905283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.392439</td>\n",
       "      <td>-3.877808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.698903</td>\n",
       "      <td>-3.672384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Observed log(TX/Txref)  Predicted log(TX/Txref)\n",
       "0               -3.386326                -3.844827\n",
       "1               -3.503140                -3.905283\n",
       "2               -4.207206                -3.905283\n",
       "3               -3.392439                -3.877808\n",
       "4               -3.698903                -3.672384"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all but 'Observed log(TX/Txref)' and 'Predicted log(TX/Txref)' columns\n",
    "\n",
    "df_eval = df[['Observed log(TX/Txref)', 'Predicted log(TX/Txref)']]\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2514652958788732\n"
     ]
    }
   ],
   "source": [
    "# evaluate using mean squared error and mean absolute error for LaFleur et al.'s model\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "results['LaFleur'] = mean_squared_error(df_eval['Observed log(TX/Txref)'], df_eval['Predicted log(TX/Txref)'])\n",
    "\n",
    "print(f\"MSE: {results['LaFleur']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h35_h10</th>\n",
       "      <th>CNN</th>\n",
       "      <th>upstream_padding</th>\n",
       "      <th>LaFleur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.166</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     h35_h10    CNN  upstream_padding  LaFleur\n",
       "MSE    0.166  0.189             0.143    0.251"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# round the MSE of each model to 2 decimal places, then display the results\n",
    "\n",
    "df_results = pd.DataFrame(results, index=['MSE'])\n",
    "df_results = df_results.applymap(lambda x: round(x, 3))\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Compared to LaFleur et al.'s Multiple Linear Regression model, both Long Short-Term Memory models performed far better. Only including fixed-length inputs, the upstream −35 motif, and the downstream −10 motif still decreased the mean-squared error from 0.251 to 0.167. These were good predictors of transcription rate as are both are the promoter sequences. This is the simpler approach, but it ignores the complexity of including variable-length inputs. \n",
    "\n",
    "Adding padding to the data to include all DNA sequences decreased the mean squared error to 0.143. Padding makes all the inputs equal in length by adding layers of zeros or other \"filler\" data outside the actual data in an input matrix. The primary purpose of padding is to preserve the spatial size of the input so that the output, after applying filters (kernels), remains the same size or adjusts according to the desired output dimensions (deepai.org, n.d.).\n",
    "\n",
    "Padding is a preprocessing step applied to the LSTM or CNN before performing the convolution operation. During backtracking, the weight of padding data decreases accordingly, allowing the network to be able to read any necessary information from the input border areas.\n",
    "According to Reddy and Reddy (2019), there is little difference in performance between pre-and post-padding in LSTMs, unlike with CNNs. However, they did find that LSTM pre-padding was marginally more accurate (5), so the padding will go upstream (before) the data. Additionally, I excluded the spacer sequence ('spacs' column) with lengths other than 16, 17, or 18. The other sequences have been synthetically developed and vary with a length from 0 to 31. This large standard deviation does not help produce more accurate results and only increases runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "For the best results, one should optimize the LSTM with the following hyperparameters: the neurons, layers, learning rate, batch size, and epochs. However, even randomized-search hyperparameter tuning is too taxing and time-intensive for my computer. In the future, I will perform hyperparameter tuning to better optimize the model. To reach robust performance results with non-parametric models, their respective hyperparameters must be optimized. Default hyperparameter settings cannot guarantee an optimal performance of machine-learning techniques, and additional attention should be directed to this critical step (Schratz et al., 2019). To do this, one should design their model to objectively search different values for model hyperparameters and choose a subset that results in a model that achieves the best performance on a given dataset (Brownlee 2020). Two common implementations for hyperparameter tuning are \"randomized search\" and \"grid search.\" While grid search finds the more optimal hyperparameter, randomized search is far less taxing regarding time and processing power (Brownlee 2020). For LSTM, there are several hyperparameters to optimize.\n",
    "\n",
    "**Number of LSTM Neurons:** Increasing the number of neurons per hidden layer will increase the complexity of the model. This will, in turn, improve accuracy, so long as it does not over-fit the data.\n",
    "\n",
    "**Number of Layers:** Increasing the hidden layers will similarly increase how \"deep\" the model is. Past a few hidden layers, the model will become \"blackboxed,\" and one will not be able to conclude meaningful relationships from it. However, this will again increase accuracy so long as it does not overfit the data.\n",
    "\n",
    "**Learning Rate (alpha):** The learning rate is a tuning parameter in the gradient descent optimization algorithm. It determines the step size at each iteration while moving toward a minimum loss function. Increasing the learning rate will increase the model's training speed and convergence, but if it is too high, the algorithm will overstep and not converge.\n",
    "\n",
    "**Batch Size:** Batches are the samples used during each epoch to update the model's weights. Smaller batch sizes can lead to faster convergence but might be less stable. Larger batch sizes may provide more stable updates but require more memory. Experiment with different batch sizes to find the optimal balance.\n",
    "\n",
    "**Number of Epochs (Not Tuned):** The number of epochs determines how many times the model is trained with one forward and one backward propagation. Too few epochs lead to underfitting; likewise, too many lead to overfitting. The number of epochs should only set as the upper bound. Ideally, the model stops itself when it reaches a minimum MSE.\n",
    "\n",
    "See \"tuning.ipynb\" for more on randomized-search hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Cited\n",
    "\n",
    "Brownlee, Jason. “Hyperparameter Optimization with Random Search and Grid Search.” MachineLearningMastery.Com, 18 Sept. 2020, machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/.\n",
    "\n",
    "LaFleur, Travis L., et al. “Automated Model-Predictive Design of Synthetic Promoters to Control Transcriptional Profiles in Bacteria.” Nature News, Nature Publishing Group, 2 Sept. 2022, www.nature.com/articles/s41467-022-32829-5.\n",
    "\n",
    "“Padding (Machine Learning).” DeepAI, DeepAI, 17 May 2019, deepai.org/machine-learning-glossary-and-terms/padding.\n",
    "\n",
    "Reddy, Mahidhar Dwarampudi, and Subba Reddy. “Effects of Padding on Lstms and CNNS.” arXiv.Org, 18 Mar. 2019, arxiv.org/abs/1903.07288.\n",
    "\n",
    "Russell, Peter J. IGenetics. Benjamin Cummings, 2006. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
