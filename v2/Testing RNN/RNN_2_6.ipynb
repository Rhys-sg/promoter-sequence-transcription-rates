{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This changes the loss to incentivize boolean outputs:\n",
    "1. Implement loss calculation based on deviation from Argmax with a Straight-Through Estimator (STE)\n",
    "\n",
    "TODO:\n",
    "1. Change Loss to compare CNN(predicted_sequence) to CNN(actual_sequence) instead of CNN(predicted_sequence) to actual_expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RNN_2_6 as parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'RNN_2_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Data/combined/LaFleur_supp.csv'\n",
    "\n",
    "df, scaler = parent.load_and_preprocess_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequence, X_expressions, y = parent.preprocess_X_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequence_train, X_sequence_test, X_expressions_train, X_expressions_test, y_train, y_test = parent.train_test_split(\n",
    "        X_sequence, X_expressions, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_model = parent.load_model('../Models/CNN_5_0.keras')\n",
    "lstm_model = parent.build_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Sequence 0/39013\n",
      "Weighted One-Hot Loss: 2.246097665192792e-06\n",
      "Weighted Expression Loss: 0.524415135383606\n",
      "Epoch 1, Sequence 512/39013\n",
      "Weighted One-Hot Loss: 2.290283418915351e-06\n",
      "Weighted Expression Loss: 0.5614840984344482\n",
      "Epoch 1, Sequence 1024/39013\n",
      "Weighted One-Hot Loss: 2.419997144897934e-06\n",
      "Weighted Expression Loss: 0.5505037307739258\n",
      "Epoch 1, Sequence 1536/39013\n",
      "Weighted One-Hot Loss: 2.2764377263229107e-06\n",
      "Weighted Expression Loss: 0.4893572926521301\n",
      "Epoch 1, Sequence 2048/39013\n",
      "Weighted One-Hot Loss: 2.23061442738981e-06\n",
      "Weighted Expression Loss: 0.5431702136993408\n",
      "Epoch 1, Sequence 2560/39013\n",
      "Weighted One-Hot Loss: 2.3474158297176473e-06\n",
      "Weighted Expression Loss: 0.5632821321487427\n",
      "Epoch 1, Sequence 3072/39013\n",
      "Weighted One-Hot Loss: 2.2984197585174115e-06\n",
      "Weighted Expression Loss: 0.5329558253288269\n",
      "Epoch 1, Sequence 3584/39013\n",
      "Weighted One-Hot Loss: 2.3026227609079797e-06\n",
      "Weighted Expression Loss: 0.48990291357040405\n",
      "Epoch 1, Sequence 4096/39013\n",
      "Weighted One-Hot Loss: 2.4121870865201345e-06\n",
      "Weighted Expression Loss: 0.524115800857544\n",
      "Epoch 1, Sequence 4608/39013\n",
      "Weighted One-Hot Loss: 2.191098928960855e-06\n",
      "Weighted Expression Loss: 0.5172510147094727\n",
      "Epoch 1, Sequence 5120/39013\n",
      "Weighted One-Hot Loss: 2.2536644337378675e-06\n",
      "Weighted Expression Loss: 0.535854697227478\n",
      "Epoch 1, Sequence 5632/39013\n",
      "Weighted One-Hot Loss: 2.30811451729096e-06\n",
      "Weighted Expression Loss: 0.5123218297958374\n",
      "Epoch 1, Sequence 6144/39013\n",
      "Weighted One-Hot Loss: 2.296369984833291e-06\n",
      "Weighted Expression Loss: 0.6055753231048584\n",
      "Epoch 1, Sequence 6656/39013\n",
      "Weighted One-Hot Loss: 2.3547511318611214e-06\n",
      "Weighted Expression Loss: 0.5995065569877625\n",
      "Epoch 1, Sequence 7168/39013\n",
      "Weighted One-Hot Loss: 2.346351720916573e-06\n",
      "Weighted Expression Loss: 0.5255426168441772\n",
      "Epoch 1, Sequence 7680/39013\n",
      "Weighted One-Hot Loss: 2.3017089461063733e-06\n",
      "Weighted Expression Loss: 0.5533745884895325\n",
      "Epoch 1, Sequence 8192/39013\n",
      "Weighted One-Hot Loss: 2.2608530798606807e-06\n",
      "Weighted Expression Loss: 0.5404065251350403\n",
      "Epoch 1, Sequence 8704/39013\n",
      "Weighted One-Hot Loss: 2.240331468783552e-06\n",
      "Weighted Expression Loss: 0.5208724141120911\n",
      "Epoch 1, Sequence 9216/39013\n",
      "Weighted One-Hot Loss: 2.2909562176209874e-06\n",
      "Weighted Expression Loss: 0.5655680894851685\n",
      "Epoch 1, Sequence 9728/39013\n",
      "Weighted One-Hot Loss: 2.2019605694367783e-06\n",
      "Weighted Expression Loss: 0.582959771156311\n",
      "Epoch 1, Sequence 10240/39013\n",
      "Weighted One-Hot Loss: 2.3600955501024146e-06\n",
      "Weighted Expression Loss: 0.45177334547042847\n",
      "Epoch 1, Sequence 10752/39013\n",
      "Weighted One-Hot Loss: 2.307564500370063e-06\n",
      "Weighted Expression Loss: 0.5671507716178894\n",
      "Epoch 1, Sequence 11264/39013\n",
      "Weighted One-Hot Loss: 2.3320496893575182e-06\n",
      "Weighted Expression Loss: 0.5084894895553589\n",
      "Epoch 1, Sequence 11776/39013\n",
      "Weighted One-Hot Loss: 2.4002390546229435e-06\n",
      "Weighted Expression Loss: 0.5422589182853699\n",
      "Epoch 1, Sequence 12288/39013\n",
      "Weighted One-Hot Loss: 2.3184113615570823e-06\n",
      "Weighted Expression Loss: 0.5055840611457825\n",
      "Epoch 1, Sequence 12800/39013\n",
      "Weighted One-Hot Loss: 2.322309910596232e-06\n",
      "Weighted Expression Loss: 0.49063676595687866\n",
      "Epoch 1, Sequence 13312/39013\n",
      "Weighted One-Hot Loss: 2.2394262941816123e-06\n",
      "Weighted Expression Loss: 0.5625232458114624\n",
      "Epoch 1, Sequence 13824/39013\n",
      "Weighted One-Hot Loss: 2.2203171283763368e-06\n",
      "Weighted Expression Loss: 0.4792896807193756\n",
      "Epoch 1, Sequence 14336/39013\n",
      "Weighted One-Hot Loss: 2.2604476725973655e-06\n",
      "Weighted Expression Loss: 0.5485931634902954\n",
      "Epoch 1, Sequence 14848/39013\n",
      "Weighted One-Hot Loss: 2.2625545170740224e-06\n",
      "Weighted Expression Loss: 0.5256838202476501\n",
      "Epoch 1, Sequence 15360/39013\n",
      "Weighted One-Hot Loss: 2.2638494101556716e-06\n",
      "Weighted Expression Loss: 0.5283719301223755\n",
      "Epoch 1, Sequence 15872/39013\n",
      "Weighted One-Hot Loss: 2.2333197193802334e-06\n",
      "Weighted Expression Loss: 0.5780196189880371\n",
      "Epoch 1, Sequence 16384/39013\n",
      "Weighted One-Hot Loss: 2.2815340798842954e-06\n",
      "Weighted Expression Loss: 0.5359949469566345\n",
      "Epoch 1, Sequence 16896/39013\n",
      "Weighted One-Hot Loss: 2.381094418524299e-06\n",
      "Weighted Expression Loss: 0.6179843544960022\n",
      "Epoch 1, Sequence 17408/39013\n",
      "Weighted One-Hot Loss: 2.4373871383431833e-06\n",
      "Weighted Expression Loss: 0.5017427206039429\n",
      "Epoch 1, Sequence 17920/39013\n",
      "Weighted One-Hot Loss: 2.3887680526968325e-06\n",
      "Weighted Expression Loss: 0.5754215121269226\n",
      "Epoch 1, Sequence 18432/39013\n",
      "Weighted One-Hot Loss: 2.1898042632528814e-06\n",
      "Weighted Expression Loss: 0.5905593037605286\n",
      "Epoch 1, Sequence 18944/39013\n",
      "Weighted One-Hot Loss: 2.2906458525540074e-06\n",
      "Weighted Expression Loss: 0.5861867070198059\n",
      "Epoch 1, Sequence 19456/39013\n",
      "Weighted One-Hot Loss: 2.284366019011941e-06\n",
      "Weighted Expression Loss: 0.48940250277519226\n",
      "Epoch 1, Sequence 19968/39013\n",
      "Weighted One-Hot Loss: 2.2255699150264263e-06\n",
      "Weighted Expression Loss: 0.518116295337677\n",
      "Epoch 1, Sequence 20480/39013\n",
      "Weighted One-Hot Loss: 2.318820179425529e-06\n",
      "Weighted Expression Loss: 0.5373905301094055\n",
      "Epoch 1, Sequence 20992/39013\n",
      "Weighted One-Hot Loss: 2.2228182388062123e-06\n",
      "Weighted Expression Loss: 0.49037402868270874\n",
      "Epoch 1, Sequence 21504/39013\n",
      "Weighted One-Hot Loss: 2.1988407752360217e-06\n",
      "Weighted Expression Loss: 0.5000241994857788\n",
      "Epoch 1, Sequence 22016/39013\n",
      "Weighted One-Hot Loss: 2.38452184930793e-06\n",
      "Weighted Expression Loss: 0.4974048137664795\n",
      "Epoch 1, Sequence 22528/39013\n",
      "Weighted One-Hot Loss: 2.3346917714661686e-06\n",
      "Weighted Expression Loss: 0.5430566668510437\n",
      "Epoch 1, Sequence 23040/39013\n",
      "Weighted One-Hot Loss: 2.271246330565191e-06\n",
      "Weighted Expression Loss: 0.5060364603996277\n",
      "Epoch 1, Sequence 23552/39013\n",
      "Weighted One-Hot Loss: 2.2813983378000557e-06\n",
      "Weighted Expression Loss: 0.4992571175098419\n",
      "Epoch 1, Sequence 24064/39013\n",
      "Weighted One-Hot Loss: 2.266981482534902e-06\n",
      "Weighted Expression Loss: 0.5134561061859131\n",
      "Epoch 1, Sequence 24576/39013\n",
      "Weighted One-Hot Loss: 2.2416718366002897e-06\n",
      "Weighted Expression Loss: 0.4760848879814148\n",
      "Epoch 1, Sequence 25088/39013\n",
      "Weighted One-Hot Loss: 2.2846572846901836e-06\n",
      "Weighted Expression Loss: 0.5487996339797974\n",
      "Epoch 1, Sequence 25600/39013\n",
      "Weighted One-Hot Loss: 2.343111646041507e-06\n",
      "Weighted Expression Loss: 0.5302686095237732\n",
      "Epoch 1, Sequence 26112/39013\n",
      "Weighted One-Hot Loss: 2.285774371557636e-06\n",
      "Weighted Expression Loss: 0.4804297387599945\n",
      "Epoch 1, Sequence 26624/39013\n",
      "Weighted One-Hot Loss: 2.373014467593748e-06\n",
      "Weighted Expression Loss: 0.5961860418319702\n",
      "Epoch 1, Sequence 27136/39013\n",
      "Weighted One-Hot Loss: 2.268677235406358e-06\n",
      "Weighted Expression Loss: 0.561369776725769\n",
      "Epoch 1, Sequence 27648/39013\n",
      "Weighted One-Hot Loss: 2.1764872144558467e-06\n",
      "Weighted Expression Loss: 0.5271630883216858\n",
      "Epoch 1, Sequence 28160/39013\n",
      "Weighted One-Hot Loss: 2.2390831873053685e-06\n",
      "Weighted Expression Loss: 0.5394566059112549\n",
      "Epoch 1, Sequence 28672/39013\n",
      "Weighted One-Hot Loss: 2.2784965949540492e-06\n",
      "Weighted Expression Loss: 0.5410743951797485\n",
      "Epoch 1, Sequence 29184/39013\n",
      "Weighted One-Hot Loss: 2.2253555016504833e-06\n",
      "Weighted Expression Loss: 0.5736799836158752\n",
      "Epoch 1, Sequence 29696/39013\n",
      "Weighted One-Hot Loss: 2.2689182515023276e-06\n",
      "Weighted Expression Loss: 0.4889232814311981\n",
      "Epoch 1, Sequence 30208/39013\n",
      "Weighted One-Hot Loss: 2.2761087166145444e-06\n",
      "Weighted Expression Loss: 0.5220736265182495\n",
      "Epoch 1, Sequence 30720/39013\n",
      "Weighted One-Hot Loss: 2.353467380089569e-06\n",
      "Weighted Expression Loss: 0.5909750461578369\n",
      "Epoch 1, Sequence 31232/39013\n",
      "Weighted One-Hot Loss: 2.3090674403647427e-06\n",
      "Weighted Expression Loss: 0.49091193079948425\n",
      "Epoch 1, Sequence 31744/39013\n",
      "Weighted One-Hot Loss: 2.3500133465859108e-06\n",
      "Weighted Expression Loss: 0.4695967435836792\n",
      "Epoch 1, Sequence 32256/39013\n",
      "Weighted One-Hot Loss: 2.2427316253015306e-06\n",
      "Weighted Expression Loss: 0.5393410325050354\n",
      "Epoch 1, Sequence 32768/39013\n",
      "Weighted One-Hot Loss: 2.330303459530114e-06\n",
      "Weighted Expression Loss: 0.4968518316745758\n",
      "Epoch 1, Sequence 33280/39013\n",
      "Weighted One-Hot Loss: 2.242359869342181e-06\n",
      "Weighted Expression Loss: 0.49791353940963745\n",
      "Epoch 1, Sequence 33792/39013\n",
      "Weighted One-Hot Loss: 2.26912970902049e-06\n",
      "Weighted Expression Loss: 0.4758591949939728\n",
      "Epoch 1, Sequence 34304/39013\n",
      "Weighted One-Hot Loss: 2.3402044462272897e-06\n",
      "Weighted Expression Loss: 0.5647665858268738\n",
      "Epoch 1, Sequence 34816/39013\n",
      "Weighted One-Hot Loss: 2.262190037072287e-06\n",
      "Weighted Expression Loss: 0.5433802604675293\n",
      "Epoch 1, Sequence 35328/39013\n",
      "Weighted One-Hot Loss: 2.3325803795160027e-06\n",
      "Weighted Expression Loss: 0.5529100298881531\n",
      "Epoch 1, Sequence 35840/39013\n",
      "Weighted One-Hot Loss: 2.3247339413501322e-06\n",
      "Weighted Expression Loss: 0.5283910036087036\n",
      "Epoch 1, Sequence 36352/39013\n",
      "Weighted One-Hot Loss: 2.2751653432351304e-06\n",
      "Weighted Expression Loss: 0.5297636985778809\n",
      "Epoch 1, Sequence 36864/39013\n",
      "Weighted One-Hot Loss: 2.338002332180622e-06\n",
      "Weighted Expression Loss: 0.5888340473175049\n",
      "Epoch 1, Sequence 37376/39013\n",
      "Weighted One-Hot Loss: 2.4109424430207582e-06\n",
      "Weighted Expression Loss: 0.5533558130264282\n",
      "Epoch 1, Sequence 37888/39013\n",
      "Weighted One-Hot Loss: 2.3212851374410093e-06\n",
      "Weighted Expression Loss: 0.6102999448776245\n",
      "Epoch 1, Sequence 38400/39013\n",
      "Weighted One-Hot Loss: 2.278444753756048e-06\n",
      "Weighted Expression Loss: 0.5513501167297363\n",
      "Epoch 1, Sequence 38912/39013\n",
      "Weighted One-Hot Loss: 2.1495925466297194e-06\n",
      "Weighted Expression Loss: 0.5112481117248535\n"
     ]
    }
   ],
   "source": [
    "loss_history = parent.train_model(lstm_model, cnn_model, X_sequence_train, X_expressions_train, y_train)\n",
    "lstm_model.save(f'../Models/{name}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 12 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "lstm_model = parent.load_model(f'../Models/{name}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Mean Squared Error on Test Data: 0.0170\n"
     ]
    }
   ],
   "source": [
    "mse, predicted_expression = parent.evaluate_model(lstm_model, cnn_model, X_sequence_test, X_expressions_test)\n",
    "print(f'Mean Squared Error on Test Data: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "AGCCGCTTTTAGCGGACGACGTGAGTAAACAAAACCCAGACATCATGGATAATGG____________________AAAAGTATGCCTCAGATCGCGCATGATCGAAAGGATAACCGCCGGAAACAAAACTGGCTTCTGGCCCTGCTAACC\n",
      "AGCCGCTTTTAGCGGACGACGTGAGTAAACAAAACCCAGACATCATGGATAATGGCCCCCCCCCCCCCCCCCCCCAAAAGTATGCCTCAGATCGCGCATGATCGAAAGGATAACCGCCGGAAACAAAACTGGCTTCTGGCCCTGCTAACC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sequence = 'TTTTCTATCTACGTACTTGACACTATTTCCT____________ATT__________ACCTTAGTTTGTACGTT'\n",
    "generated_sequence = parent.predict_with_lstm(lstm_model, sequence, 0.5, scaler, 150)\n",
    "generated_sequence_onehot = parent.predict_with_lstm(lstm_model, sequence, 0.5, scaler, 150, decode_output=False)\n",
    "\n",
    "print(sequence)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         0.         0.         0.         1.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         0.         0.         1.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         1.         0.        ]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.2182927  0.19070092 0.23048279 0.19036151 0.17016213]\n",
      "  [0.21886395 0.19292253 0.22859623 0.19094735 0.16866995]\n",
      "  [0.21916409 0.19450848 0.227166   0.19161075 0.16755064]\n",
      "  [0.21926388 0.19562693 0.22609127 0.19228591 0.1667321 ]\n",
      "  [0.21922459 0.19640309 0.22529106 0.19293498 0.16614632]\n",
      "  [0.21909264 0.19692937 0.2246996  0.19354148 0.16573691]\n",
      "  [0.21890312 0.19727406 0.22426473 0.1940997  0.16545834]\n",
      "  [0.21868221 0.19748765 0.2239462  0.1946091  0.1652748 ]\n",
      "  [0.21844889 0.19760744 0.2237134  0.19507152 0.16515873]\n",
      "  [0.21821637 0.19766112 0.22354342 0.19548982 0.1650893 ]\n",
      "  [0.21799345 0.19766906 0.2234192  0.19586717 0.1650512 ]\n",
      "  [0.21778543 0.19764633 0.22332819 0.19620667 0.16503336]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.21793652 0.19654249 0.22625005 0.19376639 0.16550459]\n",
      "  [0.21852201 0.19634126 0.22554229 0.19416988 0.16542456]\n",
      "  [0.218697   0.19638748 0.22492644 0.19469656 0.16529258]\n",
      "  [0.21864115 0.196557   0.22441562 0.19522968 0.16515659]\n",
      "  [0.21846487 0.19677262 0.22400871 0.1957143  0.1650395 ]\n",
      "  [0.21823494 0.19698772 0.22369511 0.19613232 0.16494985]\n",
      "  [0.2179907  0.19717664 0.22346053 0.1964838  0.16488832]\n",
      "  [0.21775426 0.19732748 0.22328998 0.19677646 0.16485181]\n",
      "  [0.21753712 0.19743706 0.22316973 0.19702041 0.16483568]\n",
      "  [0.21734422 0.19750741 0.22308786 0.19722545 0.16483499]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         1.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         1.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.        ]\n",
      "  [0.         0.         0.         1.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]\n",
      "  [0.         1.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(generated_sequence_onehot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
