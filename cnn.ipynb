{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DNA1 [counts]</th>\n",
       "      <th>DNA2 [counts]</th>\n",
       "      <th>DNA3 [counts]</th>\n",
       "      <th>RNA1 [counts]</th>\n",
       "      <th>RNA2 [counts]</th>\n",
       "      <th>RNA3 [counts]</th>\n",
       "      <th>TX1 [au]</th>\n",
       "      <th>TX2 [au]</th>\n",
       "      <th>TX3 [au]</th>\n",
       "      <th>...</th>\n",
       "      <th>high quality</th>\n",
       "      <th>Observed log(TX/Txref)</th>\n",
       "      <th>Predicted log(TX/Txref)</th>\n",
       "      <th>dG10</th>\n",
       "      <th>dG35</th>\n",
       "      <th>dGDisc</th>\n",
       "      <th>dGITR</th>\n",
       "      <th>dGEXT10</th>\n",
       "      <th>dGSPAC</th>\n",
       "      <th>dGUP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8263</td>\n",
       "      <td>7261</td>\n",
       "      <td>5173</td>\n",
       "      <td>16341</td>\n",
       "      <td>10320</td>\n",
       "      <td>13506</td>\n",
       "      <td>2.258071</td>\n",
       "      <td>1.523795</td>\n",
       "      <td>1.545541</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.386326</td>\n",
       "      <td>-3.844827</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.106428</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5600</td>\n",
       "      <td>4886</td>\n",
       "      <td>3264</td>\n",
       "      <td>10986</td>\n",
       "      <td>7250</td>\n",
       "      <td>10800</td>\n",
       "      <td>2.240001</td>\n",
       "      <td>1.590845</td>\n",
       "      <td>1.958709</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.503140</td>\n",
       "      <td>-3.905283</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.166884</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7898</td>\n",
       "      <td>6790</td>\n",
       "      <td>4752</td>\n",
       "      <td>19572</td>\n",
       "      <td>32204</td>\n",
       "      <td>30585</td>\n",
       "      <td>2.829533</td>\n",
       "      <td>5.084911</td>\n",
       "      <td>3.810029</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-4.207206</td>\n",
       "      <td>-3.905283</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.166884</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10651</td>\n",
       "      <td>9875</td>\n",
       "      <td>6466</td>\n",
       "      <td>15734</td>\n",
       "      <td>16246</td>\n",
       "      <td>18908</td>\n",
       "      <td>1.686729</td>\n",
       "      <td>1.763814</td>\n",
       "      <td>1.731036</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.392439</td>\n",
       "      <td>-3.877808</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.139409</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12188</td>\n",
       "      <td>10793</td>\n",
       "      <td>6965</td>\n",
       "      <td>28609</td>\n",
       "      <td>21796</td>\n",
       "      <td>26803</td>\n",
       "      <td>2.680198</td>\n",
       "      <td>2.165100</td>\n",
       "      <td>2.278025</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.698903</td>\n",
       "      <td>-3.672384</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>0.066015</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  DNA1 [counts]  DNA2 [counts]  DNA3 [counts]  RNA1 [counts]  \\\n",
       "0   0           8263           7261           5173          16341   \n",
       "1   1           5600           4886           3264          10986   \n",
       "2   2           7898           6790           4752          19572   \n",
       "3   3          10651           9875           6466          15734   \n",
       "4   4          12188          10793           6965          28609   \n",
       "\n",
       "   RNA2 [counts]  RNA3 [counts]  TX1 [au]  TX2 [au]  TX3 [au]  ...  \\\n",
       "0          10320          13506  2.258071  1.523795  1.545541  ...   \n",
       "1           7250          10800  2.240001  1.590845  1.958709  ...   \n",
       "2          32204          30585  2.829533  5.084911  3.810029  ...   \n",
       "3          16246          18908  1.686729  1.763814  1.731036  ...   \n",
       "4          21796          26803  2.680198  2.165100  2.278025  ...   \n",
       "\n",
       "   high quality  Observed log(TX/Txref) Predicted log(TX/Txref)      dG10  \\\n",
       "0           Yes               -3.386326               -3.844827 -1.781524   \n",
       "1           Yes               -3.503140               -3.905283 -1.781524   \n",
       "2           Yes               -4.207206               -3.905283 -1.781524   \n",
       "3           Yes               -3.392439               -3.877808 -1.781524   \n",
       "4           Yes               -3.698903               -3.672384 -1.781524   \n",
       "\n",
       "       dG35    dGDisc     dGITR   dGEXT10  dGSPAC      dGUP  \n",
       "0 -1.477218 -0.106428 -0.021112  0.191352 -0.0924  0.400862  \n",
       "1 -1.477218 -0.166884 -0.021112  0.191352 -0.0924  0.400862  \n",
       "2 -1.477218 -0.166884 -0.021112  0.191352 -0.0924  0.400862  \n",
       "3 -1.477218 -0.139409 -0.021112  0.191352 -0.0924  0.400862  \n",
       "4 -1.477218  0.066015 -0.021112  0.191352 -0.0924  0.400862  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset into a pandas data frame\n",
    "df = pd.read_csv('41467_2022_32829_MOESM5_ESM.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one-hot encode DNA sequences\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    encoding = np.zeros((len(sequence), len(mapping)))\n",
    "    for i, nucleotide in enumerate(sequence):\n",
    "         encoding[i, mapping[nucleotide]] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13481, 12, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode the DNA sequence\n",
    "\n",
    "X = np.array([one_hot_encode(h35 + h10) for h35, h10 in zip(df['h35'], df['h10'])])\n",
    "y = df['Observed log(TX/Txref)']\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data in training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.4735 - val_loss: 0.4014\n",
      "Epoch 2/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4057 - val_loss: 0.3800\n",
      "Epoch 3/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3692 - val_loss: 0.3006\n",
      "Epoch 4/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3034 - val_loss: 0.2314\n",
      "Epoch 5/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2326 - val_loss: 0.2104\n",
      "Epoch 6/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2281 - val_loss: 0.2081\n",
      "Epoch 7/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2179 - val_loss: 0.2131\n",
      "Epoch 8/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2313 - val_loss: 0.2072\n",
      "Epoch 9/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2225 - val_loss: 0.2045\n",
      "Epoch 10/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2200 - val_loss: 0.2040\n",
      "Epoch 11/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2149 - val_loss: 0.2092\n",
      "Epoch 12/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2174 - val_loss: 0.2057\n",
      "Epoch 13/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2144 - val_loss: 0.2025\n",
      "Epoch 14/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2138 - val_loss: 0.2057\n",
      "Epoch 15/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2145 - val_loss: 0.2023\n",
      "Epoch 16/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2179 - val_loss: 0.2031\n",
      "Epoch 17/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2240 - val_loss: 0.2074\n",
      "Epoch 18/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2181 - val_loss: 0.2071\n",
      "Epoch 19/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2181 - val_loss: 0.2012\n",
      "Epoch 20/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2102 - val_loss: 0.2013\n",
      "Epoch 21/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2156 - val_loss: 0.2053\n",
      "Epoch 22/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2092 - val_loss: 0.2017\n",
      "Epoch 23/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2084 - val_loss: 0.1994\n",
      "Epoch 24/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2092 - val_loss: 0.2003\n",
      "Epoch 25/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2139 - val_loss: 0.1979\n",
      "Epoch 26/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2047 - val_loss: 0.1999\n",
      "Epoch 27/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2150 - val_loss: 0.1991\n",
      "Epoch 28/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2129 - val_loss: 0.1987\n",
      "Epoch 29/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2056 - val_loss: 0.1965\n",
      "Epoch 30/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2062 - val_loss: 0.1979\n",
      "Epoch 31/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2163 - val_loss: 0.1955\n",
      "Epoch 32/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2042 - val_loss: 0.1992\n",
      "Epoch 33/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2079 - val_loss: 0.1973\n",
      "Epoch 34/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2080 - val_loss: 0.1957\n",
      "Epoch 35/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2076 - val_loss: 0.1939\n",
      "Epoch 36/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2029 - val_loss: 0.1934\n",
      "Epoch 37/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2062 - val_loss: 0.1929\n",
      "Epoch 38/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2095 - val_loss: 0.1934\n",
      "Epoch 39/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2046 - val_loss: 0.1962\n",
      "Epoch 40/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1988 - val_loss: 0.1979\n",
      "Epoch 41/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1981 - val_loss: 0.1929\n",
      "Epoch 42/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2018 - val_loss: 0.1949\n",
      "Epoch 43/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2001 - val_loss: 0.1894\n",
      "Epoch 44/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1907 - val_loss: 0.1895\n",
      "Epoch 45/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2015 - val_loss: 0.1892\n",
      "Epoch 46/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1959 - val_loss: 0.1878\n",
      "Epoch 47/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2055 - val_loss: 0.1903\n",
      "Epoch 48/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1959 - val_loss: 0.1885\n",
      "Epoch 49/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2040 - val_loss: 0.1880\n",
      "Epoch 50/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2023 - val_loss: 0.1897\n",
      "Epoch 51/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2063 - val_loss: 0.1859\n",
      "Epoch 52/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1902 - val_loss: 0.1859\n",
      "Epoch 53/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2029 - val_loss: 0.1835\n",
      "Epoch 54/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2019 - val_loss: 0.1856\n",
      "Epoch 55/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1920 - val_loss: 0.1860\n",
      "Epoch 56/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1952 - val_loss: 0.1825\n",
      "Epoch 57/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1972 - val_loss: 0.1816\n",
      "Epoch 58/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1918 - val_loss: 0.1841\n",
      "Epoch 59/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1902 - val_loss: 0.1819\n",
      "Epoch 60/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1910 - val_loss: 0.1891\n",
      "Epoch 61/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1951 - val_loss: 0.1844\n",
      "Epoch 62/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1923 - val_loss: 0.1885\n",
      "Epoch 63/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1982 - val_loss: 0.1814\n",
      "Epoch 64/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1933 - val_loss: 0.1821\n",
      "Epoch 65/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1905 - val_loss: 0.1860\n",
      "Epoch 66/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1933 - val_loss: 0.1814\n",
      "Epoch 67/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1944 - val_loss: 0.1810\n",
      "Epoch 68/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1827 - val_loss: 0.1820\n",
      "Epoch 69/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1826 - val_loss: 0.1872\n",
      "Epoch 70/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1891 - val_loss: 0.1803\n",
      "Epoch 71/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1887 - val_loss: 0.1814\n",
      "Epoch 72/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1862 - val_loss: 0.1802\n",
      "Epoch 73/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1898 - val_loss: 0.1809\n",
      "Epoch 74/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1850 - val_loss: 0.1774\n",
      "Epoch 75/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1829 - val_loss: 0.1776\n",
      "Epoch 76/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1884 - val_loss: 0.1817\n",
      "Epoch 77/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1838 - val_loss: 0.1779\n",
      "Epoch 78/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1870 - val_loss: 0.1774\n",
      "Epoch 79/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1781 - val_loss: 0.1760\n",
      "Epoch 80/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1841 - val_loss: 0.1769\n",
      "Epoch 81/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1825 - val_loss: 0.1756\n",
      "Epoch 82/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1820 - val_loss: 0.1753\n",
      "Epoch 83/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1820 - val_loss: 0.1766\n",
      "Epoch 84/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1873 - val_loss: 0.1760\n",
      "Epoch 85/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1824 - val_loss: 0.1755\n",
      "Epoch 86/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1765 - val_loss: 0.1768\n",
      "Epoch 87/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1895 - val_loss: 0.1754\n",
      "Epoch 88/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1884 - val_loss: 0.1761\n",
      "Epoch 89/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1805 - val_loss: 0.1751\n",
      "Epoch 90/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1805 - val_loss: 0.1734\n",
      "Epoch 91/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1838 - val_loss: 0.1730\n",
      "Epoch 92/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1756 - val_loss: 0.1743\n",
      "Epoch 93/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1770 - val_loss: 0.1729\n",
      "Epoch 94/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1821 - val_loss: 0.1715\n",
      "Epoch 95/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1730 - val_loss: 0.1733\n",
      "Epoch 96/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1743 - val_loss: 0.1728\n",
      "Epoch 97/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1782 - val_loss: 0.1722\n",
      "Epoch 98/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1787 - val_loss: 0.1714\n",
      "Epoch 99/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1814 - val_loss: 0.1708\n",
      "Epoch 100/100\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1743 - val_loss: 0.1702\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1787\n",
      "Test Loss (mse): 0.17120632529258728\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Define RNN model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(12, 4)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss (mse):\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('my_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
