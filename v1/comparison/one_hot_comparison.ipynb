{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DNA1 [counts]</th>\n",
       "      <th>DNA2 [counts]</th>\n",
       "      <th>DNA3 [counts]</th>\n",
       "      <th>RNA1 [counts]</th>\n",
       "      <th>RNA2 [counts]</th>\n",
       "      <th>RNA3 [counts]</th>\n",
       "      <th>TX1 [au]</th>\n",
       "      <th>TX2 [au]</th>\n",
       "      <th>TX3 [au]</th>\n",
       "      <th>...</th>\n",
       "      <th>high quality</th>\n",
       "      <th>Observed log(TX/Txref)</th>\n",
       "      <th>Predicted log(TX/Txref)</th>\n",
       "      <th>dG10</th>\n",
       "      <th>dG35</th>\n",
       "      <th>dGDisc</th>\n",
       "      <th>dGITR</th>\n",
       "      <th>dGEXT10</th>\n",
       "      <th>dGSPAC</th>\n",
       "      <th>dGUP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8263</td>\n",
       "      <td>7261</td>\n",
       "      <td>5173</td>\n",
       "      <td>16341</td>\n",
       "      <td>10320</td>\n",
       "      <td>13506</td>\n",
       "      <td>2.258071</td>\n",
       "      <td>1.523795</td>\n",
       "      <td>1.545541</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.386326</td>\n",
       "      <td>-3.844827</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.106428</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5600</td>\n",
       "      <td>4886</td>\n",
       "      <td>3264</td>\n",
       "      <td>10986</td>\n",
       "      <td>7250</td>\n",
       "      <td>10800</td>\n",
       "      <td>2.240001</td>\n",
       "      <td>1.590845</td>\n",
       "      <td>1.958709</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.503140</td>\n",
       "      <td>-3.905283</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.166884</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7898</td>\n",
       "      <td>6790</td>\n",
       "      <td>4752</td>\n",
       "      <td>19572</td>\n",
       "      <td>32204</td>\n",
       "      <td>30585</td>\n",
       "      <td>2.829533</td>\n",
       "      <td>5.084911</td>\n",
       "      <td>3.810029</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-4.207206</td>\n",
       "      <td>-3.905283</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.166884</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10651</td>\n",
       "      <td>9875</td>\n",
       "      <td>6466</td>\n",
       "      <td>15734</td>\n",
       "      <td>16246</td>\n",
       "      <td>18908</td>\n",
       "      <td>1.686729</td>\n",
       "      <td>1.763814</td>\n",
       "      <td>1.731036</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.392439</td>\n",
       "      <td>-3.877808</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>-0.139409</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12188</td>\n",
       "      <td>10793</td>\n",
       "      <td>6965</td>\n",
       "      <td>28609</td>\n",
       "      <td>21796</td>\n",
       "      <td>26803</td>\n",
       "      <td>2.680198</td>\n",
       "      <td>2.165100</td>\n",
       "      <td>2.278025</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-3.698903</td>\n",
       "      <td>-3.672384</td>\n",
       "      <td>-1.781524</td>\n",
       "      <td>-1.477218</td>\n",
       "      <td>0.066015</td>\n",
       "      <td>-0.021112</td>\n",
       "      <td>0.191352</td>\n",
       "      <td>-0.0924</td>\n",
       "      <td>0.400862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  DNA1 [counts]  DNA2 [counts]  DNA3 [counts]  RNA1 [counts]  \\\n",
       "0   0           8263           7261           5173          16341   \n",
       "1   1           5600           4886           3264          10986   \n",
       "2   2           7898           6790           4752          19572   \n",
       "3   3          10651           9875           6466          15734   \n",
       "4   4          12188          10793           6965          28609   \n",
       "\n",
       "   RNA2 [counts]  RNA3 [counts]  TX1 [au]  TX2 [au]  TX3 [au]  ...  \\\n",
       "0          10320          13506  2.258071  1.523795  1.545541  ...   \n",
       "1           7250          10800  2.240001  1.590845  1.958709  ...   \n",
       "2          32204          30585  2.829533  5.084911  3.810029  ...   \n",
       "3          16246          18908  1.686729  1.763814  1.731036  ...   \n",
       "4          21796          26803  2.680198  2.165100  2.278025  ...   \n",
       "\n",
       "   high quality  Observed log(TX/Txref) Predicted log(TX/Txref)      dG10  \\\n",
       "0           Yes               -3.386326               -3.844827 -1.781524   \n",
       "1           Yes               -3.503140               -3.905283 -1.781524   \n",
       "2           Yes               -4.207206               -3.905283 -1.781524   \n",
       "3           Yes               -3.392439               -3.877808 -1.781524   \n",
       "4           Yes               -3.698903               -3.672384 -1.781524   \n",
       "\n",
       "       dG35    dGDisc     dGITR   dGEXT10  dGSPAC      dGUP  \n",
       "0 -1.477218 -0.106428 -0.021112  0.191352 -0.0924  0.400862  \n",
       "1 -1.477218 -0.166884 -0.021112  0.191352 -0.0924  0.400862  \n",
       "2 -1.477218 -0.166884 -0.021112  0.191352 -0.0924  0.400862  \n",
       "3 -1.477218 -0.139409 -0.021112  0.191352 -0.0924  0.400862  \n",
       "4 -1.477218  0.066015 -0.021112  0.191352 -0.0924  0.400862  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import dataset into a pandas data frame\n",
    "\n",
    "df = pd.read_csv('../41467_2022_32829_MOESM5_ESM.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UP</th>\n",
       "      <th>h35</th>\n",
       "      <th>spacs</th>\n",
       "      <th>h10</th>\n",
       "      <th>disc</th>\n",
       "      <th>ITR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>CCCCGCGG</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>CGCGGCGG</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>CGCGCCCG</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>GCGGCGGC</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TTTTCTATCTACGTAC</td>\n",
       "      <td>TTGACA</td>\n",
       "      <td>CTATTTCCTATTTCTCT</td>\n",
       "      <td>TATAAT</td>\n",
       "      <td>CGGGGGGC</td>\n",
       "      <td>CTCTACCTTAGTTTGTACGTT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 UP     h35              spacs     h10      disc  \\\n",
       "0  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  CCCCGCGG   \n",
       "1  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  CGCGGCGG   \n",
       "2  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  CGCGCCCG   \n",
       "3  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  GCGGCGGC   \n",
       "4  TTTTCTATCTACGTAC  TTGACA  CTATTTCCTATTTCTCT  TATAAT  CGGGGGGC   \n",
       "\n",
       "                     ITR  \n",
       "0  CTCTACCTTAGTTTGTACGTT  \n",
       "1  CTCTACCTTAGTTTGTACGTT  \n",
       "2  CTCTACCTTAGTTTGTACGTT  \n",
       "3  CTCTACCTTAGTTTGTACGTT  \n",
       "4  CTCTACCTTAGTTTGTACGTT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All input and output data\n",
    "\n",
    "X = df[['UP', 'h35', 'spacs', 'h10', 'disc', 'ITR']]\n",
    "y = df['Observed log(TX/Txref)']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one-hot encode DNA sequences\n",
    "\n",
    "def encode(sequence):\n",
    "    mapping = {'A': [1,0,0,0], 'C': [0,1,0,0], 'G': [0,0,1,0], 'T': [0,0,0,1]}\n",
    "    encoding = []\n",
    "    for nucleotide in sequence:\n",
    "         encoding += [mapping[nucleotide]]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to one-hot encode DNA sequences\n",
    "\n",
    "def dummy_drop_encode(sequence):\n",
    "    mapping = {'A': [1,0,0], 'C': [0,1,0], 'G': [0,0,1], 'T': [0,0,0]}\n",
    "    encoding = []\n",
    "    for nucleotide in sequence:\n",
    "         encoding += [mapping[nucleotide]]\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the various input approaches\n",
    "X_dict = {}\n",
    "\n",
    "# stores split training/testing\n",
    "train_test = {}\n",
    "\n",
    "# stores the results\n",
    "results = {}\n",
    "\n",
    "# stores the models\n",
    "models = {}\n",
    "\n",
    "# stores the model history\n",
    "model_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate the one-hot encoded h35 + h10 sequence motifs\n",
    "X_dict['no_dummy'] = np.array([encode(h35 + h10) for h35, h10 in zip(df['h35'], df['h10'])])\n",
    "\n",
    "# The first entry for this approach, one-hot encoded from 'TTGACATATAAT'\n",
    "X_dict['no_dummy'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dict['no_dummy'], y, test_size=0.2, random_state=1, shuffle=True)\n",
    "train_test['no_dummy'] = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(current_model):\n",
    "    # Define RNN model architecture\n",
    "    models[current_model] = Sequential()\n",
    "    models[current_model].add(LSTM(64, input_shape=X_dict[current_model].shape[1:])) # dynamically generated input shape based on X data\n",
    "    models[current_model].add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    models[current_model].compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = models[current_model].fit(train_test[current_model]['X_train'],\n",
    "                                    train_test[current_model]['y_train'],\n",
    "                                    epochs=150,\n",
    "                                    batch_size=32,\n",
    "                                    validation_data=(X_test, y_test),\n",
    "                                    callbacks=[early_stopping])\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = models[current_model].evaluate(train_test[current_model]['X_test'], train_test[current_model]['y_test'])\n",
    "\n",
    "    return models[current_model], loss, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.2557 - val_loss: 0.3109\n",
      "Epoch 2/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3323 - val_loss: 0.2493\n",
      "Epoch 3/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2427 - val_loss: 0.2104\n",
      "Epoch 4/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2230 - val_loss: 0.2417\n",
      "Epoch 5/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2240 - val_loss: 0.2015\n",
      "Epoch 6/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2163 - val_loss: 0.2033\n",
      "Epoch 7/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2192 - val_loss: 0.2158\n",
      "Epoch 8/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2184 - val_loss: 0.1988\n",
      "Epoch 9/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2178 - val_loss: 0.2015\n",
      "Epoch 10/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2166 - val_loss: 0.2023\n",
      "Epoch 11/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2137 - val_loss: 0.2007\n",
      "Epoch 12/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2075 - val_loss: 0.1985\n",
      "Epoch 13/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2055 - val_loss: 0.1945\n",
      "Epoch 14/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2044 - val_loss: 0.2042\n",
      "Epoch 15/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2115 - val_loss: 0.1917\n",
      "Epoch 16/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2071 - val_loss: 0.1971\n",
      "Epoch 17/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2072 - val_loss: 0.1885\n",
      "Epoch 18/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1972 - val_loss: 0.2055\n",
      "Epoch 19/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2104 - val_loss: 0.1891\n",
      "Epoch 20/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1986 - val_loss: 0.1856\n",
      "Epoch 21/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2011 - val_loss: 0.1825\n",
      "Epoch 22/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1966 - val_loss: 0.1815\n",
      "Epoch 23/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1979 - val_loss: 0.1875\n",
      "Epoch 24/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1955 - val_loss: 0.2050\n",
      "Epoch 25/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1933 - val_loss: 0.1862\n",
      "Epoch 26/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1986 - val_loss: 0.1810\n",
      "Epoch 27/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1873 - val_loss: 0.1783\n",
      "Epoch 28/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1934 - val_loss: 0.1853\n",
      "Epoch 29/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1981 - val_loss: 0.1785\n",
      "Epoch 30/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1798 - val_loss: 0.1776\n",
      "Epoch 31/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1828 - val_loss: 0.1754\n",
      "Epoch 32/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1833 - val_loss: 0.1822\n",
      "Epoch 33/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1814 - val_loss: 0.1793\n",
      "Epoch 34/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1836 - val_loss: 0.1728\n",
      "Epoch 35/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1875 - val_loss: 0.1756\n",
      "Epoch 36/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1779 - val_loss: 0.1770\n",
      "Epoch 37/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1774 - val_loss: 0.1726\n",
      "Epoch 38/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1759 - val_loss: 0.1766\n",
      "Epoch 39/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1761 - val_loss: 0.1728\n",
      "Epoch 40/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1750 - val_loss: 0.1765\n",
      "Epoch 41/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1691 - val_loss: 0.1713\n",
      "Epoch 42/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1750 - val_loss: 0.1717\n",
      "Epoch 43/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1775 - val_loss: 0.1732\n",
      "Epoch 44/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1731 - val_loss: 0.1737\n",
      "Epoch 45/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1710 - val_loss: 0.1704\n",
      "Epoch 46/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1692 - val_loss: 0.1709\n",
      "Epoch 47/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1731 - val_loss: 0.1716\n",
      "Epoch 48/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1628 - val_loss: 0.1748\n",
      "Epoch 49/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1693 - val_loss: 0.1733\n",
      "Epoch 50/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1633 - val_loss: 0.1699\n",
      "Epoch 51/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1793 - val_loss: 0.1719\n",
      "Epoch 52/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1690 - val_loss: 0.1701\n",
      "Epoch 53/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1710 - val_loss: 0.1689\n",
      "Epoch 54/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1694 - val_loss: 0.1713\n",
      "Epoch 55/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1622 - val_loss: 0.1732\n",
      "Epoch 56/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1643 - val_loss: 0.1747\n",
      "Epoch 57/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1719 - val_loss: 0.1693\n",
      "Epoch 58/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1668 - val_loss: 0.1693\n",
      "Epoch 59/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1708 - val_loss: 0.1680\n",
      "Epoch 60/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1587 - val_loss: 0.1696\n",
      "Epoch 61/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1663 - val_loss: 0.1664\n",
      "Epoch 62/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1601 - val_loss: 0.1698\n",
      "Epoch 63/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1640 - val_loss: 0.1677\n",
      "Epoch 64/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1581 - val_loss: 0.1680\n",
      "Epoch 65/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1582 - val_loss: 0.1700\n",
      "Epoch 66/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1590 - val_loss: 0.1694\n",
      "Epoch 67/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1560 - val_loss: 0.1685\n",
      "Epoch 68/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1606 - val_loss: 0.1696\n",
      "Epoch 69/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1559 - val_loss: 0.1694\n",
      "Epoch 70/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1627 - val_loss: 0.1685\n",
      "Epoch 71/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1559 - val_loss: 0.1684\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1746\n"
     ]
    }
   ],
   "source": [
    "# Call the function to build the model, save the model, and store the results\n",
    "\n",
    "m = 'no_dummy'\n",
    "\n",
    "models[m], results[m], model_history[m] = build_model(m)\n",
    "models[m].save(m + '.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Concatenate the one-hot encoded h35 + h10 sequence motifs\n",
    "X_dict['dummy_drop'] = np.array([dummy_drop_encode(h35 + h10) for h35, h10 in zip(df['h35'], df['h10'])])\n",
    "\n",
    "# The first entry for this approach, one-hot encoded from 'TTGACATATAAT'\n",
    "X_dict['dummy_drop'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data in training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dict['dummy_drop'], y, test_size=0.2, random_state=1, shuffle=True)\n",
    "train_test['dummy'] = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train, 'y_test': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1.5073 - val_loss: 0.3413\n",
      "Epoch 2/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3216 - val_loss: 0.2168\n",
      "Epoch 3/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2316 - val_loss: 0.2190\n",
      "Epoch 4/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2282 - val_loss: 0.2201\n",
      "Epoch 5/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2327 - val_loss: 0.2043\n",
      "Epoch 6/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2345 - val_loss: 0.2252\n",
      "Epoch 7/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2188 - val_loss: 0.2052\n",
      "Epoch 8/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2110 - val_loss: 0.2061\n",
      "Epoch 9/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2170 - val_loss: 0.2181\n",
      "Epoch 10/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2196 - val_loss: 0.2064\n",
      "Epoch 11/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2196 - val_loss: 0.2016\n",
      "Epoch 12/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2170 - val_loss: 0.2071\n",
      "Epoch 13/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2190 - val_loss: 0.1994\n",
      "Epoch 14/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2130 - val_loss: 0.1984\n",
      "Epoch 15/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2136 - val_loss: 0.2027\n",
      "Epoch 16/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2069 - val_loss: 0.2067\n",
      "Epoch 17/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2091 - val_loss: 0.1990\n",
      "Epoch 18/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2136 - val_loss: 0.2020\n",
      "Epoch 19/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2138 - val_loss: 0.1968\n",
      "Epoch 20/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2113 - val_loss: 0.1929\n",
      "Epoch 21/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2079 - val_loss: 0.1931\n",
      "Epoch 22/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2090 - val_loss: 0.1901\n",
      "Epoch 23/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2068 - val_loss: 0.1895\n",
      "Epoch 24/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2007 - val_loss: 0.1941\n",
      "Epoch 25/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1965 - val_loss: 0.1955\n",
      "Epoch 26/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1961 - val_loss: 0.1871\n",
      "Epoch 27/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2011 - val_loss: 0.1832\n",
      "Epoch 28/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1954 - val_loss: 0.1811\n",
      "Epoch 29/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1927 - val_loss: 0.1852\n",
      "Epoch 30/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1935 - val_loss: 0.1812\n",
      "Epoch 31/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1928 - val_loss: 0.1906\n",
      "Epoch 32/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1869 - val_loss: 0.1842\n",
      "Epoch 33/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1876 - val_loss: 0.1811\n",
      "Epoch 34/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1853 - val_loss: 0.1782\n",
      "Epoch 35/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1834 - val_loss: 0.1815\n",
      "Epoch 36/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1825 - val_loss: 0.1794\n",
      "Epoch 37/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1847 - val_loss: 0.1771\n",
      "Epoch 38/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1824 - val_loss: 0.1784\n",
      "Epoch 39/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1870 - val_loss: 0.1758\n",
      "Epoch 40/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1815 - val_loss: 0.1777\n",
      "Epoch 41/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1812 - val_loss: 0.1810\n",
      "Epoch 42/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1783 - val_loss: 0.1741\n",
      "Epoch 43/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1792 - val_loss: 0.1744\n",
      "Epoch 44/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1770 - val_loss: 0.1799\n",
      "Epoch 45/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1763 - val_loss: 0.1775\n",
      "Epoch 46/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1744 - val_loss: 0.1761\n",
      "Epoch 47/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1755 - val_loss: 0.1730\n",
      "Epoch 48/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1752 - val_loss: 0.1748\n",
      "Epoch 49/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1815 - val_loss: 0.1757\n",
      "Epoch 50/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1721 - val_loss: 0.1732\n",
      "Epoch 51/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1736 - val_loss: 0.1719\n",
      "Epoch 52/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1767 - val_loss: 0.1724\n",
      "Epoch 53/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1732 - val_loss: 0.1715\n",
      "Epoch 54/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1715 - val_loss: 0.1756\n",
      "Epoch 55/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1646 - val_loss: 0.1720\n",
      "Epoch 56/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1733 - val_loss: 0.1748\n",
      "Epoch 57/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1695 - val_loss: 0.1712\n",
      "Epoch 58/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1684 - val_loss: 0.1695\n",
      "Epoch 59/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1631 - val_loss: 0.1706\n",
      "Epoch 60/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1662 - val_loss: 0.1689\n",
      "Epoch 61/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1633 - val_loss: 0.1740\n",
      "Epoch 62/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1637 - val_loss: 0.1685\n",
      "Epoch 63/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1655 - val_loss: 0.1707\n",
      "Epoch 64/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1641 - val_loss: 0.1716\n",
      "Epoch 65/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1654 - val_loss: 0.1683\n",
      "Epoch 66/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1683 - val_loss: 0.1705\n",
      "Epoch 67/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1621 - val_loss: 0.1724\n",
      "Epoch 68/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1641 - val_loss: 0.1672\n",
      "Epoch 69/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1642 - val_loss: 0.1730\n",
      "Epoch 70/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1652 - val_loss: 0.1719\n",
      "Epoch 71/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1636 - val_loss: 0.1677\n",
      "Epoch 72/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1638 - val_loss: 0.1685\n",
      "Epoch 73/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1653 - val_loss: 0.1673\n",
      "Epoch 74/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1508 - val_loss: 0.1691\n",
      "Epoch 75/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1609 - val_loss: 0.1701\n",
      "Epoch 76/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1575 - val_loss: 0.1686\n",
      "Epoch 77/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1571 - val_loss: 0.1699\n",
      "Epoch 78/150\n",
      "\u001b[1m337/337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1533 - val_loss: 0.1715\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1756\n"
     ]
    }
   ],
   "source": [
    "# Call the function to build the model, save the model, and store the results\n",
    "\n",
    "m = 'dummy_drop'\n",
    "\n",
    "models[m], results[m], model_history[m] = build_model(m)\n",
    "models[m].save(m + '.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
