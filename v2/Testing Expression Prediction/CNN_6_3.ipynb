{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "### PyTorch Hyperparameter Tuning (Bayesian Optimization)\n",
    "\n",
    "A hyperparameter is a parameter that can be set in order to define any configurable part of a model's learning process. For this CNN, the architecture hyperparameters we optimize are:\n",
    "- The number and structure of Conv1D layers.\n",
    "- Filter sizes, kernel sizes, and strides.\n",
    "- Max-pooling sizes and activation functions for each layer.\n",
    "- The size of the dense layer.\n",
    "- The learning rate for optimization.\n",
    "\n",
    "This approach uses PyTorch to make the sequence generation/infill easier.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Model chnages: higher TX/expression is now higher prediction.\n",
    "\n",
    "This version optimizes the process of training/testing and uses hyperparameter tuning. It uses a similar architecture to CNN_5_0. It does not include augmented data, just takes the data from La Fleur's supplemental materials including:\n",
    "- La Fleur et al (and De Novo Designs)\n",
    "- Urtecho et al\n",
    "- Hossain et al\n",
    "- Yu et al\n",
    "- Lagator (36N, Pl, and Pr)\n",
    "- Anderson Series\n",
    "\n",
    "We onehot encode each basepair and pad the whole sequence. Because we use a CNN which is designed to identify \"features,\" the input promoter can be any length (with padding) and the model will be able to accurately predict the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_6_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "# Documentation variables\n",
    "name = 'CNN_6_0'\n",
    "model_path = f'v2/Models/{name}.pt'\n",
    "data_dir = 'v2/Data/Train Test/'\n",
    "\n",
    "# Load and split the data\n",
    "X_train, y_train = load_features(f'{data_dir}train_data.csv')\n",
    "X_test, y_test = load_features(f'{data_dir}test_data.csv')\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "\n",
    "input_shape = (X_train.shape[0], X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter search\n",
    "best_params = hyperparameter_search(X_train, y_train, input_shape, epochs)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model\n",
    "model = PyTorchRegressor(input_shape, best_params, epochs=epochs)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "metrics = calc_metrics(y_test, y_pred)\n",
    "print(\"Performance Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(model, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
