{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Implement loss calculation based on deviation unmasked nucleotides. Removes mask as final layer.\n",
    "\n",
    "TODO:\n",
    "1. Change Loss to compare CNN(predicted_sequence) to CNN(actual_sequence) instead of CNN(predicted_sequence) to actual_expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RNN_2_7 as parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'RNN_2_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Data/combined/LaFleur_supp.csv'\n",
    "\n",
    "df, scaler = parent.load_and_preprocess_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequence, X_expressions, y = parent.preprocess_X_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequence_train, X_sequence_test, X_expressions_train, X_expressions_test, y_train, y_test = parent.train_test_split(\n",
    "        X_sequence, X_expressions, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = parent.load_model('../Models/CNN_5_0.keras')\n",
    "lstm_model = parent.build_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1 / 76, loss: 1.6316094398498535\n",
      "Epoch 1, batch 2 / 76, loss: 1.6453773975372314\n",
      "Epoch 1, batch 3 / 76, loss: 1.5743792057037354\n",
      "Epoch 1, batch 4 / 76, loss: 1.626101016998291\n",
      "Epoch 1, batch 5 / 76, loss: 1.6600706577301025\n",
      "Epoch 1, batch 6 / 76, loss: 1.6619415283203125\n",
      "Epoch 1, batch 7 / 76, loss: 1.6402313709259033\n",
      "Epoch 1, batch 8 / 76, loss: 1.6347935199737549\n",
      "Epoch 1, batch 9 / 76, loss: 1.591160774230957\n",
      "Epoch 1, batch 10 / 76, loss: 1.6684355735778809\n",
      "Epoch 1, batch 11 / 76, loss: 1.618983268737793\n",
      "Epoch 1, batch 12 / 76, loss: 1.6359546184539795\n",
      "Epoch 1, batch 13 / 76, loss: 1.6498713493347168\n",
      "Epoch 1, batch 14 / 76, loss: 1.6131044626235962\n",
      "Epoch 1, batch 15 / 76, loss: 1.6450469493865967\n",
      "Epoch 1, batch 16 / 76, loss: 1.6543729305267334\n",
      "Epoch 1, batch 17 / 76, loss: 1.6866686344146729\n",
      "Epoch 1, batch 18 / 76, loss: 1.6727005243301392\n",
      "Epoch 1, batch 19 / 76, loss: 1.6829757690429688\n",
      "Epoch 1, batch 20 / 76, loss: 1.6725096702575684\n",
      "Epoch 1, batch 21 / 76, loss: 1.7294466495513916\n",
      "Epoch 1, batch 22 / 76, loss: 1.736493706703186\n",
      "Epoch 1, batch 23 / 76, loss: 1.715773105621338\n",
      "Epoch 1, batch 24 / 76, loss: 1.724656581878662\n",
      "Epoch 1, batch 25 / 76, loss: 1.7021396160125732\n",
      "Epoch 1, batch 26 / 76, loss: 1.7481266260147095\n",
      "Epoch 1, batch 27 / 76, loss: 1.7263314723968506\n",
      "Epoch 1, batch 28 / 76, loss: 1.7430893182754517\n",
      "Epoch 1, batch 29 / 76, loss: 1.7771735191345215\n",
      "Epoch 1, batch 30 / 76, loss: 1.8066251277923584\n",
      "Epoch 1, batch 31 / 76, loss: 1.708678960800171\n",
      "Epoch 1, batch 32 / 76, loss: 1.7376511096954346\n",
      "Epoch 1, batch 33 / 76, loss: 1.78122878074646\n",
      "Epoch 1, batch 34 / 76, loss: 1.7750922441482544\n",
      "Epoch 1, batch 35 / 76, loss: 1.7717370986938477\n",
      "Epoch 1, batch 36 / 76, loss: 1.7337934970855713\n",
      "Epoch 1, batch 37 / 76, loss: 1.7780390977859497\n",
      "Epoch 1, batch 38 / 76, loss: 1.7593227624893188\n",
      "Epoch 1, batch 39 / 76, loss: 1.8085708618164062\n",
      "Epoch 1, batch 40 / 76, loss: 1.7580666542053223\n",
      "Epoch 1, batch 41 / 76, loss: 1.7506433725357056\n",
      "Epoch 1, batch 42 / 76, loss: 1.7561570405960083\n",
      "Epoch 1, batch 43 / 76, loss: 1.7091195583343506\n",
      "Epoch 1, batch 44 / 76, loss: 1.7405579090118408\n",
      "Epoch 1, batch 45 / 76, loss: 1.7355067729949951\n",
      "Epoch 1, batch 46 / 76, loss: 1.7918460369110107\n",
      "Epoch 1, batch 47 / 76, loss: 1.7586243152618408\n",
      "Epoch 1, batch 48 / 76, loss: 1.7516002655029297\n",
      "Epoch 1, batch 49 / 76, loss: 1.7699363231658936\n",
      "Epoch 1, batch 50 / 76, loss: 1.7515640258789062\n",
      "Epoch 1, batch 51 / 76, loss: 1.8523509502410889\n",
      "Epoch 1, batch 52 / 76, loss: 1.8126665353775024\n",
      "Epoch 1, batch 53 / 76, loss: 1.7249836921691895\n",
      "Epoch 1, batch 54 / 76, loss: 1.8067952394485474\n",
      "Epoch 1, batch 55 / 76, loss: 1.7743984460830688\n",
      "Epoch 1, batch 56 / 76, loss: 1.730116844177246\n",
      "Epoch 1, batch 57 / 76, loss: 1.764117956161499\n",
      "Epoch 1, batch 58 / 76, loss: 1.8309162855148315\n",
      "Epoch 1, batch 59 / 76, loss: 1.790710687637329\n",
      "Epoch 1, batch 60 / 76, loss: 1.7620353698730469\n",
      "Epoch 1, batch 61 / 76, loss: 1.7149606943130493\n",
      "Epoch 1, batch 62 / 76, loss: 1.7554514408111572\n",
      "Epoch 1, batch 63 / 76, loss: 1.6913349628448486\n",
      "Epoch 1, batch 64 / 76, loss: 1.7463138103485107\n",
      "Epoch 1, batch 65 / 76, loss: 1.7518746852874756\n",
      "Epoch 1, batch 66 / 76, loss: 1.7044944763183594\n",
      "Epoch 1, batch 67 / 76, loss: 1.7695703506469727\n",
      "Epoch 1, batch 68 / 76, loss: 1.8188822269439697\n",
      "Epoch 1, batch 69 / 76, loss: 1.744929313659668\n",
      "Epoch 1, batch 70 / 76, loss: 1.7500267028808594\n",
      "Epoch 1, batch 71 / 76, loss: 1.7404956817626953\n",
      "Epoch 1, batch 72 / 76, loss: 1.795310378074646\n",
      "Epoch 1, batch 73 / 76, loss: 1.775578498840332\n",
      "Epoch 1, batch 74 / 76, loss: 1.7402358055114746\n",
      "Epoch 1, batch 75 / 76, loss: 1.7003154754638672\n",
      "Epoch 1, batch 76 / 76, loss: 1.7047439813613892\n",
      "Epoch 1, batch 77 / 76, loss: 1.840097427368164\n",
      "Epoch 2, batch 1 / 76, loss: 1.7740809917449951\n",
      "Epoch 2, batch 2 / 76, loss: 1.7540667057037354\n",
      "Epoch 2, batch 3 / 76, loss: 1.717811942100525\n",
      "Epoch 2, batch 4 / 76, loss: 1.7455580234527588\n",
      "Epoch 2, batch 5 / 76, loss: 1.683558464050293\n",
      "Epoch 2, batch 6 / 76, loss: 1.7200472354888916\n",
      "Epoch 2, batch 7 / 76, loss: 1.8076825141906738\n",
      "Epoch 2, batch 8 / 76, loss: 1.7845368385314941\n",
      "Epoch 2, batch 9 / 76, loss: 1.7694816589355469\n",
      "Epoch 2, batch 10 / 76, loss: 1.8285801410675049\n",
      "Epoch 2, batch 11 / 76, loss: 1.7566447257995605\n",
      "Epoch 2, batch 12 / 76, loss: 1.7822206020355225\n",
      "Epoch 2, batch 13 / 76, loss: 1.7443206310272217\n",
      "Epoch 2, batch 14 / 76, loss: 1.7646735906600952\n",
      "Epoch 2, batch 15 / 76, loss: 1.730194330215454\n",
      "Epoch 2, batch 16 / 76, loss: 1.7331032752990723\n",
      "Epoch 2, batch 17 / 76, loss: 1.7114529609680176\n",
      "Epoch 2, batch 18 / 76, loss: 1.7368600368499756\n",
      "Epoch 2, batch 19 / 76, loss: 1.664474368095398\n",
      "Epoch 2, batch 20 / 76, loss: 1.6781644821166992\n",
      "Epoch 2, batch 21 / 76, loss: 1.744206190109253\n",
      "Epoch 2, batch 22 / 76, loss: 1.7078611850738525\n",
      "Epoch 2, batch 23 / 76, loss: 1.722630500793457\n",
      "Epoch 2, batch 24 / 76, loss: 1.7726365327835083\n",
      "Epoch 2, batch 25 / 76, loss: 1.761146068572998\n",
      "Epoch 2, batch 26 / 76, loss: 1.8015215396881104\n",
      "Epoch 2, batch 27 / 76, loss: 1.7675516605377197\n",
      "Epoch 2, batch 28 / 76, loss: 1.7405824661254883\n",
      "Epoch 2, batch 29 / 76, loss: 1.7604694366455078\n",
      "Epoch 2, batch 30 / 76, loss: 1.7764947414398193\n",
      "Epoch 2, batch 31 / 76, loss: 1.7513877153396606\n",
      "Epoch 2, batch 32 / 76, loss: 1.7460397481918335\n",
      "Epoch 2, batch 33 / 76, loss: 1.752873182296753\n",
      "Epoch 2, batch 34 / 76, loss: 1.7349510192871094\n",
      "Epoch 2, batch 35 / 76, loss: 1.7205004692077637\n",
      "Epoch 2, batch 36 / 76, loss: 1.7860352993011475\n",
      "Epoch 2, batch 37 / 76, loss: 1.7936853170394897\n",
      "Epoch 2, batch 38 / 76, loss: 1.7427927255630493\n",
      "Epoch 2, batch 39 / 76, loss: 1.7990437746047974\n",
      "Epoch 2, batch 40 / 76, loss: 1.7536424398422241\n",
      "Epoch 2, batch 41 / 76, loss: 1.7007267475128174\n",
      "Epoch 2, batch 42 / 76, loss: 1.7600245475769043\n",
      "Epoch 2, batch 43 / 76, loss: 1.7254539728164673\n",
      "Epoch 2, batch 44 / 76, loss: 1.7668869495391846\n",
      "Epoch 2, batch 45 / 76, loss: 1.7029554843902588\n",
      "Epoch 2, batch 46 / 76, loss: 1.791603684425354\n",
      "Epoch 2, batch 47 / 76, loss: 1.707908034324646\n",
      "Epoch 2, batch 48 / 76, loss: 1.6730597019195557\n",
      "Epoch 2, batch 49 / 76, loss: 1.7590769529342651\n",
      "Epoch 2, batch 50 / 76, loss: 1.7321243286132812\n",
      "Epoch 2, batch 51 / 76, loss: 1.713992714881897\n",
      "Epoch 2, batch 52 / 76, loss: 1.7178761959075928\n",
      "Epoch 2, batch 53 / 76, loss: 1.7856433391571045\n",
      "Epoch 2, batch 54 / 76, loss: 1.7929270267486572\n",
      "Epoch 2, batch 55 / 76, loss: 1.7260187864303589\n",
      "Epoch 2, batch 56 / 76, loss: 1.680142879486084\n",
      "Epoch 2, batch 57 / 76, loss: 1.7516957521438599\n",
      "Epoch 2, batch 58 / 76, loss: 1.7756636142730713\n",
      "Epoch 2, batch 59 / 76, loss: 1.6497323513031006\n",
      "Epoch 2, batch 60 / 76, loss: 1.7311179637908936\n",
      "Epoch 2, batch 61 / 76, loss: 1.7289201021194458\n",
      "Epoch 2, batch 62 / 76, loss: 1.6813437938690186\n",
      "Epoch 2, batch 63 / 76, loss: 1.7267661094665527\n",
      "Epoch 2, batch 64 / 76, loss: 1.686562180519104\n",
      "Epoch 2, batch 65 / 76, loss: 1.735726237297058\n",
      "Epoch 2, batch 66 / 76, loss: 1.738628625869751\n",
      "Epoch 2, batch 67 / 76, loss: 1.6646361351013184\n",
      "Epoch 2, batch 68 / 76, loss: 1.730323314666748\n",
      "Epoch 2, batch 69 / 76, loss: 1.6908838748931885\n",
      "Epoch 2, batch 70 / 76, loss: 1.6954240798950195\n",
      "Epoch 2, batch 71 / 76, loss: 1.713526964187622\n",
      "Epoch 2, batch 72 / 76, loss: 1.7069809436798096\n",
      "Epoch 2, batch 73 / 76, loss: 1.633507251739502\n",
      "Epoch 2, batch 74 / 76, loss: 1.7127525806427002\n",
      "Epoch 2, batch 75 / 76, loss: 1.7323284149169922\n",
      "Epoch 2, batch 76 / 76, loss: 1.745528221130371\n",
      "Epoch 2, batch 77 / 76, loss: 1.5529423952102661\n",
      "Epoch 3, batch 1 / 76, loss: 1.6266508102416992\n",
      "Epoch 3, batch 2 / 76, loss: 1.5835834741592407\n",
      "Epoch 3, batch 3 / 76, loss: 1.73095703125\n",
      "Epoch 3, batch 4 / 76, loss: 1.6688448190689087\n",
      "Epoch 3, batch 5 / 76, loss: 1.588789463043213\n",
      "Epoch 3, batch 6 / 76, loss: 1.625746488571167\n",
      "Epoch 3, batch 7 / 76, loss: 1.6640187501907349\n",
      "Epoch 3, batch 8 / 76, loss: 1.6164500713348389\n",
      "Epoch 3, batch 9 / 76, loss: 1.6797822713851929\n",
      "Epoch 3, batch 10 / 76, loss: 1.6494183540344238\n",
      "Epoch 3, batch 11 / 76, loss: 1.6175155639648438\n",
      "Epoch 3, batch 12 / 76, loss: 1.692091703414917\n",
      "Epoch 3, batch 13 / 76, loss: 1.7002633810043335\n",
      "Epoch 3, batch 14 / 76, loss: 1.6314098834991455\n",
      "Epoch 3, batch 15 / 76, loss: 1.6758522987365723\n",
      "Epoch 3, batch 16 / 76, loss: 1.636209487915039\n",
      "Epoch 3, batch 17 / 76, loss: 1.6044964790344238\n",
      "Epoch 3, batch 18 / 76, loss: 1.6301178932189941\n",
      "Epoch 3, batch 19 / 76, loss: 1.6648564338684082\n",
      "Epoch 3, batch 20 / 76, loss: 1.5942034721374512\n",
      "Epoch 3, batch 21 / 76, loss: 1.6863987445831299\n",
      "Epoch 3, batch 22 / 76, loss: 1.5968254804611206\n",
      "Epoch 3, batch 23 / 76, loss: 1.619797706604004\n",
      "Epoch 3, batch 24 / 76, loss: 1.6241440773010254\n",
      "Epoch 3, batch 25 / 76, loss: 1.6149088144302368\n",
      "Epoch 3, batch 26 / 76, loss: 1.6483955383300781\n",
      "Epoch 3, batch 27 / 76, loss: 1.586280345916748\n",
      "Epoch 3, batch 28 / 76, loss: 1.5924568176269531\n",
      "Epoch 3, batch 29 / 76, loss: 1.7082691192626953\n",
      "Epoch 3, batch 30 / 76, loss: 1.6521756649017334\n",
      "Epoch 3, batch 31 / 76, loss: 1.6441822052001953\n",
      "Epoch 3, batch 32 / 76, loss: 1.669097900390625\n",
      "Epoch 3, batch 33 / 76, loss: 1.7140583992004395\n",
      "Epoch 3, batch 34 / 76, loss: 1.6701607704162598\n",
      "Epoch 3, batch 35 / 76, loss: 1.6926541328430176\n",
      "Epoch 3, batch 36 / 76, loss: 1.623283863067627\n",
      "Epoch 3, batch 37 / 76, loss: 1.6333956718444824\n",
      "Epoch 3, batch 38 / 76, loss: 1.601259708404541\n",
      "Epoch 3, batch 39 / 76, loss: 1.622058629989624\n",
      "Epoch 3, batch 40 / 76, loss: 1.5861940383911133\n",
      "Epoch 3, batch 41 / 76, loss: 1.6173977851867676\n",
      "Epoch 3, batch 42 / 76, loss: 1.6637043952941895\n",
      "Epoch 3, batch 43 / 76, loss: 1.64046311378479\n",
      "Epoch 3, batch 44 / 76, loss: 1.6576364040374756\n",
      "Epoch 3, batch 45 / 76, loss: 1.6282292604446411\n",
      "Epoch 3, batch 46 / 76, loss: 1.5567748546600342\n",
      "Epoch 3, batch 47 / 76, loss: 1.6395082473754883\n",
      "Epoch 3, batch 48 / 76, loss: 1.6006038188934326\n",
      "Epoch 3, batch 49 / 76, loss: 1.6365076303482056\n",
      "Epoch 3, batch 50 / 76, loss: 1.6510727405548096\n",
      "Epoch 3, batch 51 / 76, loss: 1.5872799158096313\n",
      "Epoch 3, batch 52 / 76, loss: 1.5537540912628174\n",
      "Epoch 3, batch 53 / 76, loss: 1.6903882026672363\n",
      "Epoch 3, batch 54 / 76, loss: 1.6275275945663452\n",
      "Epoch 3, batch 55 / 76, loss: 1.6236677169799805\n",
      "Epoch 3, batch 56 / 76, loss: 1.5207123756408691\n",
      "Epoch 3, batch 57 / 76, loss: 1.5560081005096436\n",
      "Epoch 3, batch 58 / 76, loss: 1.5383234024047852\n",
      "Epoch 3, batch 59 / 76, loss: 1.6153357028961182\n",
      "Epoch 3, batch 60 / 76, loss: 1.540960669517517\n",
      "Epoch 3, batch 61 / 76, loss: 1.562703013420105\n",
      "Epoch 3, batch 62 / 76, loss: 1.6249566078186035\n",
      "Epoch 3, batch 63 / 76, loss: 1.5214436054229736\n",
      "Epoch 3, batch 64 / 76, loss: 1.6049349308013916\n",
      "Epoch 3, batch 65 / 76, loss: 1.5492595434188843\n",
      "Epoch 3, batch 66 / 76, loss: 1.5969526767730713\n",
      "Epoch 3, batch 67 / 76, loss: 1.5798718929290771\n",
      "Epoch 3, batch 68 / 76, loss: 1.540140151977539\n",
      "Epoch 3, batch 69 / 76, loss: 1.5891335010528564\n",
      "Epoch 3, batch 70 / 76, loss: 1.5135493278503418\n",
      "Epoch 3, batch 71 / 76, loss: 1.5570935010910034\n",
      "Epoch 3, batch 72 / 76, loss: 1.5800669193267822\n",
      "Epoch 3, batch 73 / 76, loss: 1.5383105278015137\n",
      "Epoch 3, batch 74 / 76, loss: 1.5850043296813965\n",
      "Epoch 3, batch 75 / 76, loss: 1.5586369037628174\n",
      "Epoch 3, batch 76 / 76, loss: 1.5583784580230713\n",
      "Epoch 3, batch 77 / 76, loss: 1.6797094345092773\n",
      "Epoch 4, batch 1 / 76, loss: 1.5369229316711426\n",
      "Epoch 4, batch 2 / 76, loss: 1.5685482025146484\n",
      "Epoch 4, batch 3 / 76, loss: 1.5531787872314453\n",
      "Epoch 4, batch 4 / 76, loss: 1.5320653915405273\n",
      "Epoch 4, batch 5 / 76, loss: 1.5338761806488037\n",
      "Epoch 4, batch 6 / 76, loss: 1.5088467597961426\n",
      "Epoch 4, batch 7 / 76, loss: 1.629332423210144\n",
      "Epoch 4, batch 8 / 76, loss: 1.5771784782409668\n",
      "Epoch 4, batch 9 / 76, loss: 1.6033754348754883\n",
      "Epoch 4, batch 10 / 76, loss: 1.5059605836868286\n",
      "Epoch 4, batch 11 / 76, loss: 1.588752269744873\n",
      "Epoch 4, batch 12 / 76, loss: 1.5551910400390625\n",
      "Epoch 4, batch 13 / 76, loss: 1.5603421926498413\n",
      "Epoch 4, batch 14 / 76, loss: 1.5801973342895508\n",
      "Epoch 4, batch 15 / 76, loss: 1.5267417430877686\n",
      "Epoch 4, batch 16 / 76, loss: 1.4540821313858032\n",
      "Epoch 4, batch 17 / 76, loss: 1.5603944063186646\n",
      "Epoch 4, batch 18 / 76, loss: 1.5762696266174316\n",
      "Epoch 4, batch 19 / 76, loss: 1.618422031402588\n",
      "Epoch 4, batch 20 / 76, loss: 1.5778385400772095\n",
      "Epoch 4, batch 21 / 76, loss: 1.5665825605392456\n",
      "Epoch 4, batch 22 / 76, loss: 1.5624065399169922\n",
      "Epoch 4, batch 23 / 76, loss: 1.5408446788787842\n",
      "Epoch 4, batch 24 / 76, loss: 1.531029224395752\n",
      "Epoch 4, batch 25 / 76, loss: 1.4955997467041016\n",
      "Epoch 4, batch 26 / 76, loss: 1.550365686416626\n",
      "Epoch 4, batch 27 / 76, loss: 1.504108190536499\n",
      "Epoch 4, batch 28 / 76, loss: 1.5151350498199463\n",
      "Epoch 4, batch 29 / 76, loss: 1.5625739097595215\n",
      "Epoch 4, batch 30 / 76, loss: 1.5640103816986084\n",
      "Epoch 4, batch 31 / 76, loss: 1.498305320739746\n",
      "Epoch 4, batch 32 / 76, loss: 1.5621379613876343\n",
      "Epoch 4, batch 33 / 76, loss: 1.5184719562530518\n",
      "Epoch 4, batch 34 / 76, loss: 1.527855634689331\n",
      "Epoch 4, batch 35 / 76, loss: 1.5969712734222412\n",
      "Epoch 4, batch 36 / 76, loss: 1.5883815288543701\n",
      "Epoch 4, batch 37 / 76, loss: 1.570692539215088\n",
      "Epoch 4, batch 38 / 76, loss: 1.5086499452590942\n",
      "Epoch 4, batch 39 / 76, loss: 1.5193672180175781\n",
      "Epoch 4, batch 40 / 76, loss: 1.575670599937439\n",
      "Epoch 4, batch 41 / 76, loss: 1.6261495351791382\n",
      "Epoch 4, batch 42 / 76, loss: 1.5600674152374268\n",
      "Epoch 4, batch 43 / 76, loss: 1.533134937286377\n",
      "Epoch 4, batch 44 / 76, loss: 1.5421133041381836\n",
      "Epoch 4, batch 45 / 76, loss: 1.5551137924194336\n",
      "Epoch 4, batch 46 / 76, loss: 1.5053904056549072\n",
      "Epoch 4, batch 47 / 76, loss: 1.528787612915039\n",
      "Epoch 4, batch 48 / 76, loss: 1.504857063293457\n",
      "Epoch 4, batch 49 / 76, loss: 1.558786153793335\n",
      "Epoch 4, batch 50 / 76, loss: 1.5880584716796875\n",
      "Epoch 4, batch 51 / 76, loss: 1.5698378086090088\n",
      "Epoch 4, batch 52 / 76, loss: 1.4817334413528442\n",
      "Epoch 4, batch 53 / 76, loss: 1.5703513622283936\n",
      "Epoch 4, batch 54 / 76, loss: 1.5302648544311523\n",
      "Epoch 4, batch 55 / 76, loss: 1.590226650238037\n",
      "Epoch 4, batch 56 / 76, loss: 1.4538979530334473\n",
      "Epoch 4, batch 57 / 76, loss: 1.4752219915390015\n",
      "Epoch 4, batch 58 / 76, loss: 1.5283281803131104\n",
      "Epoch 4, batch 59 / 76, loss: 1.5151278972625732\n",
      "Epoch 4, batch 60 / 76, loss: 1.630946397781372\n",
      "Epoch 4, batch 61 / 76, loss: 1.540589690208435\n",
      "Epoch 4, batch 62 / 76, loss: 1.5448012351989746\n",
      "Epoch 4, batch 63 / 76, loss: 1.4853816032409668\n",
      "Epoch 4, batch 64 / 76, loss: 1.5468063354492188\n",
      "Epoch 4, batch 65 / 76, loss: 1.5627192258834839\n",
      "Epoch 4, batch 66 / 76, loss: 1.4700721502304077\n",
      "Epoch 4, batch 67 / 76, loss: 1.557886004447937\n",
      "Epoch 4, batch 68 / 76, loss: 1.4985225200653076\n",
      "Epoch 4, batch 69 / 76, loss: 1.5409541130065918\n",
      "Epoch 4, batch 70 / 76, loss: 1.494877815246582\n",
      "Epoch 4, batch 71 / 76, loss: 1.5404736995697021\n",
      "Epoch 4, batch 72 / 76, loss: 1.589507818222046\n",
      "Epoch 4, batch 73 / 76, loss: 1.5709152221679688\n",
      "Epoch 4, batch 74 / 76, loss: 1.5781280994415283\n",
      "Epoch 4, batch 75 / 76, loss: 1.4767274856567383\n",
      "Epoch 4, batch 76 / 76, loss: 1.5582685470581055\n",
      "Epoch 4, batch 77 / 76, loss: 1.4993278980255127\n",
      "Epoch 5, batch 1 / 76, loss: 1.524026870727539\n",
      "Epoch 5, batch 2 / 76, loss: 1.533576488494873\n",
      "Epoch 5, batch 3 / 76, loss: 1.5249170064926147\n",
      "Epoch 5, batch 4 / 76, loss: 1.5658042430877686\n",
      "Epoch 5, batch 5 / 76, loss: 1.5544977188110352\n",
      "Epoch 5, batch 6 / 76, loss: 1.548158884048462\n",
      "Epoch 5, batch 7 / 76, loss: 1.5084445476531982\n",
      "Epoch 5, batch 8 / 76, loss: 1.4747071266174316\n",
      "Epoch 5, batch 9 / 76, loss: 1.5944498777389526\n",
      "Epoch 5, batch 10 / 76, loss: 1.546476125717163\n",
      "Epoch 5, batch 11 / 76, loss: 1.4968249797821045\n",
      "Epoch 5, batch 12 / 76, loss: 1.5510177612304688\n",
      "Epoch 5, batch 13 / 76, loss: 1.5372167825698853\n",
      "Epoch 5, batch 14 / 76, loss: 1.5269874334335327\n",
      "Epoch 5, batch 15 / 76, loss: 1.6052675247192383\n",
      "Epoch 5, batch 16 / 76, loss: 1.5245667695999146\n",
      "Epoch 5, batch 17 / 76, loss: 1.6148604154586792\n",
      "Epoch 5, batch 18 / 76, loss: 1.5920259952545166\n",
      "Epoch 5, batch 19 / 76, loss: 1.5614920854568481\n",
      "Epoch 5, batch 20 / 76, loss: 1.486107587814331\n",
      "Epoch 5, batch 21 / 76, loss: 1.5476899147033691\n",
      "Epoch 5, batch 22 / 76, loss: 1.531092643737793\n",
      "Epoch 5, batch 23 / 76, loss: 1.577495813369751\n",
      "Epoch 5, batch 24 / 76, loss: 1.5342967510223389\n",
      "Epoch 5, batch 25 / 76, loss: 1.6257615089416504\n",
      "Epoch 5, batch 26 / 76, loss: 1.5349313020706177\n",
      "Epoch 5, batch 27 / 76, loss: 1.5651798248291016\n",
      "Epoch 5, batch 28 / 76, loss: 1.4905962944030762\n",
      "Epoch 5, batch 29 / 76, loss: 1.5518269538879395\n",
      "Epoch 5, batch 30 / 76, loss: 1.596656084060669\n",
      "Epoch 5, batch 31 / 76, loss: 1.5459785461425781\n",
      "Epoch 5, batch 32 / 76, loss: 1.5668424367904663\n",
      "Epoch 5, batch 33 / 76, loss: 1.5706435441970825\n",
      "Epoch 5, batch 34 / 76, loss: 1.6050972938537598\n",
      "Epoch 5, batch 35 / 76, loss: 1.5390249490737915\n",
      "Epoch 5, batch 36 / 76, loss: 1.611332654953003\n",
      "Epoch 5, batch 37 / 76, loss: 1.5048656463623047\n",
      "Epoch 5, batch 38 / 76, loss: 1.5793462991714478\n",
      "Epoch 5, batch 39 / 76, loss: 1.6099514961242676\n",
      "Epoch 5, batch 40 / 76, loss: 1.5333714485168457\n",
      "Epoch 5, batch 41 / 76, loss: 1.6266345977783203\n",
      "Epoch 5, batch 42 / 76, loss: 1.5351611375808716\n",
      "Epoch 5, batch 43 / 76, loss: 1.6066324710845947\n",
      "Epoch 5, batch 44 / 76, loss: 1.5900955200195312\n",
      "Epoch 5, batch 45 / 76, loss: 1.639763593673706\n",
      "Epoch 5, batch 46 / 76, loss: 1.5617704391479492\n",
      "Epoch 5, batch 47 / 76, loss: 1.5886104106903076\n",
      "Epoch 5, batch 48 / 76, loss: 1.5474188327789307\n",
      "Epoch 5, batch 49 / 76, loss: 1.6254847049713135\n",
      "Epoch 5, batch 50 / 76, loss: 1.5561435222625732\n",
      "Epoch 5, batch 51 / 76, loss: 1.5931243896484375\n",
      "Epoch 5, batch 52 / 76, loss: 1.594850778579712\n",
      "Epoch 5, batch 53 / 76, loss: 1.6042895317077637\n",
      "Epoch 5, batch 54 / 76, loss: 1.6021065711975098\n",
      "Epoch 5, batch 55 / 76, loss: 1.5386804342269897\n",
      "Epoch 5, batch 56 / 76, loss: 1.6085288524627686\n",
      "Epoch 5, batch 57 / 76, loss: 1.6216869354248047\n",
      "Epoch 5, batch 58 / 76, loss: 1.6449804306030273\n",
      "Epoch 5, batch 59 / 76, loss: 1.5676618814468384\n",
      "Epoch 5, batch 60 / 76, loss: 1.582515835762024\n",
      "Epoch 5, batch 61 / 76, loss: 1.5588431358337402\n",
      "Epoch 5, batch 62 / 76, loss: 1.6242531538009644\n",
      "Epoch 5, batch 63 / 76, loss: 1.558087706565857\n",
      "Epoch 5, batch 64 / 76, loss: 1.593684434890747\n",
      "Epoch 5, batch 65 / 76, loss: 1.591038703918457\n",
      "Epoch 5, batch 66 / 76, loss: 1.5904157161712646\n",
      "Epoch 5, batch 67 / 76, loss: 1.5761877298355103\n",
      "Epoch 5, batch 68 / 76, loss: 1.5703606605529785\n",
      "Epoch 5, batch 69 / 76, loss: 1.5026408433914185\n",
      "Epoch 5, batch 70 / 76, loss: 1.5637236833572388\n",
      "Epoch 5, batch 71 / 76, loss: 1.5739474296569824\n",
      "Epoch 5, batch 72 / 76, loss: 1.5479276180267334\n",
      "Epoch 5, batch 73 / 76, loss: 1.5512793064117432\n",
      "Epoch 5, batch 74 / 76, loss: 1.535595178604126\n",
      "Epoch 5, batch 75 / 76, loss: 1.615494728088379\n",
      "Epoch 5, batch 76 / 76, loss: 1.540717601776123\n",
      "Epoch 5, batch 77 / 76, loss: 1.5454444885253906\n",
      "Epoch 6, batch 1 / 76, loss: 1.5718791484832764\n",
      "Epoch 6, batch 2 / 76, loss: 1.5666399002075195\n",
      "Epoch 6, batch 3 / 76, loss: 1.5592100620269775\n",
      "Epoch 6, batch 4 / 76, loss: 1.549367070198059\n",
      "Epoch 6, batch 5 / 76, loss: 1.5964691638946533\n",
      "Epoch 6, batch 6 / 76, loss: 1.5172414779663086\n",
      "Epoch 6, batch 7 / 76, loss: 1.5671195983886719\n",
      "Epoch 6, batch 8 / 76, loss: 1.6277893781661987\n",
      "Epoch 6, batch 9 / 76, loss: 1.5881155729293823\n",
      "Epoch 6, batch 10 / 76, loss: 1.5432448387145996\n",
      "Epoch 6, batch 11 / 76, loss: 1.5425913333892822\n",
      "Epoch 6, batch 12 / 76, loss: 1.5985445976257324\n",
      "Epoch 6, batch 13 / 76, loss: 1.5878033638000488\n",
      "Epoch 6, batch 14 / 76, loss: 1.5300958156585693\n",
      "Epoch 6, batch 15 / 76, loss: 1.5841833353042603\n",
      "Epoch 6, batch 16 / 76, loss: 1.5740095376968384\n",
      "Epoch 6, batch 17 / 76, loss: 1.5231982469558716\n",
      "Epoch 6, batch 18 / 76, loss: 1.5652475357055664\n",
      "Epoch 6, batch 19 / 76, loss: 1.5555497407913208\n",
      "Epoch 6, batch 20 / 76, loss: 1.5229480266571045\n",
      "Epoch 6, batch 21 / 76, loss: 1.5422303676605225\n",
      "Epoch 6, batch 22 / 76, loss: 1.5480878353118896\n",
      "Epoch 6, batch 23 / 76, loss: 1.502593994140625\n",
      "Epoch 6, batch 24 / 76, loss: 1.5202068090438843\n",
      "Epoch 6, batch 25 / 76, loss: 1.5834654569625854\n",
      "Epoch 6, batch 26 / 76, loss: 1.5370044708251953\n",
      "Epoch 6, batch 27 / 76, loss: 1.4947912693023682\n",
      "Epoch 6, batch 28 / 76, loss: 1.551681399345398\n",
      "Epoch 6, batch 29 / 76, loss: 1.5177812576293945\n",
      "Epoch 6, batch 30 / 76, loss: 1.4918711185455322\n",
      "Epoch 6, batch 31 / 76, loss: 1.5560994148254395\n",
      "Epoch 6, batch 32 / 76, loss: 1.5461950302124023\n",
      "Epoch 6, batch 33 / 76, loss: 1.5281099081039429\n",
      "Epoch 6, batch 34 / 76, loss: 1.5741095542907715\n",
      "Epoch 6, batch 35 / 76, loss: 1.5178415775299072\n",
      "Epoch 6, batch 36 / 76, loss: 1.4468779563903809\n",
      "Epoch 6, batch 37 / 76, loss: 1.5422439575195312\n",
      "Epoch 6, batch 38 / 76, loss: 1.5451877117156982\n",
      "Epoch 6, batch 39 / 76, loss: 1.5976462364196777\n",
      "Epoch 6, batch 40 / 76, loss: 1.542536735534668\n",
      "Epoch 6, batch 41 / 76, loss: 1.5261156558990479\n",
      "Epoch 6, batch 42 / 76, loss: 1.5265517234802246\n",
      "Epoch 6, batch 43 / 76, loss: 1.5048439502716064\n",
      "Epoch 6, batch 44 / 76, loss: 1.5683104991912842\n",
      "Epoch 6, batch 45 / 76, loss: 1.5392110347747803\n",
      "Epoch 6, batch 46 / 76, loss: 1.5310099124908447\n",
      "Epoch 6, batch 47 / 76, loss: 1.5860072374343872\n",
      "Epoch 6, batch 48 / 76, loss: 1.5344195365905762\n",
      "Epoch 6, batch 49 / 76, loss: 1.5492167472839355\n",
      "Epoch 6, batch 50 / 76, loss: 1.5576071739196777\n",
      "Epoch 6, batch 51 / 76, loss: 1.516876220703125\n",
      "Epoch 6, batch 52 / 76, loss: 1.5472416877746582\n",
      "Epoch 6, batch 53 / 76, loss: 1.5477133989334106\n",
      "Epoch 6, batch 54 / 76, loss: 1.501526951789856\n",
      "Epoch 6, batch 55 / 76, loss: 1.4945650100708008\n",
      "Epoch 6, batch 56 / 76, loss: 1.534483790397644\n",
      "Epoch 6, batch 57 / 76, loss: 1.5928887128829956\n",
      "Epoch 6, batch 58 / 76, loss: 1.5595602989196777\n",
      "Epoch 6, batch 59 / 76, loss: 1.5040876865386963\n",
      "Epoch 6, batch 60 / 76, loss: 1.5297462940216064\n",
      "Epoch 6, batch 61 / 76, loss: 1.4927961826324463\n",
      "Epoch 6, batch 62 / 76, loss: 1.481934905052185\n",
      "Epoch 6, batch 63 / 76, loss: 1.4614741802215576\n",
      "Epoch 6, batch 64 / 76, loss: 1.5734584331512451\n",
      "Epoch 6, batch 65 / 76, loss: 1.5215873718261719\n",
      "Epoch 6, batch 66 / 76, loss: 1.5869426727294922\n",
      "Epoch 6, batch 67 / 76, loss: 1.532958745956421\n",
      "Epoch 6, batch 68 / 76, loss: 1.4850549697875977\n",
      "Epoch 6, batch 69 / 76, loss: 1.569047451019287\n",
      "Epoch 6, batch 70 / 76, loss: 1.5361186265945435\n",
      "Epoch 6, batch 71 / 76, loss: 1.5783758163452148\n",
      "Epoch 6, batch 72 / 76, loss: 1.5020722150802612\n",
      "Epoch 6, batch 73 / 76, loss: 1.5526926517486572\n",
      "Epoch 6, batch 74 / 76, loss: 1.5488786697387695\n",
      "Epoch 6, batch 75 / 76, loss: 1.5314357280731201\n",
      "Epoch 6, batch 76 / 76, loss: 1.4914891719818115\n",
      "Epoch 6, batch 77 / 76, loss: 1.4778006076812744\n",
      "Epoch 7, batch 1 / 76, loss: 1.5603172779083252\n",
      "Epoch 7, batch 2 / 76, loss: 1.4860211610794067\n",
      "Epoch 7, batch 3 / 76, loss: 1.5046570301055908\n",
      "Epoch 7, batch 4 / 76, loss: 1.5565071105957031\n",
      "Epoch 7, batch 5 / 76, loss: 1.4812079668045044\n",
      "Epoch 7, batch 6 / 76, loss: 1.5466312170028687\n",
      "Epoch 7, batch 7 / 76, loss: 1.5508544445037842\n",
      "Epoch 7, batch 8 / 76, loss: 1.5343875885009766\n",
      "Epoch 7, batch 9 / 76, loss: 1.4523650407791138\n",
      "Epoch 7, batch 10 / 76, loss: 1.5010850429534912\n",
      "Epoch 7, batch 11 / 76, loss: 1.4584953784942627\n",
      "Epoch 7, batch 12 / 76, loss: 1.5269724130630493\n",
      "Epoch 7, batch 13 / 76, loss: 1.5529389381408691\n",
      "Epoch 7, batch 14 / 76, loss: 1.552293062210083\n",
      "Epoch 7, batch 15 / 76, loss: 1.444258213043213\n",
      "Epoch 7, batch 16 / 76, loss: 1.5424094200134277\n",
      "Epoch 7, batch 17 / 76, loss: 1.5001811981201172\n",
      "Epoch 7, batch 18 / 76, loss: 1.5589203834533691\n",
      "Epoch 7, batch 19 / 76, loss: 1.5443286895751953\n",
      "Epoch 7, batch 20 / 76, loss: 1.4526009559631348\n",
      "Epoch 7, batch 21 / 76, loss: 1.5175063610076904\n",
      "Epoch 7, batch 22 / 76, loss: 1.5594847202301025\n",
      "Epoch 7, batch 23 / 76, loss: 1.4776053428649902\n",
      "Epoch 7, batch 24 / 76, loss: 1.5193638801574707\n",
      "Epoch 7, batch 25 / 76, loss: 1.5205401182174683\n",
      "Epoch 7, batch 26 / 76, loss: 1.514837622642517\n",
      "Epoch 7, batch 27 / 76, loss: 1.5046932697296143\n",
      "Epoch 7, batch 28 / 76, loss: 1.5931565761566162\n",
      "Epoch 7, batch 29 / 76, loss: 1.556971788406372\n",
      "Epoch 7, batch 30 / 76, loss: 1.5406100749969482\n",
      "Epoch 7, batch 31 / 76, loss: 1.5418837070465088\n",
      "Epoch 7, batch 32 / 76, loss: 1.547177791595459\n",
      "Epoch 7, batch 33 / 76, loss: 1.58428955078125\n",
      "Epoch 7, batch 34 / 76, loss: 1.5121748447418213\n",
      "Epoch 7, batch 35 / 76, loss: 1.60016930103302\n",
      "Epoch 7, batch 36 / 76, loss: 1.5551354885101318\n",
      "Epoch 7, batch 37 / 76, loss: 1.6225430965423584\n",
      "Epoch 7, batch 38 / 76, loss: 1.479729413986206\n",
      "Epoch 7, batch 39 / 76, loss: 1.486116886138916\n",
      "Epoch 7, batch 40 / 76, loss: 1.5439085960388184\n",
      "Epoch 7, batch 41 / 76, loss: 1.5568538904190063\n",
      "Epoch 7, batch 42 / 76, loss: 1.5528935194015503\n",
      "Epoch 7, batch 43 / 76, loss: 1.574713945388794\n",
      "Epoch 7, batch 44 / 76, loss: 1.5515265464782715\n",
      "Epoch 7, batch 45 / 76, loss: 1.5112358331680298\n",
      "Epoch 7, batch 46 / 76, loss: 1.522571325302124\n",
      "Epoch 7, batch 47 / 76, loss: 1.539408564567566\n",
      "Epoch 7, batch 48 / 76, loss: 1.4438095092773438\n",
      "Epoch 7, batch 49 / 76, loss: 1.4519116878509521\n",
      "Epoch 7, batch 50 / 76, loss: 1.5177860260009766\n",
      "Epoch 7, batch 51 / 76, loss: 1.5190565586090088\n",
      "Epoch 7, batch 52 / 76, loss: 1.544996976852417\n",
      "Epoch 7, batch 53 / 76, loss: 1.48549222946167\n",
      "Epoch 7, batch 54 / 76, loss: 1.5632953643798828\n",
      "Epoch 7, batch 55 / 76, loss: 1.5008456707000732\n",
      "Epoch 7, batch 56 / 76, loss: 1.4803569316864014\n",
      "Epoch 7, batch 57 / 76, loss: 1.481814980506897\n",
      "Epoch 7, batch 58 / 76, loss: 1.582430362701416\n",
      "Epoch 7, batch 59 / 76, loss: 1.493698000907898\n",
      "Epoch 7, batch 60 / 76, loss: 1.4712285995483398\n",
      "Epoch 7, batch 61 / 76, loss: 1.4462227821350098\n",
      "Epoch 7, batch 62 / 76, loss: 1.5321533679962158\n",
      "Epoch 7, batch 63 / 76, loss: 1.5147398710250854\n",
      "Epoch 7, batch 64 / 76, loss: 1.520583152770996\n",
      "Epoch 7, batch 65 / 76, loss: 1.4543893337249756\n",
      "Epoch 7, batch 66 / 76, loss: 1.480947732925415\n",
      "Epoch 7, batch 67 / 76, loss: 1.51874840259552\n",
      "Epoch 7, batch 68 / 76, loss: 1.6341345310211182\n",
      "Epoch 7, batch 69 / 76, loss: 1.4765565395355225\n",
      "Epoch 7, batch 70 / 76, loss: 1.451646089553833\n",
      "Epoch 7, batch 71 / 76, loss: 1.43977952003479\n",
      "Epoch 7, batch 72 / 76, loss: 1.5590965747833252\n",
      "Epoch 7, batch 73 / 76, loss: 1.560158133506775\n",
      "Epoch 7, batch 74 / 76, loss: 1.5116515159606934\n",
      "Epoch 7, batch 75 / 76, loss: 1.5205605030059814\n",
      "Epoch 7, batch 76 / 76, loss: 1.514479398727417\n",
      "Epoch 7, batch 77 / 76, loss: 1.4982490539550781\n",
      "Epoch 8, batch 1 / 76, loss: 1.5193930864334106\n",
      "Epoch 8, batch 2 / 76, loss: 1.5683223009109497\n",
      "Epoch 8, batch 3 / 76, loss: 1.5347743034362793\n",
      "Epoch 8, batch 4 / 76, loss: 1.476193904876709\n",
      "Epoch 8, batch 5 / 76, loss: 1.5011320114135742\n",
      "Epoch 8, batch 6 / 76, loss: 1.520824670791626\n",
      "Epoch 8, batch 7 / 76, loss: 1.5511152744293213\n",
      "Epoch 8, batch 8 / 76, loss: 1.4987156391143799\n",
      "Epoch 8, batch 9 / 76, loss: 1.5352087020874023\n",
      "Epoch 8, batch 10 / 76, loss: 1.4949612617492676\n",
      "Epoch 8, batch 11 / 76, loss: 1.5549978017807007\n",
      "Epoch 8, batch 12 / 76, loss: 1.4834476709365845\n",
      "Epoch 8, batch 13 / 76, loss: 1.5509004592895508\n",
      "Epoch 8, batch 14 / 76, loss: 1.48982834815979\n",
      "Epoch 8, batch 15 / 76, loss: 1.5083913803100586\n",
      "Epoch 8, batch 16 / 76, loss: 1.4736509323120117\n",
      "Epoch 8, batch 17 / 76, loss: 1.4635608196258545\n",
      "Epoch 8, batch 18 / 76, loss: 1.5586994886398315\n",
      "Epoch 8, batch 19 / 76, loss: 1.5293519496917725\n",
      "Epoch 8, batch 20 / 76, loss: 1.4858875274658203\n",
      "Epoch 8, batch 21 / 76, loss: 1.481048583984375\n",
      "Epoch 8, batch 22 / 76, loss: 1.4996623992919922\n",
      "Epoch 8, batch 23 / 76, loss: 1.5783809423446655\n",
      "Epoch 8, batch 24 / 76, loss: 1.5162619352340698\n",
      "Epoch 8, batch 25 / 76, loss: 1.5257494449615479\n",
      "Epoch 8, batch 26 / 76, loss: 1.4901008605957031\n",
      "Epoch 8, batch 27 / 76, loss: 1.5056160688400269\n",
      "Epoch 8, batch 28 / 76, loss: 1.5168572664260864\n",
      "Epoch 8, batch 29 / 76, loss: 1.4843794107437134\n",
      "Epoch 8, batch 30 / 76, loss: 1.5188143253326416\n",
      "Epoch 8, batch 31 / 76, loss: 1.5325729846954346\n",
      "Epoch 8, batch 32 / 76, loss: 1.489591121673584\n",
      "Epoch 8, batch 33 / 76, loss: 1.4692668914794922\n",
      "Epoch 8, batch 34 / 76, loss: 1.4302308559417725\n",
      "Epoch 8, batch 35 / 76, loss: 1.5425465106964111\n",
      "Epoch 8, batch 36 / 76, loss: 1.4902669191360474\n",
      "Epoch 8, batch 37 / 76, loss: 1.474117398262024\n",
      "Epoch 8, batch 38 / 76, loss: 1.5629851818084717\n",
      "Epoch 8, batch 39 / 76, loss: 1.4887186288833618\n",
      "Epoch 8, batch 40 / 76, loss: 1.5199062824249268\n",
      "Epoch 8, batch 41 / 76, loss: 1.4477790594100952\n",
      "Epoch 8, batch 42 / 76, loss: 1.488086223602295\n",
      "Epoch 8, batch 43 / 76, loss: 1.5281932353973389\n",
      "Epoch 8, batch 44 / 76, loss: 1.5447969436645508\n",
      "Epoch 8, batch 45 / 76, loss: 1.5228667259216309\n",
      "Epoch 8, batch 46 / 76, loss: 1.5904369354248047\n",
      "Epoch 8, batch 47 / 76, loss: 1.4896602630615234\n",
      "Epoch 8, batch 48 / 76, loss: 1.490931749343872\n",
      "Epoch 8, batch 49 / 76, loss: 1.484971523284912\n",
      "Epoch 8, batch 50 / 76, loss: 1.471855878829956\n",
      "Epoch 8, batch 51 / 76, loss: 1.490562915802002\n",
      "Epoch 8, batch 52 / 76, loss: 1.4255280494689941\n",
      "Epoch 8, batch 53 / 76, loss: 1.501476764678955\n",
      "Epoch 8, batch 54 / 76, loss: 1.5065855979919434\n",
      "Epoch 8, batch 55 / 76, loss: 1.5477896928787231\n",
      "Epoch 8, batch 56 / 76, loss: 1.543939471244812\n",
      "Epoch 8, batch 57 / 76, loss: 1.5181825160980225\n",
      "Epoch 8, batch 58 / 76, loss: 1.5388426780700684\n",
      "Epoch 8, batch 59 / 76, loss: 1.4407057762145996\n",
      "Epoch 8, batch 60 / 76, loss: 1.5371930599212646\n",
      "Epoch 8, batch 61 / 76, loss: 1.5146450996398926\n",
      "Epoch 8, batch 62 / 76, loss: 1.5210037231445312\n",
      "Epoch 8, batch 63 / 76, loss: 1.5043036937713623\n",
      "Epoch 8, batch 64 / 76, loss: 1.5468230247497559\n",
      "Epoch 8, batch 65 / 76, loss: 1.4846186637878418\n",
      "Epoch 8, batch 66 / 76, loss: 1.5860496759414673\n",
      "Epoch 8, batch 67 / 76, loss: 1.5214707851409912\n",
      "Epoch 8, batch 68 / 76, loss: 1.5265753269195557\n",
      "Epoch 8, batch 69 / 76, loss: 1.4737433195114136\n",
      "Epoch 8, batch 70 / 76, loss: 1.5610058307647705\n",
      "Epoch 8, batch 71 / 76, loss: 1.4899084568023682\n",
      "Epoch 8, batch 72 / 76, loss: 1.4868991374969482\n",
      "Epoch 8, batch 73 / 76, loss: 1.5040982961654663\n",
      "Epoch 8, batch 74 / 76, loss: 1.437798261642456\n",
      "Epoch 8, batch 75 / 76, loss: 1.5543317794799805\n",
      "Epoch 8, batch 76 / 76, loss: 1.4928228855133057\n",
      "Epoch 8, batch 77 / 76, loss: 1.6612796783447266\n",
      "Epoch 9, batch 1 / 76, loss: 1.5525927543640137\n",
      "Epoch 9, batch 2 / 76, loss: 1.5519434213638306\n",
      "Epoch 9, batch 3 / 76, loss: 1.5016846656799316\n",
      "Epoch 9, batch 4 / 76, loss: 1.5432336330413818\n",
      "Epoch 9, batch 5 / 76, loss: 1.5133674144744873\n",
      "Epoch 9, batch 6 / 76, loss: 1.4700212478637695\n",
      "Epoch 9, batch 7 / 76, loss: 1.4635043144226074\n",
      "Epoch 9, batch 8 / 76, loss: 1.5204681158065796\n",
      "Epoch 9, batch 9 / 76, loss: 1.5081907510757446\n",
      "Epoch 9, batch 10 / 76, loss: 1.5018301010131836\n",
      "Epoch 9, batch 11 / 76, loss: 1.497765064239502\n",
      "Epoch 9, batch 12 / 76, loss: 1.4548625946044922\n",
      "Epoch 9, batch 13 / 76, loss: 1.404711365699768\n",
      "Epoch 9, batch 14 / 76, loss: 1.5265841484069824\n",
      "Epoch 9, batch 15 / 76, loss: 1.4341789484024048\n",
      "Epoch 9, batch 16 / 76, loss: 1.5144779682159424\n",
      "Epoch 9, batch 17 / 76, loss: 1.4701733589172363\n",
      "Epoch 9, batch 18 / 76, loss: 1.5716419219970703\n",
      "Epoch 9, batch 19 / 76, loss: 1.4809664487838745\n",
      "Epoch 9, batch 20 / 76, loss: 1.5339686870574951\n",
      "Epoch 9, batch 21 / 76, loss: 1.5328865051269531\n",
      "Epoch 9, batch 22 / 76, loss: 1.490949273109436\n",
      "Epoch 9, batch 23 / 76, loss: 1.537542700767517\n",
      "Epoch 9, batch 24 / 76, loss: 1.4880480766296387\n",
      "Epoch 9, batch 25 / 76, loss: 1.4637939929962158\n",
      "Epoch 9, batch 26 / 76, loss: 1.4651551246643066\n",
      "Epoch 9, batch 27 / 76, loss: 1.5202245712280273\n",
      "Epoch 9, batch 28 / 76, loss: 1.462273359298706\n",
      "Epoch 9, batch 29 / 76, loss: 1.4658558368682861\n",
      "Epoch 9, batch 30 / 76, loss: 1.3982934951782227\n",
      "Epoch 9, batch 31 / 76, loss: 1.4431254863739014\n",
      "Epoch 9, batch 32 / 76, loss: 1.4870736598968506\n",
      "Epoch 9, batch 33 / 76, loss: 1.4708240032196045\n",
      "Epoch 9, batch 34 / 76, loss: 1.480513572692871\n",
      "Epoch 9, batch 35 / 76, loss: 1.3769259452819824\n",
      "Epoch 9, batch 36 / 76, loss: 1.4939297437667847\n",
      "Epoch 9, batch 37 / 76, loss: 1.4852383136749268\n",
      "Epoch 9, batch 38 / 76, loss: 1.520110845565796\n",
      "Epoch 9, batch 39 / 76, loss: 1.4819464683532715\n",
      "Epoch 9, batch 40 / 76, loss: 1.4516351222991943\n",
      "Epoch 9, batch 41 / 76, loss: 1.4943103790283203\n",
      "Epoch 9, batch 42 / 76, loss: 1.5276989936828613\n",
      "Epoch 9, batch 43 / 76, loss: 1.3703336715698242\n",
      "Epoch 9, batch 44 / 76, loss: 1.5914952754974365\n",
      "Epoch 9, batch 45 / 76, loss: 1.4353110790252686\n",
      "Epoch 9, batch 46 / 76, loss: 1.4376981258392334\n",
      "Epoch 9, batch 47 / 76, loss: 1.4686028957366943\n",
      "Epoch 9, batch 48 / 76, loss: 1.3900206089019775\n",
      "Epoch 9, batch 49 / 76, loss: 1.443648099899292\n",
      "Epoch 9, batch 50 / 76, loss: 1.4319441318511963\n",
      "Epoch 9, batch 51 / 76, loss: 1.4400885105133057\n",
      "Epoch 9, batch 52 / 76, loss: 1.4117916822433472\n",
      "Epoch 9, batch 53 / 76, loss: 1.5038683414459229\n",
      "Epoch 9, batch 54 / 76, loss: 1.450034737586975\n",
      "Epoch 9, batch 55 / 76, loss: 1.4363503456115723\n",
      "Epoch 9, batch 56 / 76, loss: 1.3951094150543213\n",
      "Epoch 9, batch 57 / 76, loss: 1.454215407371521\n",
      "Epoch 9, batch 58 / 76, loss: 1.3854789733886719\n",
      "Epoch 9, batch 59 / 76, loss: 1.444409728050232\n",
      "Epoch 9, batch 60 / 76, loss: 1.4035344123840332\n",
      "Epoch 9, batch 61 / 76, loss: 1.4071050882339478\n",
      "Epoch 9, batch 62 / 76, loss: 1.395406723022461\n",
      "Epoch 9, batch 63 / 76, loss: 1.3228871822357178\n",
      "Epoch 9, batch 64 / 76, loss: 1.3607709407806396\n",
      "Epoch 9, batch 65 / 76, loss: 1.43841552734375\n",
      "Epoch 9, batch 66 / 76, loss: 1.382978916168213\n",
      "Epoch 9, batch 67 / 76, loss: 1.4228239059448242\n",
      "Epoch 9, batch 68 / 76, loss: 1.3428444862365723\n",
      "Epoch 9, batch 69 / 76, loss: 1.3802263736724854\n",
      "Epoch 9, batch 70 / 76, loss: 1.4117578268051147\n",
      "Epoch 9, batch 71 / 76, loss: 1.4216865301132202\n",
      "Epoch 9, batch 72 / 76, loss: 1.328261375427246\n",
      "Epoch 9, batch 73 / 76, loss: 1.3210294246673584\n",
      "Epoch 9, batch 74 / 76, loss: 1.3613860607147217\n",
      "Epoch 9, batch 75 / 76, loss: 1.3580573797225952\n",
      "Epoch 9, batch 76 / 76, loss: 1.3110090494155884\n",
      "Epoch 9, batch 77 / 76, loss: 1.2112774848937988\n",
      "Epoch 10, batch 1 / 76, loss: 1.3546711206436157\n",
      "Epoch 10, batch 2 / 76, loss: 1.3992335796356201\n",
      "Epoch 10, batch 3 / 76, loss: 1.3614819049835205\n",
      "Epoch 10, batch 4 / 76, loss: 1.3445202112197876\n",
      "Epoch 10, batch 5 / 76, loss: 1.3322786092758179\n",
      "Epoch 10, batch 6 / 76, loss: 1.3464499711990356\n",
      "Epoch 10, batch 7 / 76, loss: 1.3451480865478516\n",
      "Epoch 10, batch 8 / 76, loss: 1.3088936805725098\n",
      "Epoch 10, batch 9 / 76, loss: 1.3022828102111816\n",
      "Epoch 10, batch 10 / 76, loss: 1.3499720096588135\n",
      "Epoch 10, batch 11 / 76, loss: 1.291759729385376\n",
      "Epoch 10, batch 12 / 76, loss: 1.3056272268295288\n",
      "Epoch 10, batch 13 / 76, loss: 1.4242517948150635\n",
      "Epoch 10, batch 14 / 76, loss: 1.3256561756134033\n",
      "Epoch 10, batch 15 / 76, loss: 1.3697601556777954\n",
      "Epoch 10, batch 16 / 76, loss: 1.2796225547790527\n",
      "Epoch 10, batch 17 / 76, loss: 1.3252710103988647\n",
      "Epoch 10, batch 18 / 76, loss: 1.2931599617004395\n",
      "Epoch 10, batch 19 / 76, loss: 1.2395657300949097\n",
      "Epoch 10, batch 20 / 76, loss: 1.3305004835128784\n",
      "Epoch 10, batch 21 / 76, loss: 1.3197877407073975\n",
      "Epoch 10, batch 22 / 76, loss: 1.283106803894043\n",
      "Epoch 10, batch 23 / 76, loss: 1.2005882263183594\n",
      "Epoch 10, batch 24 / 76, loss: 1.2629694938659668\n",
      "Epoch 10, batch 25 / 76, loss: 1.3133916854858398\n",
      "Epoch 10, batch 26 / 76, loss: 1.2449586391448975\n",
      "Epoch 10, batch 27 / 76, loss: 1.2625281810760498\n",
      "Epoch 10, batch 28 / 76, loss: 1.3439207077026367\n",
      "Epoch 10, batch 29 / 76, loss: 1.2951287031173706\n",
      "Epoch 10, batch 30 / 76, loss: 1.2703466415405273\n",
      "Epoch 10, batch 31 / 76, loss: 1.3098728656768799\n",
      "Epoch 10, batch 32 / 76, loss: 1.289219856262207\n",
      "Epoch 10, batch 33 / 76, loss: 1.2774856090545654\n",
      "Epoch 10, batch 34 / 76, loss: 1.2951676845550537\n",
      "Epoch 10, batch 35 / 76, loss: 1.2764201164245605\n",
      "Epoch 10, batch 36 / 76, loss: 1.304002285003662\n",
      "Epoch 10, batch 37 / 76, loss: 1.3050332069396973\n",
      "Epoch 10, batch 38 / 76, loss: 1.3086795806884766\n",
      "Epoch 10, batch 39 / 76, loss: 1.3617421388626099\n",
      "Epoch 10, batch 40 / 76, loss: 1.235445499420166\n",
      "Epoch 10, batch 41 / 76, loss: 1.2922481298446655\n",
      "Epoch 10, batch 42 / 76, loss: 1.285794734954834\n",
      "Epoch 10, batch 43 / 76, loss: 1.2361702919006348\n",
      "Epoch 10, batch 44 / 76, loss: 1.2782062292099\n",
      "Epoch 10, batch 45 / 76, loss: 1.313442349433899\n",
      "Epoch 10, batch 46 / 76, loss: 1.268444538116455\n",
      "Epoch 10, batch 47 / 76, loss: 1.2839702367782593\n",
      "Epoch 10, batch 48 / 76, loss: 1.2679346799850464\n",
      "Epoch 10, batch 49 / 76, loss: 1.2578635215759277\n",
      "Epoch 10, batch 50 / 76, loss: 1.2528573274612427\n",
      "Epoch 10, batch 51 / 76, loss: 1.2824969291687012\n",
      "Epoch 10, batch 52 / 76, loss: 1.3091541528701782\n",
      "Epoch 10, batch 53 / 76, loss: 1.2685301303863525\n",
      "Epoch 10, batch 54 / 76, loss: 1.2718074321746826\n",
      "Epoch 10, batch 55 / 76, loss: 1.3204584121704102\n",
      "Epoch 10, batch 56 / 76, loss: 1.2538588047027588\n",
      "Epoch 10, batch 57 / 76, loss: 1.28922700881958\n",
      "Epoch 10, batch 58 / 76, loss: 1.2405672073364258\n",
      "Epoch 10, batch 59 / 76, loss: 1.2750334739685059\n",
      "Epoch 10, batch 60 / 76, loss: 1.3115181922912598\n",
      "Epoch 10, batch 61 / 76, loss: 1.2022353410720825\n",
      "Epoch 10, batch 62 / 76, loss: 1.2652946710586548\n",
      "Epoch 10, batch 63 / 76, loss: 1.3113114833831787\n",
      "Epoch 10, batch 64 / 76, loss: 1.2583720684051514\n",
      "Epoch 10, batch 65 / 76, loss: 1.2052563428878784\n",
      "Epoch 10, batch 66 / 76, loss: 1.3073036670684814\n",
      "Epoch 10, batch 67 / 76, loss: 1.226919174194336\n",
      "Epoch 10, batch 68 / 76, loss: 1.2292652130126953\n",
      "Epoch 10, batch 69 / 76, loss: 1.1907284259796143\n",
      "Epoch 10, batch 70 / 76, loss: 1.3109807968139648\n",
      "Epoch 10, batch 71 / 76, loss: 1.270509958267212\n",
      "Epoch 10, batch 72 / 76, loss: 1.285862922668457\n",
      "Epoch 10, batch 73 / 76, loss: 1.2373857498168945\n",
      "Epoch 10, batch 74 / 76, loss: 1.2653887271881104\n",
      "Epoch 10, batch 75 / 76, loss: 1.2719806432724\n",
      "Epoch 10, batch 76 / 76, loss: 1.2690768241882324\n",
      "Epoch 10, batch 77 / 76, loss: 1.2208738327026367\n",
      "Epoch 11, batch 1 / 76, loss: 1.263466238975525\n",
      "Epoch 11, batch 2 / 76, loss: 1.2587244510650635\n",
      "Epoch 11, batch 3 / 76, loss: 1.2598748207092285\n",
      "Epoch 11, batch 4 / 76, loss: 1.2684247493743896\n",
      "Epoch 11, batch 5 / 76, loss: 1.258429765701294\n",
      "Epoch 11, batch 6 / 76, loss: 1.2345536947250366\n",
      "Epoch 11, batch 7 / 76, loss: 1.2824805974960327\n",
      "Epoch 11, batch 8 / 76, loss: 1.2609221935272217\n",
      "Epoch 11, batch 9 / 76, loss: 1.2272441387176514\n",
      "Epoch 11, batch 10 / 76, loss: 1.3348288536071777\n",
      "Epoch 11, batch 11 / 76, loss: 1.3175632953643799\n",
      "Epoch 11, batch 12 / 76, loss: 1.2829194068908691\n",
      "Epoch 11, batch 13 / 76, loss: 1.2623884677886963\n",
      "Epoch 11, batch 14 / 76, loss: 1.3031296730041504\n",
      "Epoch 11, batch 15 / 76, loss: 1.3593729734420776\n",
      "Epoch 11, batch 16 / 76, loss: 1.261408805847168\n",
      "Epoch 11, batch 17 / 76, loss: 1.3185093402862549\n",
      "Epoch 11, batch 18 / 76, loss: 1.3481965065002441\n",
      "Epoch 11, batch 19 / 76, loss: 1.2943778038024902\n",
      "Epoch 11, batch 20 / 76, loss: 1.2939693927764893\n",
      "Epoch 11, batch 21 / 76, loss: 1.2730298042297363\n",
      "Epoch 11, batch 22 / 76, loss: 1.3128776550292969\n",
      "Epoch 11, batch 23 / 76, loss: 1.2948732376098633\n",
      "Epoch 11, batch 24 / 76, loss: 1.2619423866271973\n",
      "Epoch 11, batch 25 / 76, loss: 1.2551801204681396\n",
      "Epoch 11, batch 26 / 76, loss: 1.2507848739624023\n",
      "Epoch 11, batch 27 / 76, loss: 1.3007723093032837\n",
      "Epoch 11, batch 28 / 76, loss: 1.2657238245010376\n",
      "Epoch 11, batch 29 / 76, loss: 1.248213768005371\n",
      "Epoch 11, batch 30 / 76, loss: 1.2746374607086182\n",
      "Epoch 11, batch 31 / 76, loss: 1.3200044631958008\n",
      "Epoch 11, batch 32 / 76, loss: 1.3416907787322998\n",
      "Epoch 11, batch 33 / 76, loss: 1.2978124618530273\n",
      "Epoch 11, batch 34 / 76, loss: 1.3243303298950195\n",
      "Epoch 11, batch 35 / 76, loss: 1.2886797189712524\n",
      "Epoch 11, batch 36 / 76, loss: 1.3503049612045288\n",
      "Epoch 11, batch 37 / 76, loss: 1.3364875316619873\n",
      "Epoch 11, batch 38 / 76, loss: 1.2674198150634766\n",
      "Epoch 11, batch 39 / 76, loss: 1.239905834197998\n",
      "Epoch 11, batch 40 / 76, loss: 1.2892847061157227\n",
      "Epoch 11, batch 41 / 76, loss: 1.2892093658447266\n",
      "Epoch 11, batch 42 / 76, loss: 1.3058972358703613\n",
      "Epoch 11, batch 43 / 76, loss: 1.2588307857513428\n",
      "Epoch 11, batch 44 / 76, loss: 1.297541618347168\n",
      "Epoch 11, batch 45 / 76, loss: 1.3280422687530518\n",
      "Epoch 11, batch 46 / 76, loss: 1.3228440284729004\n",
      "Epoch 11, batch 47 / 76, loss: 1.3219523429870605\n",
      "Epoch 11, batch 48 / 76, loss: 1.3269038200378418\n",
      "Epoch 11, batch 49 / 76, loss: 1.2777595520019531\n",
      "Epoch 11, batch 50 / 76, loss: 1.3272993564605713\n",
      "Epoch 11, batch 51 / 76, loss: 1.3158918619155884\n",
      "Epoch 11, batch 52 / 76, loss: 1.324509620666504\n",
      "Epoch 11, batch 53 / 76, loss: 1.3518903255462646\n",
      "Epoch 11, batch 54 / 76, loss: 1.3445051908493042\n",
      "Epoch 11, batch 55 / 76, loss: 1.3053640127182007\n",
      "Epoch 11, batch 56 / 76, loss: 1.2957639694213867\n",
      "Epoch 11, batch 57 / 76, loss: 1.3257641792297363\n",
      "Epoch 11, batch 58 / 76, loss: 1.3375062942504883\n",
      "Epoch 11, batch 59 / 76, loss: 1.2717723846435547\n",
      "Epoch 11, batch 60 / 76, loss: 1.3544461727142334\n",
      "Epoch 11, batch 61 / 76, loss: 1.2321103811264038\n",
      "Epoch 11, batch 62 / 76, loss: 1.2929418087005615\n",
      "Epoch 11, batch 63 / 76, loss: 1.3499908447265625\n",
      "Epoch 11, batch 64 / 76, loss: 1.3138166666030884\n",
      "Epoch 11, batch 65 / 76, loss: 1.2741951942443848\n",
      "Epoch 11, batch 66 / 76, loss: 1.319213628768921\n",
      "Epoch 11, batch 67 / 76, loss: 1.3356070518493652\n",
      "Epoch 11, batch 68 / 76, loss: 1.3310928344726562\n",
      "Epoch 11, batch 69 / 76, loss: 1.3080847263336182\n",
      "Epoch 11, batch 70 / 76, loss: 1.3247793912887573\n",
      "Epoch 11, batch 71 / 76, loss: 1.3444621562957764\n",
      "Epoch 11, batch 72 / 76, loss: 1.3214895725250244\n",
      "Epoch 11, batch 73 / 76, loss: 1.3097925186157227\n",
      "Epoch 11, batch 74 / 76, loss: 1.2855193614959717\n",
      "Epoch 11, batch 75 / 76, loss: 1.352107048034668\n",
      "Epoch 11, batch 76 / 76, loss: 1.3534959554672241\n",
      "Epoch 11, batch 77 / 76, loss: 1.3301234245300293\n",
      "Epoch 12, batch 1 / 76, loss: 1.2912180423736572\n",
      "Epoch 12, batch 2 / 76, loss: 1.3029189109802246\n",
      "Epoch 12, batch 3 / 76, loss: 1.3834095001220703\n",
      "Epoch 12, batch 4 / 76, loss: 1.242409110069275\n",
      "Epoch 12, batch 5 / 76, loss: 1.3680522441864014\n",
      "Epoch 12, batch 6 / 76, loss: 1.3649193048477173\n",
      "Epoch 12, batch 7 / 76, loss: 1.3605235815048218\n",
      "Epoch 12, batch 8 / 76, loss: 1.2854340076446533\n",
      "Epoch 12, batch 9 / 76, loss: 1.3121929168701172\n",
      "Epoch 12, batch 10 / 76, loss: 1.3232688903808594\n",
      "Epoch 12, batch 11 / 76, loss: 1.3276317119598389\n",
      "Epoch 12, batch 12 / 76, loss: 1.3018018007278442\n",
      "Epoch 12, batch 13 / 76, loss: 1.3569164276123047\n",
      "Epoch 12, batch 14 / 76, loss: 1.3894206285476685\n",
      "Epoch 12, batch 15 / 76, loss: 1.325706124305725\n",
      "Epoch 12, batch 16 / 76, loss: 1.2993175983428955\n",
      "Epoch 12, batch 17 / 76, loss: 1.3499629497528076\n",
      "Epoch 12, batch 18 / 76, loss: 1.354480504989624\n",
      "Epoch 12, batch 19 / 76, loss: 1.3545118570327759\n",
      "Epoch 12, batch 20 / 76, loss: 1.3573673963546753\n",
      "Epoch 12, batch 21 / 76, loss: 1.3446851968765259\n",
      "Epoch 12, batch 22 / 76, loss: 1.3708869218826294\n",
      "Epoch 12, batch 23 / 76, loss: 1.335416316986084\n",
      "Epoch 12, batch 24 / 76, loss: 1.3149187564849854\n",
      "Epoch 12, batch 25 / 76, loss: 1.3595054149627686\n",
      "Epoch 12, batch 26 / 76, loss: 1.3336946964263916\n",
      "Epoch 12, batch 27 / 76, loss: 1.3149304389953613\n",
      "Epoch 12, batch 28 / 76, loss: 1.3089497089385986\n",
      "Epoch 12, batch 29 / 76, loss: 1.4053492546081543\n",
      "Epoch 12, batch 30 / 76, loss: 1.3508296012878418\n",
      "Epoch 12, batch 31 / 76, loss: 1.3778283596038818\n",
      "Epoch 12, batch 32 / 76, loss: 1.3158094882965088\n",
      "Epoch 12, batch 33 / 76, loss: 1.3273816108703613\n",
      "Epoch 12, batch 34 / 76, loss: 1.3107378482818604\n",
      "Epoch 12, batch 35 / 76, loss: 1.3572230339050293\n",
      "Epoch 12, batch 36 / 76, loss: 1.359215497970581\n",
      "Epoch 12, batch 37 / 76, loss: 1.3265910148620605\n",
      "Epoch 12, batch 38 / 76, loss: 1.4324345588684082\n",
      "Epoch 12, batch 39 / 76, loss: 1.365502119064331\n",
      "Epoch 12, batch 40 / 76, loss: 1.3250131607055664\n",
      "Epoch 12, batch 41 / 76, loss: 1.3565809726715088\n",
      "Epoch 12, batch 42 / 76, loss: 1.3092565536499023\n",
      "Epoch 12, batch 43 / 76, loss: 1.3085441589355469\n",
      "Epoch 12, batch 44 / 76, loss: 1.3863028287887573\n",
      "Epoch 12, batch 45 / 76, loss: 1.3176493644714355\n",
      "Epoch 12, batch 46 / 76, loss: 1.3846839666366577\n",
      "Epoch 12, batch 47 / 76, loss: 1.364722728729248\n",
      "Epoch 12, batch 48 / 76, loss: 1.3085455894470215\n",
      "Epoch 12, batch 49 / 76, loss: 1.409968614578247\n",
      "Epoch 12, batch 50 / 76, loss: 1.394237995147705\n",
      "Epoch 12, batch 51 / 76, loss: 1.375638723373413\n",
      "Epoch 12, batch 52 / 76, loss: 1.3935508728027344\n",
      "Epoch 12, batch 53 / 76, loss: 1.2694828510284424\n",
      "Epoch 12, batch 54 / 76, loss: 1.3111730813980103\n",
      "Epoch 12, batch 55 / 76, loss: 1.3968696594238281\n",
      "Epoch 12, batch 56 / 76, loss: 1.3711347579956055\n",
      "Epoch 12, batch 57 / 76, loss: 1.3434932231903076\n",
      "Epoch 12, batch 58 / 76, loss: 1.3433506488800049\n",
      "Epoch 12, batch 59 / 76, loss: 1.347341775894165\n",
      "Epoch 12, batch 60 / 76, loss: 1.4000396728515625\n",
      "Epoch 12, batch 61 / 76, loss: 1.3486751317977905\n",
      "Epoch 12, batch 62 / 76, loss: 1.3821125030517578\n",
      "Epoch 12, batch 63 / 76, loss: 1.348816156387329\n",
      "Epoch 12, batch 64 / 76, loss: 1.4094698429107666\n",
      "Epoch 12, batch 65 / 76, loss: 1.4524240493774414\n",
      "Epoch 12, batch 66 / 76, loss: 1.3350841999053955\n",
      "Epoch 12, batch 67 / 76, loss: 1.375370740890503\n",
      "Epoch 12, batch 68 / 76, loss: 1.3899428844451904\n",
      "Epoch 12, batch 69 / 76, loss: 1.3670809268951416\n",
      "Epoch 12, batch 70 / 76, loss: 1.399751901626587\n",
      "Epoch 12, batch 71 / 76, loss: 1.4261631965637207\n",
      "Epoch 12, batch 72 / 76, loss: 1.4316587448120117\n",
      "Epoch 12, batch 73 / 76, loss: 1.4133918285369873\n",
      "Epoch 12, batch 74 / 76, loss: 1.4130024909973145\n",
      "Epoch 12, batch 75 / 76, loss: 1.3575232028961182\n",
      "Epoch 12, batch 76 / 76, loss: 1.3539931774139404\n",
      "Epoch 12, batch 77 / 76, loss: 1.3254172801971436\n",
      "Epoch 13, batch 1 / 76, loss: 1.446284294128418\n",
      "Epoch 13, batch 2 / 76, loss: 1.398228406906128\n",
      "Epoch 13, batch 3 / 76, loss: 1.3826521635055542\n",
      "Epoch 13, batch 4 / 76, loss: 1.34257173538208\n",
      "Epoch 13, batch 5 / 76, loss: 1.3077170848846436\n",
      "Epoch 13, batch 6 / 76, loss: 1.407602310180664\n",
      "Epoch 13, batch 7 / 76, loss: 1.3582236766815186\n",
      "Epoch 13, batch 8 / 76, loss: 1.325805902481079\n",
      "Epoch 13, batch 9 / 76, loss: 1.4386141300201416\n",
      "Epoch 13, batch 10 / 76, loss: 1.408562421798706\n",
      "Epoch 13, batch 11 / 76, loss: 1.3202345371246338\n",
      "Epoch 13, batch 12 / 76, loss: 1.3820525407791138\n",
      "Epoch 13, batch 13 / 76, loss: 1.4066531658172607\n",
      "Epoch 13, batch 14 / 76, loss: 1.3521223068237305\n",
      "Epoch 13, batch 15 / 76, loss: 1.37642240524292\n",
      "Epoch 13, batch 16 / 76, loss: 1.4104660749435425\n",
      "Epoch 13, batch 17 / 76, loss: 1.410423755645752\n",
      "Epoch 13, batch 18 / 76, loss: 1.3431041240692139\n",
      "Epoch 13, batch 19 / 76, loss: 1.334543228149414\n",
      "Epoch 13, batch 20 / 76, loss: 1.359119176864624\n",
      "Epoch 13, batch 21 / 76, loss: 1.4054521322250366\n",
      "Epoch 13, batch 22 / 76, loss: 1.394672155380249\n",
      "Epoch 13, batch 23 / 76, loss: 1.3590922355651855\n",
      "Epoch 13, batch 24 / 76, loss: 1.3476606607437134\n",
      "Epoch 13, batch 25 / 76, loss: 1.4073987007141113\n",
      "Epoch 13, batch 26 / 76, loss: 1.380049705505371\n",
      "Epoch 13, batch 27 / 76, loss: 1.3577041625976562\n",
      "Epoch 13, batch 28 / 76, loss: 1.3514811992645264\n",
      "Epoch 13, batch 29 / 76, loss: 1.3250677585601807\n",
      "Epoch 13, batch 30 / 76, loss: 1.364326000213623\n",
      "Epoch 13, batch 31 / 76, loss: 1.351923942565918\n",
      "Epoch 13, batch 32 / 76, loss: 1.387373447418213\n",
      "Epoch 13, batch 33 / 76, loss: 1.3683395385742188\n",
      "Epoch 13, batch 34 / 76, loss: 1.3536028861999512\n",
      "Epoch 13, batch 35 / 76, loss: 1.367850661277771\n",
      "Epoch 13, batch 36 / 76, loss: 1.2996928691864014\n",
      "Epoch 13, batch 37 / 76, loss: 1.3868091106414795\n",
      "Epoch 13, batch 38 / 76, loss: 1.3701852560043335\n",
      "Epoch 13, batch 39 / 76, loss: 1.3670520782470703\n",
      "Epoch 13, batch 40 / 76, loss: 1.3038768768310547\n",
      "Epoch 13, batch 41 / 76, loss: 1.3462932109832764\n",
      "Epoch 13, batch 42 / 76, loss: 1.3469748497009277\n",
      "Epoch 13, batch 43 / 76, loss: 1.4016227722167969\n",
      "Epoch 13, batch 44 / 76, loss: 1.3039493560791016\n",
      "Epoch 13, batch 45 / 76, loss: 1.292726993560791\n",
      "Epoch 13, batch 46 / 76, loss: 1.3705333471298218\n",
      "Epoch 13, batch 47 / 76, loss: 1.3725900650024414\n",
      "Epoch 13, batch 48 / 76, loss: 1.390411376953125\n",
      "Epoch 13, batch 49 / 76, loss: 1.31966233253479\n",
      "Epoch 13, batch 50 / 76, loss: 1.344541311264038\n",
      "Epoch 13, batch 51 / 76, loss: 1.320743441581726\n",
      "Epoch 13, batch 52 / 76, loss: 1.3292429447174072\n",
      "Epoch 13, batch 53 / 76, loss: 1.332816481590271\n",
      "Epoch 13, batch 54 / 76, loss: 1.3620476722717285\n",
      "Epoch 13, batch 55 / 76, loss: 1.3287142515182495\n",
      "Epoch 13, batch 56 / 76, loss: 1.3548104763031006\n",
      "Epoch 13, batch 57 / 76, loss: 1.3503615856170654\n",
      "Epoch 13, batch 58 / 76, loss: 1.2625195980072021\n",
      "Epoch 13, batch 59 / 76, loss: 1.346975326538086\n",
      "Epoch 13, batch 60 / 76, loss: 1.3476455211639404\n",
      "Epoch 13, batch 61 / 76, loss: 1.399322748184204\n",
      "Epoch 13, batch 62 / 76, loss: 1.3826253414154053\n",
      "Epoch 13, batch 63 / 76, loss: 1.3372527360916138\n",
      "Epoch 13, batch 64 / 76, loss: 1.3648757934570312\n",
      "Epoch 13, batch 65 / 76, loss: 1.4037994146347046\n",
      "Epoch 13, batch 66 / 76, loss: 1.3102846145629883\n",
      "Epoch 13, batch 67 / 76, loss: 1.3567731380462646\n",
      "Epoch 13, batch 68 / 76, loss: 1.3221371173858643\n",
      "Epoch 13, batch 69 / 76, loss: 1.35733962059021\n",
      "Epoch 13, batch 70 / 76, loss: 1.3914672136306763\n",
      "Epoch 13, batch 71 / 76, loss: 1.317598581314087\n",
      "Epoch 13, batch 72 / 76, loss: 1.2818890810012817\n",
      "Epoch 13, batch 73 / 76, loss: 1.3739101886749268\n",
      "Epoch 13, batch 74 / 76, loss: 1.3278515338897705\n",
      "Epoch 13, batch 75 / 76, loss: 1.3516700267791748\n",
      "Epoch 13, batch 76 / 76, loss: 1.4202780723571777\n",
      "Epoch 13, batch 77 / 76, loss: 1.3758392333984375\n",
      "Epoch 14, batch 1 / 76, loss: 1.3844190835952759\n",
      "Epoch 14, batch 2 / 76, loss: 1.3236984014511108\n",
      "Epoch 14, batch 3 / 76, loss: 1.3729920387268066\n",
      "Epoch 14, batch 4 / 76, loss: 1.3889985084533691\n",
      "Epoch 14, batch 5 / 76, loss: 1.349719524383545\n",
      "Epoch 14, batch 6 / 76, loss: 1.4172250032424927\n",
      "Epoch 14, batch 7 / 76, loss: 1.3620789051055908\n",
      "Epoch 14, batch 8 / 76, loss: 1.357504963874817\n",
      "Epoch 14, batch 9 / 76, loss: 1.357514500617981\n",
      "Epoch 14, batch 10 / 76, loss: 1.3700246810913086\n",
      "Epoch 14, batch 11 / 76, loss: 1.4306952953338623\n",
      "Epoch 14, batch 12 / 76, loss: 1.341416835784912\n",
      "Epoch 14, batch 13 / 76, loss: 1.3197530508041382\n",
      "Epoch 14, batch 14 / 76, loss: 1.3703051805496216\n",
      "Epoch 14, batch 15 / 76, loss: 1.4266119003295898\n",
      "Epoch 14, batch 16 / 76, loss: 1.4364510774612427\n",
      "Epoch 14, batch 17 / 76, loss: 1.4603463411331177\n",
      "Epoch 14, batch 18 / 76, loss: 1.363938808441162\n",
      "Epoch 14, batch 19 / 76, loss: 1.407691240310669\n",
      "Epoch 14, batch 20 / 76, loss: 1.380262851715088\n",
      "Epoch 14, batch 21 / 76, loss: 1.3384909629821777\n",
      "Epoch 14, batch 22 / 76, loss: 1.4151809215545654\n",
      "Epoch 14, batch 23 / 76, loss: 1.3893561363220215\n",
      "Epoch 14, batch 24 / 76, loss: 1.4370224475860596\n",
      "Epoch 14, batch 25 / 76, loss: 1.3380130529403687\n",
      "Epoch 14, batch 26 / 76, loss: 1.3955082893371582\n",
      "Epoch 14, batch 27 / 76, loss: 1.3673979043960571\n",
      "Epoch 14, batch 28 / 76, loss: 1.3573508262634277\n",
      "Epoch 14, batch 29 / 76, loss: 1.4093737602233887\n",
      "Epoch 14, batch 30 / 76, loss: 1.4198929071426392\n",
      "Epoch 14, batch 31 / 76, loss: 1.460564136505127\n",
      "Epoch 14, batch 32 / 76, loss: 1.412711262702942\n",
      "Epoch 14, batch 33 / 76, loss: 1.3913424015045166\n",
      "Epoch 14, batch 34 / 76, loss: 1.4514037370681763\n",
      "Epoch 14, batch 35 / 76, loss: 1.4573198556900024\n",
      "Epoch 14, batch 36 / 76, loss: 1.3953678607940674\n",
      "Epoch 14, batch 37 / 76, loss: 1.3908522129058838\n",
      "Epoch 14, batch 38 / 76, loss: 1.4269661903381348\n",
      "Epoch 14, batch 39 / 76, loss: 1.4106950759887695\n",
      "Epoch 14, batch 40 / 76, loss: 1.4178736209869385\n",
      "Epoch 14, batch 41 / 76, loss: 1.3401830196380615\n",
      "Epoch 14, batch 42 / 76, loss: 1.4522325992584229\n",
      "Epoch 14, batch 43 / 76, loss: 1.4841694831848145\n",
      "Epoch 14, batch 44 / 76, loss: 1.4419759511947632\n",
      "Epoch 14, batch 45 / 76, loss: 1.5062223672866821\n",
      "Epoch 14, batch 46 / 76, loss: 1.4196937084197998\n",
      "Epoch 14, batch 47 / 76, loss: 1.4319294691085815\n",
      "Epoch 14, batch 48 / 76, loss: 1.4302566051483154\n",
      "Epoch 14, batch 49 / 76, loss: 1.4481000900268555\n",
      "Epoch 14, batch 50 / 76, loss: 1.4842700958251953\n",
      "Epoch 14, batch 51 / 76, loss: 1.4392316341400146\n",
      "Epoch 14, batch 52 / 76, loss: 1.4018802642822266\n",
      "Epoch 14, batch 53 / 76, loss: 1.4074901342391968\n",
      "Epoch 14, batch 54 / 76, loss: 1.4736762046813965\n",
      "Epoch 14, batch 55 / 76, loss: 1.4005593061447144\n",
      "Epoch 14, batch 56 / 76, loss: 1.4286203384399414\n",
      "Epoch 14, batch 57 / 76, loss: 1.4696645736694336\n",
      "Epoch 14, batch 58 / 76, loss: 1.425419569015503\n",
      "Epoch 14, batch 59 / 76, loss: 1.485633134841919\n",
      "Epoch 14, batch 60 / 76, loss: 1.4672174453735352\n",
      "Epoch 14, batch 61 / 76, loss: 1.4798928499221802\n",
      "Epoch 14, batch 62 / 76, loss: 1.4908969402313232\n",
      "Epoch 14, batch 63 / 76, loss: 1.4345786571502686\n",
      "Epoch 14, batch 64 / 76, loss: 1.4853187799453735\n",
      "Epoch 14, batch 65 / 76, loss: 1.4758625030517578\n",
      "Epoch 14, batch 66 / 76, loss: 1.4646217823028564\n",
      "Epoch 14, batch 67 / 76, loss: 1.460864543914795\n",
      "Epoch 14, batch 68 / 76, loss: 1.5537495613098145\n",
      "Epoch 14, batch 69 / 76, loss: 1.409318447113037\n",
      "Epoch 14, batch 70 / 76, loss: 1.5056366920471191\n",
      "Epoch 14, batch 71 / 76, loss: 1.4135291576385498\n",
      "Epoch 14, batch 72 / 76, loss: 1.4195218086242676\n",
      "Epoch 14, batch 73 / 76, loss: 1.4032636880874634\n",
      "Epoch 14, batch 74 / 76, loss: 1.5245252847671509\n",
      "Epoch 14, batch 75 / 76, loss: 1.4739243984222412\n",
      "Epoch 14, batch 76 / 76, loss: 1.4366132020950317\n",
      "Epoch 14, batch 77 / 76, loss: 1.5886327028274536\n",
      "Epoch 15, batch 1 / 76, loss: 1.4284687042236328\n",
      "Epoch 15, batch 2 / 76, loss: 1.430842638015747\n",
      "Epoch 15, batch 3 / 76, loss: 1.4792594909667969\n",
      "Epoch 15, batch 4 / 76, loss: 1.4128823280334473\n",
      "Epoch 15, batch 5 / 76, loss: 1.5328459739685059\n",
      "Epoch 15, batch 6 / 76, loss: 1.5423927307128906\n",
      "Epoch 15, batch 7 / 76, loss: 1.4171528816223145\n",
      "Epoch 15, batch 8 / 76, loss: 1.4599214792251587\n",
      "Epoch 15, batch 9 / 76, loss: 1.449352741241455\n",
      "Epoch 15, batch 10 / 76, loss: 1.450276255607605\n",
      "Epoch 15, batch 11 / 76, loss: 1.5059709548950195\n",
      "Epoch 15, batch 12 / 76, loss: 1.4447786808013916\n",
      "Epoch 15, batch 13 / 76, loss: 1.4549846649169922\n",
      "Epoch 15, batch 14 / 76, loss: 1.3909268379211426\n",
      "Epoch 15, batch 15 / 76, loss: 1.4333791732788086\n",
      "Epoch 15, batch 16 / 76, loss: 1.4749503135681152\n",
      "Epoch 15, batch 17 / 76, loss: 1.488048791885376\n",
      "Epoch 15, batch 18 / 76, loss: 1.4567346572875977\n",
      "Epoch 15, batch 19 / 76, loss: 1.541109323501587\n",
      "Epoch 15, batch 20 / 76, loss: 1.4657073020935059\n",
      "Epoch 15, batch 21 / 76, loss: 1.4236379861831665\n",
      "Epoch 15, batch 22 / 76, loss: 1.4586886167526245\n",
      "Epoch 15, batch 23 / 76, loss: 1.4773153066635132\n",
      "Epoch 15, batch 24 / 76, loss: 1.4528213739395142\n",
      "Epoch 15, batch 25 / 76, loss: 1.5111064910888672\n",
      "Epoch 15, batch 26 / 76, loss: 1.4922358989715576\n",
      "Epoch 15, batch 27 / 76, loss: 1.428717851638794\n",
      "Epoch 15, batch 28 / 76, loss: 1.5253459215164185\n",
      "Epoch 15, batch 29 / 76, loss: 1.40253746509552\n",
      "Epoch 15, batch 30 / 76, loss: 1.4960383176803589\n",
      "Epoch 15, batch 31 / 76, loss: 1.4309539794921875\n",
      "Epoch 15, batch 32 / 76, loss: 1.4830535650253296\n",
      "Epoch 15, batch 33 / 76, loss: 1.4655554294586182\n",
      "Epoch 15, batch 34 / 76, loss: 1.427448034286499\n",
      "Epoch 15, batch 35 / 76, loss: 1.387948751449585\n",
      "Epoch 15, batch 36 / 76, loss: 1.4542205333709717\n",
      "Epoch 15, batch 37 / 76, loss: 1.480817198753357\n",
      "Epoch 15, batch 38 / 76, loss: 1.4380168914794922\n",
      "Epoch 15, batch 39 / 76, loss: 1.4471768140792847\n",
      "Epoch 15, batch 40 / 76, loss: 1.4869654178619385\n",
      "Epoch 15, batch 41 / 76, loss: 1.4412424564361572\n",
      "Epoch 15, batch 42 / 76, loss: 1.4845905303955078\n",
      "Epoch 15, batch 43 / 76, loss: 1.5242043733596802\n",
      "Epoch 15, batch 44 / 76, loss: 1.522197961807251\n",
      "Epoch 15, batch 45 / 76, loss: 1.5585179328918457\n",
      "Epoch 15, batch 46 / 76, loss: 1.5395809412002563\n",
      "Epoch 15, batch 47 / 76, loss: 1.450488567352295\n",
      "Epoch 15, batch 48 / 76, loss: 1.3955094814300537\n",
      "Epoch 15, batch 49 / 76, loss: 1.4742214679718018\n",
      "Epoch 15, batch 50 / 76, loss: 1.448737621307373\n",
      "Epoch 15, batch 51 / 76, loss: 1.571303129196167\n",
      "Epoch 15, batch 52 / 76, loss: 1.5496885776519775\n",
      "Epoch 15, batch 53 / 76, loss: 1.524718165397644\n",
      "Epoch 15, batch 54 / 76, loss: 1.4612865447998047\n",
      "Epoch 15, batch 55 / 76, loss: 1.4926269054412842\n",
      "Epoch 15, batch 56 / 76, loss: 1.511037826538086\n",
      "Epoch 15, batch 57 / 76, loss: 1.5282080173492432\n",
      "Epoch 15, batch 58 / 76, loss: 1.4398133754730225\n",
      "Epoch 15, batch 59 / 76, loss: 1.4862184524536133\n",
      "Epoch 15, batch 60 / 76, loss: 1.5018187761306763\n",
      "Epoch 15, batch 61 / 76, loss: 1.547804832458496\n",
      "Epoch 15, batch 62 / 76, loss: 1.486029863357544\n",
      "Epoch 15, batch 63 / 76, loss: 1.4973907470703125\n",
      "Epoch 15, batch 64 / 76, loss: 1.5432032346725464\n",
      "Epoch 15, batch 65 / 76, loss: 1.5195081233978271\n",
      "Epoch 15, batch 66 / 76, loss: 1.4109407663345337\n",
      "Epoch 15, batch 67 / 76, loss: 1.4929163455963135\n",
      "Epoch 15, batch 68 / 76, loss: 1.5039243698120117\n",
      "Epoch 15, batch 69 / 76, loss: 1.5121558904647827\n",
      "Epoch 15, batch 70 / 76, loss: 1.509126901626587\n",
      "Epoch 15, batch 71 / 76, loss: 1.4931542873382568\n",
      "Epoch 15, batch 72 / 76, loss: 1.510353922843933\n",
      "Epoch 15, batch 73 / 76, loss: 1.4929983615875244\n",
      "Epoch 15, batch 74 / 76, loss: 1.4848891496658325\n",
      "Epoch 15, batch 75 / 76, loss: 1.5526747703552246\n",
      "Epoch 15, batch 76 / 76, loss: 1.5092263221740723\n",
      "Epoch 15, batch 77 / 76, loss: 1.6678650379180908\n",
      "Epoch 16, batch 1 / 76, loss: 1.5697543621063232\n",
      "Epoch 16, batch 2 / 76, loss: 1.5141699314117432\n",
      "Epoch 16, batch 3 / 76, loss: 1.444509744644165\n",
      "Epoch 16, batch 4 / 76, loss: 1.4837579727172852\n",
      "Epoch 16, batch 5 / 76, loss: 1.529935359954834\n",
      "Epoch 16, batch 6 / 76, loss: 1.5161707401275635\n",
      "Epoch 16, batch 7 / 76, loss: 1.5583648681640625\n",
      "Epoch 16, batch 8 / 76, loss: 1.6402397155761719\n",
      "Epoch 16, batch 9 / 76, loss: 1.598757028579712\n",
      "Epoch 16, batch 10 / 76, loss: 1.5336716175079346\n",
      "Epoch 16, batch 11 / 76, loss: 1.5188331604003906\n",
      "Epoch 16, batch 12 / 76, loss: 1.4558024406433105\n",
      "Epoch 16, batch 13 / 76, loss: 1.5936124324798584\n",
      "Epoch 16, batch 14 / 76, loss: 1.4934508800506592\n",
      "Epoch 16, batch 15 / 76, loss: 1.4937207698822021\n",
      "Epoch 16, batch 16 / 76, loss: 1.4532628059387207\n",
      "Epoch 16, batch 17 / 76, loss: 1.5981959104537964\n",
      "Epoch 16, batch 18 / 76, loss: 1.5113475322723389\n",
      "Epoch 16, batch 19 / 76, loss: 1.4582346677780151\n",
      "Epoch 16, batch 20 / 76, loss: 1.5422099828720093\n",
      "Epoch 16, batch 21 / 76, loss: 1.4990487098693848\n",
      "Epoch 16, batch 22 / 76, loss: 1.509145736694336\n",
      "Epoch 16, batch 23 / 76, loss: 1.4690852165222168\n",
      "Epoch 16, batch 24 / 76, loss: 1.5539758205413818\n",
      "Epoch 16, batch 25 / 76, loss: 1.6114683151245117\n",
      "Epoch 16, batch 26 / 76, loss: 1.4620933532714844\n",
      "Epoch 16, batch 27 / 76, loss: 1.5256177186965942\n",
      "Epoch 16, batch 28 / 76, loss: 1.4513356685638428\n",
      "Epoch 16, batch 29 / 76, loss: 1.555715560913086\n",
      "Epoch 16, batch 30 / 76, loss: 1.4731559753417969\n",
      "Epoch 16, batch 31 / 76, loss: 1.441971778869629\n",
      "Epoch 16, batch 32 / 76, loss: 1.537520170211792\n",
      "Epoch 16, batch 33 / 76, loss: 1.502293586730957\n",
      "Epoch 16, batch 34 / 76, loss: 1.5650579929351807\n",
      "Epoch 16, batch 35 / 76, loss: 1.48268723487854\n",
      "Epoch 16, batch 36 / 76, loss: 1.4710521697998047\n",
      "Epoch 16, batch 37 / 76, loss: 1.488208532333374\n",
      "Epoch 16, batch 38 / 76, loss: 1.5276691913604736\n",
      "Epoch 16, batch 39 / 76, loss: 1.4582854509353638\n",
      "Epoch 16, batch 40 / 76, loss: 1.5002326965332031\n",
      "Epoch 16, batch 41 / 76, loss: 1.4935977458953857\n",
      "Epoch 16, batch 42 / 76, loss: 1.4463629722595215\n",
      "Epoch 16, batch 43 / 76, loss: 1.4919347763061523\n",
      "Epoch 16, batch 44 / 76, loss: 1.5221366882324219\n",
      "Epoch 16, batch 45 / 76, loss: 1.5030447244644165\n",
      "Epoch 16, batch 46 / 76, loss: 1.4794509410858154\n",
      "Epoch 16, batch 47 / 76, loss: 1.5591577291488647\n",
      "Epoch 16, batch 48 / 76, loss: 1.5501983165740967\n",
      "Epoch 16, batch 49 / 76, loss: 1.5228900909423828\n",
      "Epoch 16, batch 50 / 76, loss: 1.5289199352264404\n",
      "Epoch 16, batch 51 / 76, loss: 1.4098575115203857\n",
      "Epoch 16, batch 52 / 76, loss: 1.4794632196426392\n",
      "Epoch 16, batch 53 / 76, loss: 1.5313003063201904\n",
      "Epoch 16, batch 54 / 76, loss: 1.616161823272705\n",
      "Epoch 16, batch 55 / 76, loss: 1.4713215827941895\n",
      "Epoch 16, batch 56 / 76, loss: 1.5910269021987915\n",
      "Epoch 16, batch 57 / 76, loss: 1.490759015083313\n",
      "Epoch 16, batch 58 / 76, loss: 1.5728683471679688\n",
      "Epoch 16, batch 59 / 76, loss: 1.5480685234069824\n",
      "Epoch 16, batch 60 / 76, loss: 1.5532941818237305\n",
      "Epoch 16, batch 61 / 76, loss: 1.5982677936553955\n",
      "Epoch 16, batch 62 / 76, loss: 1.6407973766326904\n",
      "Epoch 16, batch 63 / 76, loss: 1.540419578552246\n",
      "Epoch 16, batch 64 / 76, loss: 1.62465238571167\n",
      "Epoch 16, batch 65 / 76, loss: 1.583893060684204\n",
      "Epoch 16, batch 66 / 76, loss: 1.5667216777801514\n",
      "Epoch 16, batch 67 / 76, loss: 1.5169110298156738\n",
      "Epoch 16, batch 68 / 76, loss: 1.5720343589782715\n",
      "Epoch 16, batch 69 / 76, loss: 1.565425157546997\n",
      "Epoch 16, batch 70 / 76, loss: 1.5273792743682861\n",
      "Epoch 16, batch 71 / 76, loss: 1.5439178943634033\n",
      "Epoch 16, batch 72 / 76, loss: 1.5740705728530884\n",
      "Epoch 16, batch 73 / 76, loss: 1.6112488508224487\n",
      "Epoch 16, batch 74 / 76, loss: 1.514357328414917\n",
      "Epoch 16, batch 75 / 76, loss: 1.557758092880249\n",
      "Epoch 16, batch 76 / 76, loss: 1.5174212455749512\n",
      "Epoch 16, batch 77 / 76, loss: 1.5526974201202393\n",
      "Epoch 17, batch 1 / 76, loss: 1.5933942794799805\n",
      "Epoch 17, batch 2 / 76, loss: 1.4801180362701416\n",
      "Epoch 17, batch 3 / 76, loss: 1.6452734470367432\n",
      "Epoch 17, batch 4 / 76, loss: 1.5199103355407715\n",
      "Epoch 17, batch 5 / 76, loss: 1.545454978942871\n",
      "Epoch 17, batch 6 / 76, loss: 1.6152316331863403\n",
      "Epoch 17, batch 7 / 76, loss: 1.6151586771011353\n",
      "Epoch 17, batch 8 / 76, loss: 1.5306272506713867\n",
      "Epoch 17, batch 9 / 76, loss: 1.5168808698654175\n",
      "Epoch 17, batch 10 / 76, loss: 1.5600717067718506\n",
      "Epoch 17, batch 11 / 76, loss: 1.5604912042617798\n",
      "Epoch 17, batch 12 / 76, loss: 1.519319772720337\n",
      "Epoch 17, batch 13 / 76, loss: 1.5213184356689453\n",
      "Epoch 17, batch 14 / 76, loss: 1.5353283882141113\n",
      "Epoch 17, batch 15 / 76, loss: 1.5305495262145996\n",
      "Epoch 17, batch 16 / 76, loss: 1.492459774017334\n",
      "Epoch 17, batch 17 / 76, loss: 1.5198614597320557\n",
      "Epoch 17, batch 18 / 76, loss: 1.456270694732666\n",
      "Epoch 17, batch 19 / 76, loss: 1.5362238883972168\n",
      "Epoch 17, batch 20 / 76, loss: 1.4909918308258057\n",
      "Epoch 17, batch 21 / 76, loss: 1.54433274269104\n",
      "Epoch 17, batch 22 / 76, loss: 1.53257155418396\n",
      "Epoch 17, batch 23 / 76, loss: 1.5698246955871582\n",
      "Epoch 17, batch 24 / 76, loss: 1.5247600078582764\n",
      "Epoch 17, batch 25 / 76, loss: 1.5525212287902832\n",
      "Epoch 17, batch 26 / 76, loss: 1.6046056747436523\n",
      "Epoch 17, batch 27 / 76, loss: 1.4655015468597412\n",
      "Epoch 17, batch 28 / 76, loss: 1.5138444900512695\n",
      "Epoch 17, batch 29 / 76, loss: 1.5966861248016357\n",
      "Epoch 17, batch 30 / 76, loss: 1.5664316415786743\n",
      "Epoch 17, batch 31 / 76, loss: 1.5151665210723877\n",
      "Epoch 17, batch 32 / 76, loss: 1.5932729244232178\n",
      "Epoch 17, batch 33 / 76, loss: 1.4787015914916992\n",
      "Epoch 17, batch 34 / 76, loss: 1.5387111902236938\n",
      "Epoch 17, batch 35 / 76, loss: 1.5081944465637207\n",
      "Epoch 17, batch 36 / 76, loss: 1.5908894538879395\n",
      "Epoch 17, batch 37 / 76, loss: 1.6011488437652588\n",
      "Epoch 17, batch 38 / 76, loss: 1.532219648361206\n",
      "Epoch 17, batch 39 / 76, loss: 1.6120915412902832\n",
      "Epoch 17, batch 40 / 76, loss: 1.6041654348373413\n",
      "Epoch 17, batch 41 / 76, loss: 1.5559638738632202\n",
      "Epoch 17, batch 42 / 76, loss: 1.621040940284729\n",
      "Epoch 17, batch 43 / 76, loss: 1.5583972930908203\n",
      "Epoch 17, batch 44 / 76, loss: 1.6430872678756714\n",
      "Epoch 17, batch 45 / 76, loss: 1.6077752113342285\n",
      "Epoch 17, batch 46 / 76, loss: 1.5645108222961426\n",
      "Epoch 17, batch 47 / 76, loss: 1.479056715965271\n",
      "Epoch 17, batch 48 / 76, loss: 1.5130418539047241\n",
      "Epoch 17, batch 49 / 76, loss: 1.5263925790786743\n",
      "Epoch 17, batch 50 / 76, loss: 1.5581536293029785\n",
      "Epoch 17, batch 51 / 76, loss: 1.630692481994629\n",
      "Epoch 17, batch 52 / 76, loss: 1.5830682516098022\n",
      "Epoch 17, batch 53 / 76, loss: 1.5394200086593628\n",
      "Epoch 17, batch 54 / 76, loss: 1.6041693687438965\n",
      "Epoch 17, batch 55 / 76, loss: 1.5928906202316284\n",
      "Epoch 17, batch 56 / 76, loss: 1.571390986442566\n",
      "Epoch 17, batch 57 / 76, loss: 1.538041591644287\n",
      "Epoch 17, batch 58 / 76, loss: 1.5342657566070557\n",
      "Epoch 17, batch 59 / 76, loss: 1.5925906896591187\n",
      "Epoch 17, batch 60 / 76, loss: 1.6104226112365723\n",
      "Epoch 17, batch 61 / 76, loss: 1.5931951999664307\n",
      "Epoch 17, batch 62 / 76, loss: 1.5064853429794312\n",
      "Epoch 17, batch 63 / 76, loss: 1.579612135887146\n",
      "Epoch 17, batch 64 / 76, loss: 1.5232897996902466\n",
      "Epoch 17, batch 65 / 76, loss: 1.5953953266143799\n",
      "Epoch 17, batch 66 / 76, loss: 1.560767412185669\n",
      "Epoch 17, batch 67 / 76, loss: 1.6433956623077393\n",
      "Epoch 17, batch 68 / 76, loss: 1.5697323083877563\n",
      "Epoch 17, batch 69 / 76, loss: 1.5367484092712402\n",
      "Epoch 17, batch 70 / 76, loss: 1.5554523468017578\n",
      "Epoch 17, batch 71 / 76, loss: 1.5965025424957275\n",
      "Epoch 17, batch 72 / 76, loss: 1.5477485656738281\n",
      "Epoch 17, batch 73 / 76, loss: 1.5329914093017578\n",
      "Epoch 17, batch 74 / 76, loss: 1.6266995668411255\n",
      "Epoch 17, batch 75 / 76, loss: 1.5882923603057861\n",
      "Epoch 17, batch 76 / 76, loss: 1.5669111013412476\n",
      "Epoch 17, batch 77 / 76, loss: 1.5134046077728271\n",
      "Epoch 18, batch 1 / 76, loss: 1.6329011917114258\n",
      "Epoch 18, batch 2 / 76, loss: 1.6049549579620361\n",
      "Epoch 18, batch 3 / 76, loss: 1.5742292404174805\n",
      "Epoch 18, batch 4 / 76, loss: 1.5721778869628906\n",
      "Epoch 18, batch 5 / 76, loss: 1.564227819442749\n",
      "Epoch 18, batch 6 / 76, loss: 1.5054740905761719\n",
      "Epoch 18, batch 7 / 76, loss: 1.5840696096420288\n",
      "Epoch 18, batch 8 / 76, loss: 1.5722429752349854\n",
      "Epoch 18, batch 9 / 76, loss: 1.499366283416748\n",
      "Epoch 18, batch 10 / 76, loss: 1.5186375379562378\n",
      "Epoch 18, batch 11 / 76, loss: 1.5440008640289307\n",
      "Epoch 18, batch 12 / 76, loss: 1.4772562980651855\n",
      "Epoch 18, batch 13 / 76, loss: 1.5310702323913574\n",
      "Epoch 18, batch 14 / 76, loss: 1.4858508110046387\n",
      "Epoch 18, batch 15 / 76, loss: 1.5352344512939453\n",
      "Epoch 18, batch 16 / 76, loss: 1.6333532333374023\n",
      "Epoch 18, batch 17 / 76, loss: 1.5328824520111084\n",
      "Epoch 18, batch 18 / 76, loss: 1.585179328918457\n",
      "Epoch 18, batch 19 / 76, loss: 1.5839029550552368\n",
      "Epoch 18, batch 20 / 76, loss: 1.5467438697814941\n",
      "Epoch 18, batch 21 / 76, loss: 1.5881896018981934\n",
      "Epoch 18, batch 22 / 76, loss: 1.5334532260894775\n",
      "Epoch 18, batch 23 / 76, loss: 1.492843508720398\n",
      "Epoch 18, batch 24 / 76, loss: 1.5447494983673096\n",
      "Epoch 18, batch 25 / 76, loss: 1.4833505153656006\n",
      "Epoch 18, batch 26 / 76, loss: 1.5903195142745972\n",
      "Epoch 18, batch 27 / 76, loss: 1.5801873207092285\n",
      "Epoch 18, batch 28 / 76, loss: 1.5015106201171875\n",
      "Epoch 18, batch 29 / 76, loss: 1.4773151874542236\n",
      "Epoch 18, batch 30 / 76, loss: 1.5664448738098145\n",
      "Epoch 18, batch 31 / 76, loss: 1.5398991107940674\n",
      "Epoch 18, batch 32 / 76, loss: 1.5797321796417236\n",
      "Epoch 18, batch 33 / 76, loss: 1.5602266788482666\n",
      "Epoch 18, batch 34 / 76, loss: 1.5011873245239258\n",
      "Epoch 18, batch 35 / 76, loss: 1.4870878458023071\n",
      "Epoch 18, batch 36 / 76, loss: 1.5644986629486084\n",
      "Epoch 18, batch 37 / 76, loss: 1.5857539176940918\n",
      "Epoch 18, batch 38 / 76, loss: 1.6277134418487549\n",
      "Epoch 18, batch 39 / 76, loss: 1.4909510612487793\n",
      "Epoch 18, batch 40 / 76, loss: 1.5604968070983887\n",
      "Epoch 18, batch 41 / 76, loss: 1.5587122440338135\n",
      "Epoch 18, batch 42 / 76, loss: 1.5600709915161133\n",
      "Epoch 18, batch 43 / 76, loss: 1.47249174118042\n",
      "Epoch 18, batch 44 / 76, loss: 1.5205016136169434\n",
      "Epoch 18, batch 45 / 76, loss: 1.4641475677490234\n",
      "Epoch 18, batch 46 / 76, loss: 1.5693066120147705\n",
      "Epoch 18, batch 47 / 76, loss: 1.4725995063781738\n",
      "Epoch 18, batch 48 / 76, loss: 1.4479773044586182\n",
      "Epoch 18, batch 49 / 76, loss: 1.5108277797698975\n",
      "Epoch 18, batch 50 / 76, loss: 1.518182396888733\n",
      "Epoch 18, batch 51 / 76, loss: 1.4918303489685059\n",
      "Epoch 18, batch 52 / 76, loss: 1.5322434902191162\n",
      "Epoch 18, batch 53 / 76, loss: 1.5363869667053223\n",
      "Epoch 18, batch 54 / 76, loss: 1.4803705215454102\n",
      "Epoch 18, batch 55 / 76, loss: 1.5778915882110596\n",
      "Epoch 18, batch 56 / 76, loss: 1.4849145412445068\n",
      "Epoch 18, batch 57 / 76, loss: 1.5407004356384277\n",
      "Epoch 18, batch 58 / 76, loss: 1.4976806640625\n",
      "Epoch 18, batch 59 / 76, loss: 1.5211619138717651\n",
      "Epoch 18, batch 60 / 76, loss: 1.5575082302093506\n",
      "Epoch 18, batch 61 / 76, loss: 1.515294075012207\n",
      "Epoch 18, batch 62 / 76, loss: 1.494849443435669\n",
      "Epoch 18, batch 63 / 76, loss: 1.5443296432495117\n",
      "Epoch 18, batch 64 / 76, loss: 1.4596742391586304\n",
      "Epoch 18, batch 65 / 76, loss: 1.472381353378296\n",
      "Epoch 18, batch 66 / 76, loss: 1.495692253112793\n",
      "Epoch 18, batch 67 / 76, loss: 1.4991874694824219\n",
      "Epoch 18, batch 68 / 76, loss: 1.5425090789794922\n",
      "Epoch 18, batch 69 / 76, loss: 1.4740369319915771\n",
      "Epoch 18, batch 70 / 76, loss: 1.4818470478057861\n",
      "Epoch 18, batch 71 / 76, loss: 1.437807321548462\n",
      "Epoch 18, batch 72 / 76, loss: 1.5098698139190674\n",
      "Epoch 18, batch 73 / 76, loss: 1.4307078123092651\n",
      "Epoch 18, batch 74 / 76, loss: 1.460984230041504\n",
      "Epoch 18, batch 75 / 76, loss: 1.6045801639556885\n",
      "Epoch 18, batch 76 / 76, loss: 1.5391104221343994\n",
      "Epoch 18, batch 77 / 76, loss: 1.470864176750183\n",
      "Epoch 19, batch 1 / 76, loss: 1.4628639221191406\n",
      "Epoch 19, batch 2 / 76, loss: 1.4812266826629639\n",
      "Epoch 19, batch 3 / 76, loss: 1.535459280014038\n",
      "Epoch 19, batch 4 / 76, loss: 1.5007450580596924\n",
      "Epoch 19, batch 5 / 76, loss: 1.47049880027771\n",
      "Epoch 19, batch 6 / 76, loss: 1.5058097839355469\n",
      "Epoch 19, batch 7 / 76, loss: 1.4522309303283691\n",
      "Epoch 19, batch 8 / 76, loss: 1.5280711650848389\n",
      "Epoch 19, batch 9 / 76, loss: 1.4478940963745117\n",
      "Epoch 19, batch 10 / 76, loss: 1.4908156394958496\n",
      "Epoch 19, batch 11 / 76, loss: 1.4692597389221191\n",
      "Epoch 19, batch 12 / 76, loss: 1.493622064590454\n",
      "Epoch 19, batch 13 / 76, loss: 1.3650257587432861\n",
      "Epoch 19, batch 14 / 76, loss: 1.5278065204620361\n",
      "Epoch 19, batch 15 / 76, loss: 1.4785397052764893\n",
      "Epoch 19, batch 16 / 76, loss: 1.4359087944030762\n",
      "Epoch 19, batch 17 / 76, loss: 1.4740564823150635\n",
      "Epoch 19, batch 18 / 76, loss: 1.4361220598220825\n",
      "Epoch 19, batch 19 / 76, loss: 1.5274498462677002\n",
      "Epoch 19, batch 20 / 76, loss: 1.506842851638794\n",
      "Epoch 19, batch 21 / 76, loss: 1.4925458431243896\n",
      "Epoch 19, batch 22 / 76, loss: 1.4847614765167236\n",
      "Epoch 19, batch 23 / 76, loss: 1.4288105964660645\n",
      "Epoch 19, batch 24 / 76, loss: 1.4455385208129883\n",
      "Epoch 19, batch 25 / 76, loss: 1.4827089309692383\n",
      "Epoch 19, batch 26 / 76, loss: 1.414423942565918\n",
      "Epoch 19, batch 27 / 76, loss: 1.4408509731292725\n",
      "Epoch 19, batch 28 / 76, loss: 1.3907067775726318\n",
      "Epoch 19, batch 29 / 76, loss: 1.4194406270980835\n",
      "Epoch 19, batch 30 / 76, loss: 1.4330120086669922\n",
      "Epoch 19, batch 31 / 76, loss: 1.4734803438186646\n",
      "Epoch 19, batch 32 / 76, loss: 1.5287985801696777\n",
      "Epoch 19, batch 33 / 76, loss: 1.4718186855316162\n",
      "Epoch 19, batch 34 / 76, loss: 1.5345124006271362\n",
      "Epoch 19, batch 35 / 76, loss: 1.4683122634887695\n",
      "Epoch 19, batch 36 / 76, loss: 1.4132370948791504\n",
      "Epoch 19, batch 37 / 76, loss: 1.454988718032837\n",
      "Epoch 19, batch 38 / 76, loss: 1.5072141885757446\n",
      "Epoch 19, batch 39 / 76, loss: 1.5417360067367554\n",
      "Epoch 19, batch 40 / 76, loss: 1.4617321491241455\n",
      "Epoch 19, batch 41 / 76, loss: 1.483656883239746\n",
      "Epoch 19, batch 42 / 76, loss: 1.5172359943389893\n",
      "Epoch 19, batch 43 / 76, loss: 1.4618909358978271\n",
      "Epoch 19, batch 44 / 76, loss: 1.5054869651794434\n",
      "Epoch 19, batch 45 / 76, loss: 1.5042495727539062\n",
      "Epoch 19, batch 46 / 76, loss: 1.4625897407531738\n",
      "Epoch 19, batch 47 / 76, loss: 1.3971681594848633\n",
      "Epoch 19, batch 48 / 76, loss: 1.451176404953003\n",
      "Epoch 19, batch 49 / 76, loss: 1.4915821552276611\n",
      "Epoch 19, batch 50 / 76, loss: 1.4251635074615479\n",
      "Epoch 19, batch 51 / 76, loss: 1.437971830368042\n",
      "Epoch 19, batch 52 / 76, loss: 1.5759596824645996\n",
      "Epoch 19, batch 53 / 76, loss: 1.4780480861663818\n",
      "Epoch 19, batch 54 / 76, loss: 1.4609992504119873\n",
      "Epoch 19, batch 55 / 76, loss: 1.5070422887802124\n",
      "Epoch 19, batch 56 / 76, loss: 1.4131525754928589\n",
      "Epoch 19, batch 57 / 76, loss: 1.4525015354156494\n",
      "Epoch 19, batch 58 / 76, loss: 1.5278644561767578\n",
      "Epoch 19, batch 59 / 76, loss: 1.4151644706726074\n",
      "Epoch 19, batch 60 / 76, loss: 1.4832091331481934\n",
      "Epoch 19, batch 61 / 76, loss: 1.4379196166992188\n",
      "Epoch 19, batch 62 / 76, loss: 1.3945930004119873\n",
      "Epoch 19, batch 63 / 76, loss: 1.4583196640014648\n",
      "Epoch 19, batch 64 / 76, loss: 1.4793291091918945\n",
      "Epoch 19, batch 65 / 76, loss: 1.5154023170471191\n",
      "Epoch 19, batch 66 / 76, loss: 1.4817723035812378\n",
      "Epoch 19, batch 67 / 76, loss: 1.4181523323059082\n",
      "Epoch 19, batch 68 / 76, loss: 1.547977089881897\n",
      "Epoch 19, batch 69 / 76, loss: 1.5099472999572754\n",
      "Epoch 19, batch 70 / 76, loss: 1.4381129741668701\n",
      "Epoch 19, batch 71 / 76, loss: 1.4784326553344727\n",
      "Epoch 19, batch 72 / 76, loss: 1.4294308423995972\n",
      "Epoch 19, batch 73 / 76, loss: 1.4051783084869385\n",
      "Epoch 19, batch 74 / 76, loss: 1.4956774711608887\n",
      "Epoch 19, batch 75 / 76, loss: 1.4937692880630493\n",
      "Epoch 19, batch 76 / 76, loss: 1.4249504804611206\n",
      "Epoch 19, batch 77 / 76, loss: 1.5625946521759033\n",
      "Epoch 20, batch 1 / 76, loss: 1.417048692703247\n",
      "Epoch 20, batch 2 / 76, loss: 1.4596922397613525\n",
      "Epoch 20, batch 3 / 76, loss: 1.4574427604675293\n",
      "Epoch 20, batch 4 / 76, loss: 1.3896946907043457\n",
      "Epoch 20, batch 5 / 76, loss: 1.4876585006713867\n",
      "Epoch 20, batch 6 / 76, loss: 1.468820333480835\n",
      "Epoch 20, batch 7 / 76, loss: 1.4424405097961426\n",
      "Epoch 20, batch 8 / 76, loss: 1.4876527786254883\n",
      "Epoch 20, batch 9 / 76, loss: 1.3237757682800293\n",
      "Epoch 20, batch 10 / 76, loss: 1.3453333377838135\n",
      "Epoch 20, batch 11 / 76, loss: 1.4552865028381348\n",
      "Epoch 20, batch 12 / 76, loss: 1.4669594764709473\n",
      "Epoch 20, batch 13 / 76, loss: 1.4783575534820557\n",
      "Epoch 20, batch 14 / 76, loss: 1.4459861516952515\n",
      "Epoch 20, batch 15 / 76, loss: 1.425408124923706\n",
      "Epoch 20, batch 16 / 76, loss: 1.366290807723999\n",
      "Epoch 20, batch 17 / 76, loss: 1.4446871280670166\n",
      "Epoch 20, batch 18 / 76, loss: 1.4443581104278564\n",
      "Epoch 20, batch 19 / 76, loss: 1.4806063175201416\n",
      "Epoch 20, batch 20 / 76, loss: 1.4022045135498047\n",
      "Epoch 20, batch 21 / 76, loss: 1.4480173587799072\n",
      "Epoch 20, batch 22 / 76, loss: 1.5031617879867554\n",
      "Epoch 20, batch 23 / 76, loss: 1.3564190864562988\n",
      "Epoch 20, batch 24 / 76, loss: 1.4902393817901611\n",
      "Epoch 20, batch 25 / 76, loss: 1.4029500484466553\n",
      "Epoch 20, batch 26 / 76, loss: 1.4336252212524414\n",
      "Epoch 20, batch 27 / 76, loss: 1.5566270351409912\n",
      "Epoch 20, batch 28 / 76, loss: 1.433529257774353\n",
      "Epoch 20, batch 29 / 76, loss: 1.4676299095153809\n",
      "Epoch 20, batch 30 / 76, loss: 1.4351940155029297\n",
      "Epoch 20, batch 31 / 76, loss: 1.4901518821716309\n",
      "Epoch 20, batch 32 / 76, loss: 1.5090413093566895\n",
      "Epoch 20, batch 33 / 76, loss: 1.4098272323608398\n",
      "Epoch 20, batch 34 / 76, loss: 1.4316515922546387\n",
      "Epoch 20, batch 35 / 76, loss: 1.536417007446289\n",
      "Epoch 20, batch 36 / 76, loss: 1.4619977474212646\n",
      "Epoch 20, batch 37 / 76, loss: 1.431458592414856\n",
      "Epoch 20, batch 38 / 76, loss: 1.4217381477355957\n",
      "Epoch 20, batch 39 / 76, loss: 1.426027536392212\n",
      "Epoch 20, batch 40 / 76, loss: 1.4578182697296143\n",
      "Epoch 20, batch 41 / 76, loss: 1.49474036693573\n",
      "Epoch 20, batch 42 / 76, loss: 1.4643641710281372\n",
      "Epoch 20, batch 43 / 76, loss: 1.4270613193511963\n",
      "Epoch 20, batch 44 / 76, loss: 1.4507997035980225\n",
      "Epoch 20, batch 45 / 76, loss: 1.4503405094146729\n",
      "Epoch 20, batch 46 / 76, loss: 1.361091136932373\n",
      "Epoch 20, batch 47 / 76, loss: 1.4658279418945312\n",
      "Epoch 20, batch 48 / 76, loss: 1.4126548767089844\n",
      "Epoch 20, batch 49 / 76, loss: 1.4694000482559204\n",
      "Epoch 20, batch 50 / 76, loss: 1.3983359336853027\n",
      "Epoch 20, batch 51 / 76, loss: 1.4425740242004395\n",
      "Epoch 20, batch 52 / 76, loss: 1.36122727394104\n",
      "Epoch 20, batch 53 / 76, loss: 1.477935552597046\n",
      "Epoch 20, batch 54 / 76, loss: 1.4310165643692017\n",
      "Epoch 20, batch 55 / 76, loss: 1.4566607475280762\n",
      "Epoch 20, batch 56 / 76, loss: 1.3758673667907715\n",
      "Epoch 20, batch 57 / 76, loss: 1.4811525344848633\n",
      "Epoch 20, batch 58 / 76, loss: 1.429328441619873\n",
      "Epoch 20, batch 59 / 76, loss: 1.3972671031951904\n",
      "Epoch 20, batch 60 / 76, loss: 1.4176748991012573\n",
      "Epoch 20, batch 61 / 76, loss: 1.3632357120513916\n",
      "Epoch 20, batch 62 / 76, loss: 1.454294204711914\n",
      "Epoch 20, batch 63 / 76, loss: 1.4288885593414307\n",
      "Epoch 20, batch 64 / 76, loss: 1.3733917474746704\n",
      "Epoch 20, batch 65 / 76, loss: 1.5173457860946655\n",
      "Epoch 20, batch 66 / 76, loss: 1.4058589935302734\n",
      "Epoch 20, batch 67 / 76, loss: 1.3611676692962646\n",
      "Epoch 20, batch 68 / 76, loss: 1.4740920066833496\n",
      "Epoch 20, batch 69 / 76, loss: 1.3977253437042236\n",
      "Epoch 20, batch 70 / 76, loss: 1.376373052597046\n",
      "Epoch 20, batch 71 / 76, loss: 1.4073817729949951\n",
      "Epoch 20, batch 72 / 76, loss: 1.4753708839416504\n",
      "Epoch 20, batch 73 / 76, loss: 1.363258719444275\n",
      "Epoch 20, batch 74 / 76, loss: 1.4802781343460083\n",
      "Epoch 20, batch 75 / 76, loss: 1.4042818546295166\n",
      "Epoch 20, batch 76 / 76, loss: 1.4318573474884033\n",
      "Epoch 20, batch 77 / 76, loss: 1.4628194570541382\n",
      "Epoch 21, batch 1 / 76, loss: 1.4140583276748657\n",
      "Epoch 21, batch 2 / 76, loss: 1.3708813190460205\n",
      "Epoch 21, batch 3 / 76, loss: 1.4534403085708618\n",
      "Epoch 21, batch 4 / 76, loss: 1.3586158752441406\n",
      "Epoch 21, batch 5 / 76, loss: 1.4418599605560303\n",
      "Epoch 21, batch 6 / 76, loss: 1.3825774192810059\n",
      "Epoch 21, batch 7 / 76, loss: 1.3169993162155151\n",
      "Epoch 21, batch 8 / 76, loss: 1.4048595428466797\n",
      "Epoch 21, batch 9 / 76, loss: 1.425707459449768\n",
      "Epoch 21, batch 10 / 76, loss: 1.4730868339538574\n",
      "Epoch 21, batch 11 / 76, loss: 1.4514172077178955\n",
      "Epoch 21, batch 12 / 76, loss: 1.419089674949646\n",
      "Epoch 21, batch 13 / 76, loss: 1.4503099918365479\n",
      "Epoch 21, batch 14 / 76, loss: 1.4257738590240479\n",
      "Epoch 21, batch 15 / 76, loss: 1.5269560813903809\n",
      "Epoch 21, batch 16 / 76, loss: 1.470991611480713\n",
      "Epoch 21, batch 17 / 76, loss: 1.409772515296936\n",
      "Epoch 21, batch 18 / 76, loss: 1.358940601348877\n",
      "Epoch 21, batch 19 / 76, loss: 1.4497556686401367\n",
      "Epoch 21, batch 20 / 76, loss: 1.4817321300506592\n",
      "Epoch 21, batch 21 / 76, loss: 1.4391474723815918\n",
      "Epoch 21, batch 22 / 76, loss: 1.413008213043213\n",
      "Epoch 21, batch 23 / 76, loss: 1.3906272649765015\n",
      "Epoch 21, batch 24 / 76, loss: 1.4326717853546143\n",
      "Epoch 21, batch 25 / 76, loss: 1.5456594228744507\n",
      "Epoch 21, batch 26 / 76, loss: 1.4130566120147705\n",
      "Epoch 21, batch 27 / 76, loss: 1.4364306926727295\n",
      "Epoch 21, batch 28 / 76, loss: 1.4725821018218994\n",
      "Epoch 21, batch 29 / 76, loss: 1.379392385482788\n",
      "Epoch 21, batch 30 / 76, loss: 1.4885611534118652\n",
      "Epoch 21, batch 31 / 76, loss: 1.4001684188842773\n",
      "Epoch 21, batch 32 / 76, loss: 1.3699631690979004\n",
      "Epoch 21, batch 33 / 76, loss: 1.4652446508407593\n",
      "Epoch 21, batch 34 / 76, loss: 1.4141494035720825\n",
      "Epoch 21, batch 35 / 76, loss: 1.4521242380142212\n",
      "Epoch 21, batch 36 / 76, loss: 1.54874587059021\n",
      "Epoch 21, batch 37 / 76, loss: 1.4059011936187744\n",
      "Epoch 21, batch 38 / 76, loss: 1.4041309356689453\n",
      "Epoch 21, batch 39 / 76, loss: 1.4127424955368042\n",
      "Epoch 21, batch 40 / 76, loss: 1.5155527591705322\n",
      "Epoch 21, batch 41 / 76, loss: 1.413133144378662\n",
      "Epoch 21, batch 42 / 76, loss: 1.4812006950378418\n",
      "Epoch 21, batch 43 / 76, loss: 1.5157792568206787\n",
      "Epoch 21, batch 44 / 76, loss: 1.3870577812194824\n",
      "Epoch 21, batch 45 / 76, loss: 1.4364460706710815\n",
      "Epoch 21, batch 46 / 76, loss: 1.4069339036941528\n",
      "Epoch 21, batch 47 / 76, loss: 1.448517084121704\n",
      "Epoch 21, batch 48 / 76, loss: 1.3934624195098877\n",
      "Epoch 21, batch 49 / 76, loss: 1.4466023445129395\n",
      "Epoch 21, batch 50 / 76, loss: 1.418271780014038\n",
      "Epoch 21, batch 51 / 76, loss: 1.4220284223556519\n",
      "Epoch 21, batch 52 / 76, loss: 1.4286924600601196\n",
      "Epoch 21, batch 53 / 76, loss: 1.4580988883972168\n",
      "Epoch 21, batch 54 / 76, loss: 1.4293041229248047\n",
      "Epoch 21, batch 55 / 76, loss: 1.454613208770752\n",
      "Epoch 21, batch 56 / 76, loss: 1.452460527420044\n",
      "Epoch 21, batch 57 / 76, loss: 1.4517805576324463\n",
      "Epoch 21, batch 58 / 76, loss: 1.4216563701629639\n",
      "Epoch 21, batch 59 / 76, loss: 1.3512225151062012\n",
      "Epoch 21, batch 60 / 76, loss: 1.3981010913848877\n",
      "Epoch 21, batch 61 / 76, loss: 1.373995065689087\n",
      "Epoch 21, batch 62 / 76, loss: 1.3373699188232422\n",
      "Epoch 21, batch 63 / 76, loss: 1.366403341293335\n",
      "Epoch 21, batch 64 / 76, loss: 1.249098539352417\n",
      "Epoch 21, batch 65 / 76, loss: 1.297410011291504\n",
      "Epoch 21, batch 66 / 76, loss: 1.334615707397461\n",
      "Epoch 21, batch 67 / 76, loss: 1.288675308227539\n",
      "Epoch 21, batch 68 / 76, loss: 1.2957041263580322\n",
      "Epoch 21, batch 69 / 76, loss: 1.2921767234802246\n",
      "Epoch 21, batch 70 / 76, loss: 1.3208212852478027\n",
      "Epoch 21, batch 71 / 76, loss: 1.313793420791626\n",
      "Epoch 21, batch 72 / 76, loss: 1.312772274017334\n",
      "Epoch 21, batch 73 / 76, loss: 1.320939064025879\n",
      "Epoch 21, batch 74 / 76, loss: 1.3121894598007202\n",
      "Epoch 21, batch 75 / 76, loss: 1.2702577114105225\n",
      "Epoch 21, batch 76 / 76, loss: 1.2532963752746582\n",
      "Epoch 21, batch 77 / 76, loss: 1.465662956237793\n",
      "Epoch 22, batch 1 / 76, loss: 1.2838627099990845\n",
      "Epoch 22, batch 2 / 76, loss: 1.2219290733337402\n",
      "Epoch 22, batch 3 / 76, loss: 1.2512578964233398\n",
      "Epoch 22, batch 4 / 76, loss: 1.2356568574905396\n",
      "Epoch 22, batch 5 / 76, loss: 1.2070176601409912\n",
      "Epoch 22, batch 6 / 76, loss: 1.2267036437988281\n",
      "Epoch 22, batch 7 / 76, loss: 1.2037960290908813\n",
      "Epoch 22, batch 8 / 76, loss: 1.2258442640304565\n",
      "Epoch 22, batch 9 / 76, loss: 1.2751836776733398\n",
      "Epoch 22, batch 10 / 76, loss: 1.2249104976654053\n",
      "Epoch 22, batch 11 / 76, loss: 1.2504029273986816\n",
      "Epoch 22, batch 12 / 76, loss: 1.3170825242996216\n",
      "Epoch 22, batch 13 / 76, loss: 1.1601839065551758\n",
      "Epoch 22, batch 14 / 76, loss: 1.2024755477905273\n",
      "Epoch 22, batch 15 / 76, loss: 1.2785186767578125\n",
      "Epoch 22, batch 16 / 76, loss: 1.1505616903305054\n",
      "Epoch 22, batch 17 / 76, loss: 1.186530351638794\n",
      "Epoch 22, batch 18 / 76, loss: 1.1359951496124268\n",
      "Epoch 22, batch 19 / 76, loss: 1.2342064380645752\n",
      "Epoch 22, batch 20 / 76, loss: 1.1615841388702393\n",
      "Epoch 22, batch 21 / 76, loss: 1.1984710693359375\n",
      "Epoch 22, batch 22 / 76, loss: 1.1848158836364746\n",
      "Epoch 22, batch 23 / 76, loss: 1.18985915184021\n",
      "Epoch 22, batch 24 / 76, loss: 1.198129653930664\n",
      "Epoch 22, batch 25 / 76, loss: 1.102649211883545\n",
      "Epoch 22, batch 26 / 76, loss: 1.1358036994934082\n",
      "Epoch 22, batch 27 / 76, loss: 1.1565155982971191\n",
      "Epoch 22, batch 28 / 76, loss: 1.1858841180801392\n",
      "Epoch 22, batch 29 / 76, loss: 1.1511907577514648\n",
      "Epoch 22, batch 30 / 76, loss: 1.0971097946166992\n",
      "Epoch 22, batch 31 / 76, loss: 1.1521813869476318\n",
      "Epoch 22, batch 32 / 76, loss: 1.1272615194320679\n",
      "Epoch 22, batch 33 / 76, loss: 1.150118112564087\n",
      "Epoch 22, batch 34 / 76, loss: 1.1853322982788086\n",
      "Epoch 22, batch 35 / 76, loss: 1.2060353755950928\n",
      "Epoch 22, batch 36 / 76, loss: 1.1263586282730103\n",
      "Epoch 22, batch 37 / 76, loss: 1.1111221313476562\n",
      "Epoch 22, batch 38 / 76, loss: 1.1385326385498047\n",
      "Epoch 22, batch 39 / 76, loss: 1.138720154762268\n",
      "Epoch 22, batch 40 / 76, loss: 1.1795861721038818\n",
      "Epoch 22, batch 41 / 76, loss: 1.123192548751831\n",
      "Epoch 22, batch 42 / 76, loss: 1.071724534034729\n",
      "Epoch 22, batch 43 / 76, loss: 1.1666772365570068\n",
      "Epoch 22, batch 44 / 76, loss: 1.1386194229125977\n",
      "Epoch 22, batch 45 / 76, loss: 1.1581878662109375\n",
      "Epoch 22, batch 46 / 76, loss: 1.0830628871917725\n",
      "Epoch 22, batch 47 / 76, loss: 1.0860066413879395\n",
      "Epoch 22, batch 48 / 76, loss: 1.0830302238464355\n",
      "Epoch 22, batch 49 / 76, loss: 1.1616358757019043\n",
      "Epoch 22, batch 50 / 76, loss: 1.1272834539413452\n",
      "Epoch 22, batch 51 / 76, loss: 1.185398817062378\n",
      "Epoch 22, batch 52 / 76, loss: 1.203743815422058\n",
      "Epoch 22, batch 53 / 76, loss: 1.181506872177124\n",
      "Epoch 22, batch 54 / 76, loss: 1.1160677671432495\n",
      "Epoch 22, batch 55 / 76, loss: 1.1473045349121094\n",
      "Epoch 22, batch 56 / 76, loss: 1.1002693176269531\n",
      "Epoch 22, batch 57 / 76, loss: 1.118079662322998\n",
      "Epoch 22, batch 58 / 76, loss: 1.1220165491104126\n",
      "Epoch 22, batch 59 / 76, loss: 1.1182518005371094\n",
      "Epoch 22, batch 60 / 76, loss: 1.104745864868164\n",
      "Epoch 22, batch 61 / 76, loss: 1.203233242034912\n",
      "Epoch 22, batch 62 / 76, loss: 1.1688477993011475\n",
      "Epoch 22, batch 63 / 76, loss: 1.151047706604004\n",
      "Epoch 22, batch 64 / 76, loss: 1.1062607765197754\n",
      "Epoch 22, batch 65 / 76, loss: 1.144585132598877\n",
      "Epoch 22, batch 66 / 76, loss: 1.0758100748062134\n",
      "Epoch 22, batch 67 / 76, loss: 1.1567038297653198\n",
      "Epoch 22, batch 68 / 76, loss: 1.1826696395874023\n",
      "Epoch 22, batch 69 / 76, loss: 1.1037449836730957\n",
      "Epoch 22, batch 70 / 76, loss: 1.196157455444336\n",
      "Epoch 22, batch 71 / 76, loss: 1.1579432487487793\n",
      "Epoch 22, batch 72 / 76, loss: 1.1486396789550781\n",
      "Epoch 22, batch 73 / 76, loss: 1.1716092824935913\n",
      "Epoch 22, batch 74 / 76, loss: 1.1602702140808105\n",
      "Epoch 22, batch 75 / 76, loss: 1.1084277629852295\n",
      "Epoch 22, batch 76 / 76, loss: 1.1829817295074463\n",
      "Epoch 22, batch 77 / 76, loss: 1.1351324319839478\n",
      "Epoch 23, batch 1 / 76, loss: 1.1065421104431152\n",
      "Epoch 23, batch 2 / 76, loss: 1.123060703277588\n",
      "Epoch 23, batch 3 / 76, loss: 1.1528174877166748\n",
      "Epoch 23, batch 4 / 76, loss: 1.1517727375030518\n",
      "Epoch 23, batch 5 / 76, loss: 1.196448802947998\n",
      "Epoch 23, batch 6 / 76, loss: 1.1337597370147705\n",
      "Epoch 23, batch 7 / 76, loss: 1.155585527420044\n",
      "Epoch 23, batch 8 / 76, loss: 1.1983451843261719\n",
      "Epoch 23, batch 9 / 76, loss: 1.0934419631958008\n",
      "Epoch 23, batch 10 / 76, loss: 1.1801602840423584\n",
      "Epoch 23, batch 11 / 76, loss: 1.1665480136871338\n",
      "Epoch 23, batch 12 / 76, loss: 1.1869127750396729\n",
      "Epoch 23, batch 13 / 76, loss: 1.178030014038086\n",
      "Epoch 23, batch 14 / 76, loss: 1.2290425300598145\n",
      "Epoch 23, batch 15 / 76, loss: 1.138892650604248\n",
      "Epoch 23, batch 16 / 76, loss: 1.1713263988494873\n",
      "Epoch 23, batch 17 / 76, loss: 1.109987497329712\n",
      "Epoch 23, batch 18 / 76, loss: 1.1419265270233154\n",
      "Epoch 23, batch 19 / 76, loss: 1.0887696743011475\n",
      "Epoch 23, batch 20 / 76, loss: 1.077589750289917\n",
      "Epoch 23, batch 21 / 76, loss: 1.0879642963409424\n",
      "Epoch 23, batch 22 / 76, loss: 1.1487067937850952\n",
      "Epoch 23, batch 23 / 76, loss: 1.1728951930999756\n",
      "Epoch 23, batch 24 / 76, loss: 1.260258674621582\n",
      "Epoch 23, batch 25 / 76, loss: 1.1241693496704102\n",
      "Epoch 23, batch 26 / 76, loss: 1.1275179386138916\n",
      "Epoch 23, batch 27 / 76, loss: 1.1275534629821777\n",
      "Epoch 23, batch 28 / 76, loss: 1.1741490364074707\n",
      "Epoch 23, batch 29 / 76, loss: 1.2055343389511108\n",
      "Epoch 23, batch 30 / 76, loss: 1.1624796390533447\n",
      "Epoch 23, batch 31 / 76, loss: 1.1453801393508911\n",
      "Epoch 23, batch 32 / 76, loss: 1.1138789653778076\n",
      "Epoch 23, batch 33 / 76, loss: 1.0977128744125366\n",
      "Epoch 23, batch 34 / 76, loss: 1.151221513748169\n",
      "Epoch 23, batch 35 / 76, loss: 1.0864765644073486\n",
      "Epoch 23, batch 36 / 76, loss: 1.1750373840332031\n",
      "Epoch 23, batch 37 / 76, loss: 1.1698198318481445\n",
      "Epoch 23, batch 38 / 76, loss: 1.1247189044952393\n",
      "Epoch 23, batch 39 / 76, loss: 1.1569764614105225\n",
      "Epoch 23, batch 40 / 76, loss: 1.125510811805725\n",
      "Epoch 23, batch 41 / 76, loss: 1.08266282081604\n",
      "Epoch 23, batch 42 / 76, loss: 1.1551202535629272\n",
      "Epoch 23, batch 43 / 76, loss: 1.119734525680542\n",
      "Epoch 23, batch 44 / 76, loss: 1.133164644241333\n",
      "Epoch 23, batch 45 / 76, loss: 1.1210577487945557\n",
      "Epoch 23, batch 46 / 76, loss: 1.1389919519424438\n",
      "Epoch 23, batch 47 / 76, loss: 1.0884532928466797\n",
      "Epoch 23, batch 48 / 76, loss: 1.113640546798706\n",
      "Epoch 23, batch 49 / 76, loss: 1.0963237285614014\n",
      "Epoch 23, batch 50 / 76, loss: 1.2027453184127808\n",
      "Epoch 23, batch 51 / 76, loss: 1.1424376964569092\n",
      "Epoch 23, batch 52 / 76, loss: 1.160717487335205\n",
      "Epoch 23, batch 53 / 76, loss: 1.1680781841278076\n",
      "Epoch 23, batch 54 / 76, loss: 1.1646122932434082\n",
      "Epoch 23, batch 55 / 76, loss: 1.1605805158615112\n",
      "Epoch 23, batch 56 / 76, loss: 1.1645667552947998\n",
      "Epoch 23, batch 57 / 76, loss: 1.114912986755371\n",
      "Epoch 23, batch 58 / 76, loss: 1.1710659265518188\n",
      "Epoch 23, batch 59 / 76, loss: 1.1891052722930908\n",
      "Epoch 23, batch 60 / 76, loss: 1.1333937644958496\n",
      "Epoch 23, batch 61 / 76, loss: 1.1244804859161377\n",
      "Epoch 23, batch 62 / 76, loss: 1.1667413711547852\n",
      "Epoch 23, batch 63 / 76, loss: 1.1157885789871216\n",
      "Epoch 23, batch 64 / 76, loss: 1.145183801651001\n",
      "Epoch 23, batch 65 / 76, loss: 1.1115670204162598\n",
      "Epoch 23, batch 66 / 76, loss: 1.0917863845825195\n",
      "Epoch 23, batch 67 / 76, loss: 1.155937910079956\n",
      "Epoch 23, batch 68 / 76, loss: 1.159653663635254\n",
      "Epoch 23, batch 69 / 76, loss: 1.2276527881622314\n",
      "Epoch 23, batch 70 / 76, loss: 1.1706961393356323\n",
      "Epoch 23, batch 71 / 76, loss: 1.121846079826355\n",
      "Epoch 23, batch 72 / 76, loss: 1.1253478527069092\n",
      "Epoch 23, batch 73 / 76, loss: 1.1817781925201416\n",
      "Epoch 23, batch 74 / 76, loss: 1.1728549003601074\n",
      "Epoch 23, batch 75 / 76, loss: 1.1328225135803223\n",
      "Epoch 23, batch 76 / 76, loss: 1.11989164352417\n",
      "Epoch 23, batch 77 / 76, loss: 1.2094831466674805\n",
      "Epoch 24, batch 1 / 76, loss: 1.111238956451416\n",
      "Epoch 24, batch 2 / 76, loss: 1.1499240398406982\n",
      "Epoch 24, batch 3 / 76, loss: 1.1208410263061523\n",
      "Epoch 24, batch 4 / 76, loss: 1.1321051120758057\n",
      "Epoch 24, batch 5 / 76, loss: 1.138297200202942\n",
      "Epoch 24, batch 6 / 76, loss: 1.1294424533843994\n",
      "Epoch 24, batch 7 / 76, loss: 1.0959603786468506\n",
      "Epoch 24, batch 8 / 76, loss: 1.130784034729004\n",
      "Epoch 24, batch 9 / 76, loss: 1.126341462135315\n",
      "Epoch 24, batch 10 / 76, loss: 1.1059558391571045\n",
      "Epoch 24, batch 11 / 76, loss: 1.1837735176086426\n",
      "Epoch 24, batch 12 / 76, loss: 1.078523874282837\n",
      "Epoch 24, batch 13 / 76, loss: 1.1789823770523071\n",
      "Epoch 24, batch 14 / 76, loss: 1.106062889099121\n",
      "Epoch 24, batch 15 / 76, loss: 1.1454466581344604\n",
      "Epoch 24, batch 16 / 76, loss: 1.185523509979248\n",
      "Epoch 24, batch 17 / 76, loss: 1.1476290225982666\n",
      "Epoch 24, batch 18 / 76, loss: 1.1466807126998901\n",
      "Epoch 24, batch 19 / 76, loss: 1.1467061042785645\n",
      "Epoch 24, batch 20 / 76, loss: 1.2192678451538086\n",
      "Epoch 24, batch 21 / 76, loss: 1.1661646366119385\n",
      "Epoch 24, batch 22 / 76, loss: 1.1416406631469727\n",
      "Epoch 24, batch 23 / 76, loss: 1.1398537158966064\n",
      "Epoch 24, batch 24 / 76, loss: 1.126936435699463\n",
      "Epoch 24, batch 25 / 76, loss: 1.1565122604370117\n",
      "Epoch 24, batch 26 / 76, loss: 1.1497502326965332\n",
      "Epoch 24, batch 27 / 76, loss: 1.0857280492782593\n",
      "Epoch 24, batch 28 / 76, loss: 1.142425298690796\n",
      "Epoch 24, batch 29 / 76, loss: 1.1792478561401367\n",
      "Epoch 24, batch 30 / 76, loss: 1.1854342222213745\n",
      "Epoch 24, batch 31 / 76, loss: 1.1189228296279907\n",
      "Epoch 24, batch 32 / 76, loss: 1.1964977979660034\n",
      "Epoch 24, batch 33 / 76, loss: 1.1670516729354858\n",
      "Epoch 24, batch 34 / 76, loss: 1.1377309560775757\n",
      "Epoch 24, batch 35 / 76, loss: 1.0881271362304688\n",
      "Epoch 24, batch 36 / 76, loss: 1.1571872234344482\n",
      "Epoch 24, batch 37 / 76, loss: 1.1238267421722412\n",
      "Epoch 24, batch 38 / 76, loss: 1.1428587436676025\n",
      "Epoch 24, batch 39 / 76, loss: 1.1248112916946411\n",
      "Epoch 24, batch 40 / 76, loss: 1.1578013896942139\n",
      "Epoch 24, batch 41 / 76, loss: 1.1655480861663818\n",
      "Epoch 24, batch 42 / 76, loss: 1.1734678745269775\n",
      "Epoch 24, batch 43 / 76, loss: 1.0834944248199463\n",
      "Epoch 24, batch 44 / 76, loss: 1.1194688081741333\n",
      "Epoch 24, batch 45 / 76, loss: 1.138725996017456\n",
      "Epoch 24, batch 46 / 76, loss: 1.1405367851257324\n",
      "Epoch 24, batch 47 / 76, loss: 1.2153444290161133\n",
      "Epoch 24, batch 48 / 76, loss: 1.1771626472473145\n",
      "Epoch 24, batch 49 / 76, loss: 1.1431901454925537\n",
      "Epoch 24, batch 50 / 76, loss: 1.121645450592041\n",
      "Epoch 24, batch 51 / 76, loss: 1.1523618698120117\n",
      "Epoch 24, batch 52 / 76, loss: 1.1520700454711914\n",
      "Epoch 24, batch 53 / 76, loss: 1.199344515800476\n",
      "Epoch 24, batch 54 / 76, loss: 1.129966139793396\n",
      "Epoch 24, batch 55 / 76, loss: 1.1854246854782104\n",
      "Epoch 24, batch 56 / 76, loss: 1.1769790649414062\n",
      "Epoch 24, batch 57 / 76, loss: 1.220826268196106\n",
      "Epoch 24, batch 58 / 76, loss: 1.087716817855835\n",
      "Epoch 24, batch 59 / 76, loss: 1.158198595046997\n",
      "Epoch 24, batch 60 / 76, loss: 1.1658260822296143\n",
      "Epoch 24, batch 61 / 76, loss: 1.1088080406188965\n",
      "Epoch 24, batch 62 / 76, loss: 1.1623237133026123\n",
      "Epoch 24, batch 63 / 76, loss: 1.1598351001739502\n",
      "Epoch 24, batch 64 / 76, loss: 1.098625659942627\n",
      "Epoch 24, batch 65 / 76, loss: 1.101302981376648\n",
      "Epoch 24, batch 66 / 76, loss: 1.1373422145843506\n",
      "Epoch 24, batch 67 / 76, loss: 1.1920862197875977\n",
      "Epoch 24, batch 68 / 76, loss: 1.1316897869110107\n",
      "Epoch 24, batch 69 / 76, loss: 1.1381090879440308\n",
      "Epoch 24, batch 70 / 76, loss: 1.1379754543304443\n",
      "Epoch 24, batch 71 / 76, loss: 1.1301567554473877\n",
      "Epoch 24, batch 72 / 76, loss: 1.1494395732879639\n",
      "Epoch 24, batch 73 / 76, loss: 1.0604112148284912\n",
      "Epoch 24, batch 74 / 76, loss: 1.0592396259307861\n",
      "Epoch 24, batch 75 / 76, loss: 1.154789924621582\n",
      "Epoch 24, batch 76 / 76, loss: 1.1229145526885986\n",
      "Epoch 24, batch 77 / 76, loss: 1.2004790306091309\n",
      "Epoch 25, batch 1 / 76, loss: 1.148280382156372\n",
      "Epoch 25, batch 2 / 76, loss: 1.1901626586914062\n",
      "Epoch 25, batch 3 / 76, loss: 1.157640814781189\n",
      "Epoch 25, batch 4 / 76, loss: 1.1520276069641113\n",
      "Epoch 25, batch 5 / 76, loss: 1.2389147281646729\n",
      "Epoch 25, batch 6 / 76, loss: 1.1434848308563232\n",
      "Epoch 25, batch 7 / 76, loss: 1.1720587015151978\n",
      "Epoch 25, batch 8 / 76, loss: 1.1072607040405273\n",
      "Epoch 25, batch 9 / 76, loss: 1.1237907409667969\n",
      "Epoch 25, batch 10 / 76, loss: 1.1600655317306519\n",
      "Epoch 25, batch 11 / 76, loss: 1.070959210395813\n",
      "Epoch 25, batch 12 / 76, loss: 1.134634256362915\n",
      "Epoch 25, batch 13 / 76, loss: 1.1234140396118164\n",
      "Epoch 25, batch 14 / 76, loss: 1.1019632816314697\n",
      "Epoch 25, batch 15 / 76, loss: 1.143953561782837\n",
      "Epoch 25, batch 16 / 76, loss: 1.1496492624282837\n",
      "Epoch 25, batch 17 / 76, loss: 1.213193655014038\n",
      "Epoch 25, batch 18 / 76, loss: 1.1673548221588135\n",
      "Epoch 25, batch 19 / 76, loss: 1.131150245666504\n",
      "Epoch 25, batch 20 / 76, loss: 1.1276936531066895\n",
      "Epoch 25, batch 21 / 76, loss: 1.1226685047149658\n",
      "Epoch 25, batch 22 / 76, loss: 1.1112443208694458\n",
      "Epoch 25, batch 23 / 76, loss: 1.1115068197250366\n",
      "Epoch 25, batch 24 / 76, loss: 1.1298303604125977\n",
      "Epoch 25, batch 25 / 76, loss: 1.155875563621521\n",
      "Epoch 25, batch 26 / 76, loss: 1.1282049417495728\n",
      "Epoch 25, batch 27 / 76, loss: 1.1138091087341309\n",
      "Epoch 25, batch 28 / 76, loss: 1.1921217441558838\n",
      "Epoch 25, batch 29 / 76, loss: 1.166223168373108\n",
      "Epoch 25, batch 30 / 76, loss: 1.1966174840927124\n",
      "Epoch 25, batch 31 / 76, loss: 1.1555860042572021\n",
      "Epoch 25, batch 32 / 76, loss: 1.1216514110565186\n",
      "Epoch 25, batch 33 / 76, loss: 1.1807793378829956\n",
      "Epoch 25, batch 34 / 76, loss: 1.1398733854293823\n",
      "Epoch 25, batch 35 / 76, loss: 1.1187787055969238\n",
      "Epoch 25, batch 36 / 76, loss: 1.176102876663208\n",
      "Epoch 25, batch 37 / 76, loss: 1.1034274101257324\n",
      "Epoch 25, batch 38 / 76, loss: 1.142029047012329\n",
      "Epoch 25, batch 39 / 76, loss: 1.0990355014801025\n",
      "Epoch 25, batch 40 / 76, loss: 1.1357660293579102\n",
      "Epoch 25, batch 41 / 76, loss: 1.1611840724945068\n",
      "Epoch 25, batch 42 / 76, loss: 1.1567655801773071\n",
      "Epoch 25, batch 43 / 76, loss: 1.118969202041626\n",
      "Epoch 25, batch 44 / 76, loss: 1.1029202938079834\n",
      "Epoch 25, batch 45 / 76, loss: 1.080612301826477\n",
      "Epoch 25, batch 46 / 76, loss: 1.1756854057312012\n",
      "Epoch 25, batch 47 / 76, loss: 1.1551883220672607\n",
      "Epoch 25, batch 48 / 76, loss: 1.1414400339126587\n",
      "Epoch 25, batch 49 / 76, loss: 1.1742498874664307\n",
      "Epoch 25, batch 50 / 76, loss: 1.1512787342071533\n",
      "Epoch 25, batch 51 / 76, loss: 1.1545655727386475\n",
      "Epoch 25, batch 52 / 76, loss: 1.0979816913604736\n",
      "Epoch 25, batch 53 / 76, loss: 1.1669079065322876\n",
      "Epoch 25, batch 54 / 76, loss: 1.0765751600265503\n",
      "Epoch 25, batch 55 / 76, loss: 1.1206128597259521\n",
      "Epoch 25, batch 56 / 76, loss: 1.125144124031067\n",
      "Epoch 25, batch 57 / 76, loss: 1.117959976196289\n",
      "Epoch 25, batch 58 / 76, loss: 1.0733873844146729\n",
      "Epoch 25, batch 59 / 76, loss: 1.1383635997772217\n",
      "Epoch 25, batch 60 / 76, loss: 1.1159706115722656\n",
      "Epoch 25, batch 61 / 76, loss: 1.0869367122650146\n",
      "Epoch 25, batch 62 / 76, loss: 1.1542236804962158\n",
      "Epoch 25, batch 63 / 76, loss: 1.106655240058899\n",
      "Epoch 25, batch 64 / 76, loss: 1.1478838920593262\n",
      "Epoch 25, batch 65 / 76, loss: 1.120539903640747\n",
      "Epoch 25, batch 66 / 76, loss: 1.10481595993042\n",
      "Epoch 25, batch 67 / 76, loss: 1.1036219596862793\n",
      "Epoch 25, batch 68 / 76, loss: 1.068966031074524\n",
      "Epoch 25, batch 69 / 76, loss: 1.1336171627044678\n",
      "Epoch 25, batch 70 / 76, loss: 1.1045246124267578\n",
      "Epoch 25, batch 71 / 76, loss: 1.1372841596603394\n",
      "Epoch 25, batch 72 / 76, loss: 1.129289150238037\n",
      "Epoch 25, batch 73 / 76, loss: 1.1441254615783691\n",
      "Epoch 25, batch 74 / 76, loss: 1.168508768081665\n",
      "Epoch 25, batch 75 / 76, loss: 1.1706314086914062\n",
      "Epoch 25, batch 76 / 76, loss: 1.0829498767852783\n",
      "Epoch 25, batch 77 / 76, loss: 1.167691946029663\n",
      "Epoch 26, batch 1 / 76, loss: 1.1621979475021362\n",
      "Epoch 26, batch 2 / 76, loss: 1.139542579650879\n",
      "Epoch 26, batch 3 / 76, loss: 1.086387276649475\n",
      "Epoch 26, batch 4 / 76, loss: 1.1266893148422241\n",
      "Epoch 26, batch 5 / 76, loss: 1.1385418176651\n",
      "Epoch 26, batch 6 / 76, loss: 1.1466954946517944\n",
      "Epoch 26, batch 7 / 76, loss: 1.1387269496917725\n",
      "Epoch 26, batch 8 / 76, loss: 1.097428798675537\n",
      "Epoch 26, batch 9 / 76, loss: 1.1985867023468018\n",
      "Epoch 26, batch 10 / 76, loss: 1.1115498542785645\n",
      "Epoch 26, batch 11 / 76, loss: 1.1446349620819092\n",
      "Epoch 26, batch 12 / 76, loss: 1.113720417022705\n",
      "Epoch 26, batch 13 / 76, loss: 1.1207592487335205\n",
      "Epoch 26, batch 14 / 76, loss: 1.0406417846679688\n",
      "Epoch 26, batch 15 / 76, loss: 1.1198992729187012\n",
      "Epoch 26, batch 16 / 76, loss: 1.1353323459625244\n",
      "Epoch 26, batch 17 / 76, loss: 1.0741796493530273\n",
      "Epoch 26, batch 18 / 76, loss: 1.121946096420288\n",
      "Epoch 26, batch 19 / 76, loss: 1.1368052959442139\n",
      "Epoch 26, batch 20 / 76, loss: 1.1189112663269043\n",
      "Epoch 26, batch 21 / 76, loss: 1.1779935359954834\n",
      "Epoch 26, batch 22 / 76, loss: 1.0790297985076904\n",
      "Epoch 26, batch 23 / 76, loss: 1.1598701477050781\n",
      "Epoch 26, batch 24 / 76, loss: 1.0928435325622559\n",
      "Epoch 26, batch 25 / 76, loss: 1.155653476715088\n",
      "Epoch 26, batch 26 / 76, loss: 1.1092511415481567\n",
      "Epoch 26, batch 27 / 76, loss: 1.093412160873413\n",
      "Epoch 26, batch 28 / 76, loss: 1.1165817975997925\n",
      "Epoch 26, batch 29 / 76, loss: 1.16413152217865\n",
      "Epoch 26, batch 30 / 76, loss: 1.1098639965057373\n",
      "Epoch 26, batch 31 / 76, loss: 1.0563483238220215\n",
      "Epoch 26, batch 32 / 76, loss: 1.1282018423080444\n",
      "Epoch 26, batch 33 / 76, loss: 1.1626973152160645\n",
      "Epoch 26, batch 34 / 76, loss: 1.191249132156372\n",
      "Epoch 26, batch 35 / 76, loss: 1.0973258018493652\n",
      "Epoch 26, batch 36 / 76, loss: 1.1369376182556152\n",
      "Epoch 26, batch 37 / 76, loss: 1.1655080318450928\n",
      "Epoch 26, batch 38 / 76, loss: 1.1083285808563232\n",
      "Epoch 26, batch 39 / 76, loss: 1.0799139738082886\n",
      "Epoch 26, batch 40 / 76, loss: 1.1391180753707886\n",
      "Epoch 26, batch 41 / 76, loss: 1.1026990413665771\n",
      "Epoch 26, batch 42 / 76, loss: 1.1306519508361816\n",
      "Epoch 26, batch 43 / 76, loss: 1.0979863405227661\n",
      "Epoch 26, batch 44 / 76, loss: 1.133760929107666\n",
      "Epoch 26, batch 45 / 76, loss: 1.1581623554229736\n",
      "Epoch 26, batch 46 / 76, loss: 1.1747636795043945\n",
      "Epoch 26, batch 47 / 76, loss: 1.06305730342865\n",
      "Epoch 26, batch 48 / 76, loss: 1.1075972318649292\n",
      "Epoch 26, batch 49 / 76, loss: 1.1521350145339966\n",
      "Epoch 26, batch 50 / 76, loss: 1.1013305187225342\n",
      "Epoch 26, batch 51 / 76, loss: 1.1437816619873047\n",
      "Epoch 26, batch 52 / 76, loss: 1.0397014617919922\n",
      "Epoch 26, batch 53 / 76, loss: 1.0813014507293701\n",
      "Epoch 26, batch 54 / 76, loss: 1.129448652267456\n",
      "Epoch 26, batch 55 / 76, loss: 1.1103253364562988\n",
      "Epoch 26, batch 56 / 76, loss: 1.072335958480835\n",
      "Epoch 26, batch 57 / 76, loss: 1.0428087711334229\n",
      "Epoch 26, batch 58 / 76, loss: 1.0425267219543457\n",
      "Epoch 26, batch 59 / 76, loss: 1.170870065689087\n",
      "Epoch 26, batch 60 / 76, loss: 1.1274818181991577\n",
      "Epoch 26, batch 61 / 76, loss: 1.1420948505401611\n",
      "Epoch 26, batch 62 / 76, loss: 1.1146453619003296\n",
      "Epoch 26, batch 63 / 76, loss: 1.162114143371582\n",
      "Epoch 26, batch 64 / 76, loss: 1.1501429080963135\n",
      "Epoch 26, batch 65 / 76, loss: 1.2102762460708618\n",
      "Epoch 26, batch 66 / 76, loss: 1.2404288053512573\n",
      "Epoch 26, batch 67 / 76, loss: 1.1000065803527832\n",
      "Epoch 26, batch 68 / 76, loss: 1.117619276046753\n",
      "Epoch 26, batch 69 / 76, loss: 1.1202564239501953\n",
      "Epoch 26, batch 70 / 76, loss: 1.1857271194458008\n",
      "Epoch 26, batch 71 / 76, loss: 1.0884344577789307\n",
      "Epoch 26, batch 72 / 76, loss: 1.1219877004623413\n",
      "Epoch 26, batch 73 / 76, loss: 1.0617668628692627\n",
      "Epoch 26, batch 74 / 76, loss: 1.0810850858688354\n",
      "Epoch 26, batch 75 / 76, loss: 1.1173017024993896\n",
      "Epoch 26, batch 76 / 76, loss: 1.1252284049987793\n",
      "Epoch 26, batch 77 / 76, loss: 1.00162672996521\n",
      "Epoch 27, batch 1 / 76, loss: 1.1844815015792847\n",
      "Epoch 27, batch 2 / 76, loss: 1.103766918182373\n",
      "Epoch 27, batch 3 / 76, loss: 1.1497701406478882\n",
      "Epoch 27, batch 4 / 76, loss: 1.1391595602035522\n",
      "Epoch 27, batch 5 / 76, loss: 1.0607450008392334\n",
      "Epoch 27, batch 6 / 76, loss: 1.1267988681793213\n",
      "Epoch 27, batch 7 / 76, loss: 1.0576772689819336\n",
      "Epoch 27, batch 8 / 76, loss: 1.0518141984939575\n",
      "Epoch 27, batch 9 / 76, loss: 1.1377630233764648\n",
      "Epoch 27, batch 10 / 76, loss: 1.0860044956207275\n",
      "Epoch 27, batch 11 / 76, loss: 1.1272438764572144\n",
      "Epoch 27, batch 12 / 76, loss: 1.0932058095932007\n",
      "Epoch 27, batch 13 / 76, loss: 1.0522258281707764\n",
      "Epoch 27, batch 14 / 76, loss: 1.1195132732391357\n",
      "Epoch 27, batch 15 / 76, loss: 1.0634307861328125\n",
      "Epoch 27, batch 16 / 76, loss: 1.0817188024520874\n",
      "Epoch 27, batch 17 / 76, loss: 1.059554934501648\n",
      "Epoch 27, batch 18 / 76, loss: 1.1147187948226929\n",
      "Epoch 27, batch 19 / 76, loss: 1.1078228950500488\n",
      "Epoch 27, batch 20 / 76, loss: 1.0988329648971558\n",
      "Epoch 27, batch 21 / 76, loss: 1.2031588554382324\n",
      "Epoch 27, batch 22 / 76, loss: 1.0325498580932617\n",
      "Epoch 27, batch 23 / 76, loss: 1.1269385814666748\n",
      "Epoch 27, batch 24 / 76, loss: 1.1576786041259766\n",
      "Epoch 27, batch 25 / 76, loss: 1.1193115711212158\n",
      "Epoch 27, batch 26 / 76, loss: 1.1423697471618652\n",
      "Epoch 27, batch 27 / 76, loss: 1.1033657789230347\n",
      "Epoch 27, batch 28 / 76, loss: 1.086357831954956\n",
      "Epoch 27, batch 29 / 76, loss: 1.075993537902832\n",
      "Epoch 27, batch 30 / 76, loss: 1.1109039783477783\n",
      "Epoch 27, batch 31 / 76, loss: 1.099755048751831\n",
      "Epoch 27, batch 32 / 76, loss: 1.055670142173767\n",
      "Epoch 27, batch 33 / 76, loss: 1.1264913082122803\n",
      "Epoch 27, batch 34 / 76, loss: 1.0711631774902344\n",
      "Epoch 27, batch 35 / 76, loss: 1.074753761291504\n",
      "Epoch 27, batch 36 / 76, loss: 1.0498789548873901\n",
      "Epoch 27, batch 37 / 76, loss: 1.1407052278518677\n",
      "Epoch 27, batch 38 / 76, loss: 1.104625940322876\n",
      "Epoch 27, batch 39 / 76, loss: 1.0673936605453491\n",
      "Epoch 27, batch 40 / 76, loss: 1.0935382843017578\n",
      "Epoch 27, batch 41 / 76, loss: 1.1318378448486328\n",
      "Epoch 27, batch 42 / 76, loss: 1.1263494491577148\n",
      "Epoch 27, batch 43 / 76, loss: 1.1179280281066895\n",
      "Epoch 27, batch 44 / 76, loss: 1.0067639350891113\n",
      "Epoch 27, batch 45 / 76, loss: 1.0825412273406982\n",
      "Epoch 27, batch 46 / 76, loss: 1.0338014364242554\n",
      "Epoch 27, batch 47 / 76, loss: 1.0801188945770264\n",
      "Epoch 27, batch 48 / 76, loss: 1.032243013381958\n",
      "Epoch 27, batch 49 / 76, loss: 1.0494308471679688\n",
      "Epoch 27, batch 50 / 76, loss: 1.1000326871871948\n",
      "Epoch 27, batch 51 / 76, loss: 1.072056531906128\n",
      "Epoch 27, batch 52 / 76, loss: 1.1533578634262085\n",
      "Epoch 27, batch 53 / 76, loss: 1.1685680150985718\n",
      "Epoch 27, batch 54 / 76, loss: 1.0404200553894043\n",
      "Epoch 27, batch 55 / 76, loss: 1.0792601108551025\n",
      "Epoch 27, batch 56 / 76, loss: 1.1168485879898071\n",
      "Epoch 27, batch 57 / 76, loss: 1.1173458099365234\n",
      "Epoch 27, batch 58 / 76, loss: 1.1446869373321533\n",
      "Epoch 27, batch 59 / 76, loss: 1.0095067024230957\n",
      "Epoch 27, batch 60 / 76, loss: 1.1015355587005615\n",
      "Epoch 27, batch 61 / 76, loss: 1.0918205976486206\n",
      "Epoch 27, batch 62 / 76, loss: 1.1039388179779053\n",
      "Epoch 27, batch 63 / 76, loss: 1.1066782474517822\n",
      "Epoch 27, batch 64 / 76, loss: 1.0390669107437134\n",
      "Epoch 27, batch 65 / 76, loss: 1.108898639678955\n",
      "Epoch 27, batch 66 / 76, loss: 1.0862691402435303\n",
      "Epoch 27, batch 67 / 76, loss: 1.1059941053390503\n",
      "Epoch 27, batch 68 / 76, loss: 1.1002755165100098\n",
      "Epoch 27, batch 69 / 76, loss: 1.06229567527771\n",
      "Epoch 27, batch 70 / 76, loss: 1.0602778196334839\n",
      "Epoch 27, batch 71 / 76, loss: 1.0613036155700684\n",
      "Epoch 27, batch 72 / 76, loss: 1.1303902864456177\n",
      "Epoch 27, batch 73 / 76, loss: 1.1298149824142456\n",
      "Epoch 27, batch 74 / 76, loss: 1.1037287712097168\n",
      "Epoch 27, batch 75 / 76, loss: 1.1220755577087402\n",
      "Epoch 27, batch 76 / 76, loss: 1.1345702409744263\n",
      "Epoch 27, batch 77 / 76, loss: 0.9325336217880249\n",
      "Epoch 28, batch 1 / 76, loss: 1.088220238685608\n",
      "Epoch 28, batch 2 / 76, loss: 1.1020700931549072\n",
      "Epoch 28, batch 3 / 76, loss: 1.1277977228164673\n",
      "Epoch 28, batch 4 / 76, loss: 1.0995526313781738\n",
      "Epoch 28, batch 5 / 76, loss: 1.1125597953796387\n",
      "Epoch 28, batch 6 / 76, loss: 1.085268259048462\n",
      "Epoch 28, batch 7 / 76, loss: 1.1233336925506592\n",
      "Epoch 28, batch 8 / 76, loss: 1.1381266117095947\n",
      "Epoch 28, batch 9 / 76, loss: 1.0611997842788696\n",
      "Epoch 28, batch 10 / 76, loss: 1.042781114578247\n",
      "Epoch 28, batch 11 / 76, loss: 1.0918550491333008\n",
      "Epoch 28, batch 12 / 76, loss: 1.0741398334503174\n",
      "Epoch 28, batch 13 / 76, loss: 1.0965297222137451\n",
      "Epoch 28, batch 14 / 76, loss: 1.1456773281097412\n",
      "Epoch 28, batch 15 / 76, loss: 1.0756537914276123\n",
      "Epoch 28, batch 16 / 76, loss: 1.0295720100402832\n",
      "Epoch 28, batch 17 / 76, loss: 1.1123995780944824\n",
      "Epoch 28, batch 18 / 76, loss: 1.1611213684082031\n",
      "Epoch 28, batch 19 / 76, loss: 1.1052263975143433\n",
      "Epoch 28, batch 20 / 76, loss: 1.1195974349975586\n",
      "Epoch 28, batch 21 / 76, loss: 1.1076997518539429\n",
      "Epoch 28, batch 22 / 76, loss: 1.0898900032043457\n",
      "Epoch 28, batch 23 / 76, loss: 1.1247217655181885\n",
      "Epoch 28, batch 24 / 76, loss: 1.1700875759124756\n",
      "Epoch 28, batch 25 / 76, loss: 1.081671118736267\n",
      "Epoch 28, batch 26 / 76, loss: 1.0094273090362549\n",
      "Epoch 28, batch 27 / 76, loss: 1.1019083261489868\n",
      "Epoch 28, batch 28 / 76, loss: 1.1469159126281738\n",
      "Epoch 28, batch 29 / 76, loss: 1.0812413692474365\n",
      "Epoch 28, batch 30 / 76, loss: 1.0806611776351929\n",
      "Epoch 28, batch 31 / 76, loss: 1.1430304050445557\n",
      "Epoch 28, batch 32 / 76, loss: 1.043505311012268\n",
      "Epoch 28, batch 33 / 76, loss: 1.0959800481796265\n",
      "Epoch 28, batch 34 / 76, loss: 1.071758508682251\n",
      "Epoch 28, batch 35 / 76, loss: 1.1082813739776611\n",
      "Epoch 28, batch 36 / 76, loss: 1.1395263671875\n",
      "Epoch 28, batch 37 / 76, loss: 1.1100434064865112\n",
      "Epoch 28, batch 38 / 76, loss: 1.1018174886703491\n",
      "Epoch 28, batch 39 / 76, loss: 1.0626477003097534\n",
      "Epoch 28, batch 40 / 76, loss: 1.033705234527588\n",
      "Epoch 28, batch 41 / 76, loss: 1.1201303005218506\n",
      "Epoch 28, batch 42 / 76, loss: 1.0893075466156006\n",
      "Epoch 28, batch 43 / 76, loss: 1.133775234222412\n",
      "Epoch 28, batch 44 / 76, loss: 1.121992826461792\n",
      "Epoch 28, batch 45 / 76, loss: 1.077782392501831\n",
      "Epoch 28, batch 46 / 76, loss: 1.0729923248291016\n",
      "Epoch 28, batch 47 / 76, loss: 1.0923240184783936\n",
      "Epoch 28, batch 48 / 76, loss: 1.1356925964355469\n",
      "Epoch 28, batch 49 / 76, loss: 1.1034328937530518\n",
      "Epoch 28, batch 50 / 76, loss: 1.1553598642349243\n",
      "Epoch 28, batch 51 / 76, loss: 1.1231319904327393\n",
      "Epoch 28, batch 52 / 76, loss: 1.0851308107376099\n",
      "Epoch 28, batch 53 / 76, loss: 1.0265158414840698\n",
      "Epoch 28, batch 54 / 76, loss: 1.1336928606033325\n",
      "Epoch 28, batch 55 / 76, loss: 1.0594301223754883\n",
      "Epoch 28, batch 56 / 76, loss: 1.1222623586654663\n",
      "Epoch 28, batch 57 / 76, loss: 1.1234533786773682\n",
      "Epoch 28, batch 58 / 76, loss: 1.1021194458007812\n",
      "Epoch 28, batch 59 / 76, loss: 1.0781718492507935\n",
      "Epoch 28, batch 60 / 76, loss: 1.1190738677978516\n",
      "Epoch 28, batch 61 / 76, loss: 1.0964053869247437\n",
      "Epoch 28, batch 62 / 76, loss: 1.0682270526885986\n",
      "Epoch 28, batch 63 / 76, loss: 1.1161938905715942\n",
      "Epoch 28, batch 64 / 76, loss: 1.1293766498565674\n",
      "Epoch 28, batch 65 / 76, loss: 1.1215262413024902\n",
      "Epoch 28, batch 66 / 76, loss: 1.0883197784423828\n",
      "Epoch 28, batch 67 / 76, loss: 1.0817400217056274\n",
      "Epoch 28, batch 68 / 76, loss: 1.1309069395065308\n",
      "Epoch 28, batch 69 / 76, loss: 1.0646984577178955\n",
      "Epoch 28, batch 70 / 76, loss: 1.082579255104065\n",
      "Epoch 28, batch 71 / 76, loss: 1.055711030960083\n",
      "Epoch 28, batch 72 / 76, loss: 1.0708298683166504\n",
      "Epoch 28, batch 73 / 76, loss: 1.0792046785354614\n",
      "Epoch 28, batch 74 / 76, loss: 1.0576375722885132\n",
      "Epoch 28, batch 75 / 76, loss: 1.062845230102539\n",
      "Epoch 28, batch 76 / 76, loss: 1.0796102285385132\n",
      "Epoch 28, batch 77 / 76, loss: 1.0112478733062744\n",
      "Epoch 29, batch 1 / 76, loss: 1.078984260559082\n",
      "Epoch 29, batch 2 / 76, loss: 1.0985829830169678\n",
      "Epoch 29, batch 3 / 76, loss: 1.1035076379776\n",
      "Epoch 29, batch 4 / 76, loss: 1.0919787883758545\n",
      "Epoch 29, batch 5 / 76, loss: 1.1560012102127075\n",
      "Epoch 29, batch 6 / 76, loss: 1.1566922664642334\n",
      "Epoch 29, batch 7 / 76, loss: 1.1057093143463135\n",
      "Epoch 29, batch 8 / 76, loss: 1.1318705081939697\n",
      "Epoch 29, batch 9 / 76, loss: 1.1143436431884766\n",
      "Epoch 29, batch 10 / 76, loss: 1.0961464643478394\n",
      "Epoch 29, batch 11 / 76, loss: 1.1116448640823364\n",
      "Epoch 29, batch 12 / 76, loss: 1.0147048234939575\n",
      "Epoch 29, batch 13 / 76, loss: 1.0362008810043335\n",
      "Epoch 29, batch 14 / 76, loss: 1.0366486310958862\n",
      "Epoch 29, batch 15 / 76, loss: 1.1117228269577026\n",
      "Epoch 29, batch 16 / 76, loss: 1.07456374168396\n",
      "Epoch 29, batch 17 / 76, loss: 1.1034080982208252\n",
      "Epoch 29, batch 18 / 76, loss: 1.1098240613937378\n",
      "Epoch 29, batch 19 / 76, loss: 1.0755510330200195\n",
      "Epoch 29, batch 20 / 76, loss: 1.088587760925293\n",
      "Epoch 29, batch 21 / 76, loss: 1.1044111251831055\n",
      "Epoch 29, batch 22 / 76, loss: 1.1338889598846436\n",
      "Epoch 29, batch 23 / 76, loss: 1.1590588092803955\n",
      "Epoch 29, batch 24 / 76, loss: 1.1263755559921265\n",
      "Epoch 29, batch 25 / 76, loss: 1.0900332927703857\n",
      "Epoch 29, batch 26 / 76, loss: 1.049607753753662\n",
      "Epoch 29, batch 27 / 76, loss: 1.0586822032928467\n",
      "Epoch 29, batch 28 / 76, loss: 1.0749469995498657\n",
      "Epoch 29, batch 29 / 76, loss: 1.085574746131897\n",
      "Epoch 29, batch 30 / 76, loss: 1.0846298933029175\n",
      "Epoch 29, batch 31 / 76, loss: 1.0634610652923584\n",
      "Epoch 29, batch 32 / 76, loss: 1.1035652160644531\n",
      "Epoch 29, batch 33 / 76, loss: 1.1607316732406616\n",
      "Epoch 29, batch 34 / 76, loss: 1.0877971649169922\n",
      "Epoch 29, batch 35 / 76, loss: 1.071833610534668\n",
      "Epoch 29, batch 36 / 76, loss: 1.0709052085876465\n",
      "Epoch 29, batch 37 / 76, loss: 1.1371803283691406\n",
      "Epoch 29, batch 38 / 76, loss: 1.1167452335357666\n",
      "Epoch 29, batch 39 / 76, loss: 1.0715473890304565\n",
      "Epoch 29, batch 40 / 76, loss: 1.1124966144561768\n",
      "Epoch 29, batch 41 / 76, loss: 1.083242654800415\n",
      "Epoch 29, batch 42 / 76, loss: 1.094675064086914\n",
      "Epoch 29, batch 43 / 76, loss: 1.0857429504394531\n",
      "Epoch 29, batch 44 / 76, loss: 1.1694635152816772\n",
      "Epoch 29, batch 45 / 76, loss: 1.049744725227356\n",
      "Epoch 29, batch 46 / 76, loss: 1.1015310287475586\n",
      "Epoch 29, batch 47 / 76, loss: 1.1188558340072632\n",
      "Epoch 29, batch 48 / 76, loss: 1.0466047525405884\n",
      "Epoch 29, batch 49 / 76, loss: 1.1412616968154907\n",
      "Epoch 29, batch 50 / 76, loss: 1.1615986824035645\n",
      "Epoch 29, batch 51 / 76, loss: 1.1121058464050293\n",
      "Epoch 29, batch 52 / 76, loss: 1.1208305358886719\n",
      "Epoch 29, batch 53 / 76, loss: 1.1858373880386353\n",
      "Epoch 29, batch 54 / 76, loss: 1.0443538427352905\n",
      "Epoch 29, batch 55 / 76, loss: 1.1136229038238525\n",
      "Epoch 29, batch 56 / 76, loss: 1.1189156770706177\n",
      "Epoch 29, batch 57 / 76, loss: 1.1059952974319458\n",
      "Epoch 29, batch 58 / 76, loss: 1.0963281393051147\n",
      "Epoch 29, batch 59 / 76, loss: 1.160184383392334\n",
      "Epoch 29, batch 60 / 76, loss: 1.1074596643447876\n",
      "Epoch 29, batch 61 / 76, loss: 1.113843560218811\n",
      "Epoch 29, batch 62 / 76, loss: 1.1494014263153076\n",
      "Epoch 29, batch 63 / 76, loss: 1.1223459243774414\n",
      "Epoch 29, batch 64 / 76, loss: 1.1244442462921143\n",
      "Epoch 29, batch 65 / 76, loss: 1.1424853801727295\n",
      "Epoch 29, batch 66 / 76, loss: 1.15315580368042\n",
      "Epoch 29, batch 67 / 76, loss: 1.0788192749023438\n",
      "Epoch 29, batch 68 / 76, loss: 1.0836478471755981\n",
      "Epoch 29, batch 69 / 76, loss: 1.0643824338912964\n",
      "Epoch 29, batch 70 / 76, loss: 1.132053256034851\n",
      "Epoch 29, batch 71 / 76, loss: 1.1322952508926392\n",
      "Epoch 29, batch 72 / 76, loss: 1.1422947645187378\n",
      "Epoch 29, batch 73 / 76, loss: 1.0977712869644165\n",
      "Epoch 29, batch 74 / 76, loss: 1.0781902074813843\n",
      "Epoch 29, batch 75 / 76, loss: 1.1445364952087402\n",
      "Epoch 29, batch 76 / 76, loss: 1.0880773067474365\n",
      "Epoch 29, batch 77 / 76, loss: 1.2257978916168213\n",
      "Epoch 30, batch 1 / 76, loss: 1.1644840240478516\n",
      "Epoch 30, batch 2 / 76, loss: 1.1435372829437256\n",
      "Epoch 30, batch 3 / 76, loss: 1.1464619636535645\n",
      "Epoch 30, batch 4 / 76, loss: 1.1249243021011353\n",
      "Epoch 30, batch 5 / 76, loss: 1.1211721897125244\n",
      "Epoch 30, batch 6 / 76, loss: 1.106645107269287\n",
      "Epoch 30, batch 7 / 76, loss: 1.181666612625122\n",
      "Epoch 30, batch 8 / 76, loss: 1.1578041315078735\n",
      "Epoch 30, batch 9 / 76, loss: 1.1829777956008911\n",
      "Epoch 30, batch 10 / 76, loss: 1.1589642763137817\n",
      "Epoch 30, batch 11 / 76, loss: 1.1224040985107422\n",
      "Epoch 30, batch 12 / 76, loss: 1.076819896697998\n",
      "Epoch 30, batch 13 / 76, loss: 1.1045582294464111\n",
      "Epoch 30, batch 14 / 76, loss: 1.1360130310058594\n",
      "Epoch 30, batch 15 / 76, loss: 1.1530629396438599\n",
      "Epoch 30, batch 16 / 76, loss: 1.1265661716461182\n",
      "Epoch 30, batch 17 / 76, loss: 1.212501049041748\n",
      "Epoch 30, batch 18 / 76, loss: 1.0883996486663818\n",
      "Epoch 30, batch 19 / 76, loss: 1.1189947128295898\n",
      "Epoch 30, batch 20 / 76, loss: 1.1130186319351196\n",
      "Epoch 30, batch 21 / 76, loss: 1.163629174232483\n",
      "Epoch 30, batch 22 / 76, loss: 1.1176385879516602\n",
      "Epoch 30, batch 23 / 76, loss: 1.078789472579956\n",
      "Epoch 30, batch 24 / 76, loss: 1.1874216794967651\n",
      "Epoch 30, batch 25 / 76, loss: 1.1597824096679688\n",
      "Epoch 30, batch 26 / 76, loss: 1.145303726196289\n",
      "Epoch 30, batch 27 / 76, loss: 1.105246663093567\n",
      "Epoch 30, batch 28 / 76, loss: 1.1120264530181885\n",
      "Epoch 30, batch 29 / 76, loss: 1.0750259160995483\n",
      "Epoch 30, batch 30 / 76, loss: 1.1900873184204102\n",
      "Epoch 30, batch 31 / 76, loss: 1.0901490449905396\n",
      "Epoch 30, batch 32 / 76, loss: 1.1424490213394165\n",
      "Epoch 30, batch 33 / 76, loss: 1.068784475326538\n",
      "Epoch 30, batch 34 / 76, loss: 1.1057771444320679\n",
      "Epoch 30, batch 35 / 76, loss: 1.157895803451538\n",
      "Epoch 30, batch 36 / 76, loss: 1.138802170753479\n",
      "Epoch 30, batch 37 / 76, loss: 1.1510612964630127\n",
      "Epoch 30, batch 38 / 76, loss: 1.129393458366394\n",
      "Epoch 30, batch 39 / 76, loss: 1.0892138481140137\n",
      "Epoch 30, batch 40 / 76, loss: 1.1018176078796387\n",
      "Epoch 30, batch 41 / 76, loss: 1.1532187461853027\n",
      "Epoch 30, batch 42 / 76, loss: 1.1120343208312988\n",
      "Epoch 30, batch 43 / 76, loss: 1.0586423873901367\n",
      "Epoch 30, batch 44 / 76, loss: 1.1727168560028076\n",
      "Epoch 30, batch 45 / 76, loss: 1.109710931777954\n",
      "Epoch 30, batch 46 / 76, loss: 1.1927893161773682\n",
      "Epoch 30, batch 47 / 76, loss: 1.1903042793273926\n",
      "Epoch 30, batch 48 / 76, loss: 1.1632006168365479\n",
      "Epoch 30, batch 49 / 76, loss: 1.1324701309204102\n",
      "Epoch 30, batch 50 / 76, loss: 1.1127090454101562\n",
      "Epoch 30, batch 51 / 76, loss: 1.0747779607772827\n",
      "Epoch 30, batch 52 / 76, loss: 1.1878294944763184\n",
      "Epoch 30, batch 53 / 76, loss: 1.142471432685852\n",
      "Epoch 30, batch 54 / 76, loss: 1.070695400238037\n",
      "Epoch 30, batch 55 / 76, loss: 1.127333641052246\n",
      "Epoch 30, batch 56 / 76, loss: 1.1944072246551514\n",
      "Epoch 30, batch 57 / 76, loss: 1.2097115516662598\n",
      "Epoch 30, batch 58 / 76, loss: 1.128191590309143\n",
      "Epoch 30, batch 59 / 76, loss: 1.217915415763855\n",
      "Epoch 30, batch 60 / 76, loss: 1.1511238813400269\n",
      "Epoch 30, batch 61 / 76, loss: 1.0949088335037231\n",
      "Epoch 30, batch 62 / 76, loss: 1.1706316471099854\n",
      "Epoch 30, batch 63 / 76, loss: 1.1459825038909912\n",
      "Epoch 30, batch 64 / 76, loss: 1.1110244989395142\n",
      "Epoch 30, batch 65 / 76, loss: 1.1605396270751953\n",
      "Epoch 30, batch 66 / 76, loss: 1.1524155139923096\n",
      "Epoch 30, batch 67 / 76, loss: 1.1508702039718628\n",
      "Epoch 30, batch 68 / 76, loss: 1.0998976230621338\n",
      "Epoch 30, batch 69 / 76, loss: 1.1670119762420654\n",
      "Epoch 30, batch 70 / 76, loss: 1.1502611637115479\n",
      "Epoch 30, batch 71 / 76, loss: 1.183044672012329\n",
      "Epoch 30, batch 72 / 76, loss: 1.1107547283172607\n",
      "Epoch 30, batch 73 / 76, loss: 1.1698931455612183\n",
      "Epoch 30, batch 74 / 76, loss: 1.1112459897994995\n",
      "Epoch 30, batch 75 / 76, loss: 1.1534113883972168\n",
      "Epoch 30, batch 76 / 76, loss: 1.1618704795837402\n",
      "Epoch 30, batch 77 / 76, loss: 1.0615334510803223\n",
      "Epoch 31, batch 1 / 76, loss: 1.2271137237548828\n",
      "Epoch 31, batch 2 / 76, loss: 1.1432976722717285\n",
      "Epoch 31, batch 3 / 76, loss: 1.1966071128845215\n",
      "Epoch 31, batch 4 / 76, loss: 1.10743248462677\n",
      "Epoch 31, batch 5 / 76, loss: 1.1371607780456543\n",
      "Epoch 31, batch 6 / 76, loss: 1.236524224281311\n",
      "Epoch 31, batch 7 / 76, loss: 1.1222541332244873\n",
      "Epoch 31, batch 8 / 76, loss: 1.1410928964614868\n",
      "Epoch 31, batch 9 / 76, loss: 1.1520135402679443\n",
      "Epoch 31, batch 10 / 76, loss: 1.1006393432617188\n",
      "Epoch 31, batch 11 / 76, loss: 1.1519994735717773\n",
      "Epoch 31, batch 12 / 76, loss: 1.1818828582763672\n",
      "Epoch 31, batch 13 / 76, loss: 1.1592161655426025\n",
      "Epoch 31, batch 14 / 76, loss: 1.1792303323745728\n",
      "Epoch 31, batch 15 / 76, loss: 1.1474964618682861\n",
      "Epoch 31, batch 16 / 76, loss: 1.1695393323898315\n",
      "Epoch 31, batch 17 / 76, loss: 1.1824241876602173\n",
      "Epoch 31, batch 18 / 76, loss: 1.0979536771774292\n",
      "Epoch 31, batch 19 / 76, loss: 1.2317065000534058\n",
      "Epoch 31, batch 20 / 76, loss: 1.128253698348999\n",
      "Epoch 31, batch 21 / 76, loss: 1.1757405996322632\n",
      "Epoch 31, batch 22 / 76, loss: 1.1936101913452148\n",
      "Epoch 31, batch 23 / 76, loss: 1.15523362159729\n",
      "Epoch 31, batch 24 / 76, loss: 1.1776281595230103\n",
      "Epoch 31, batch 25 / 76, loss: 1.1872611045837402\n",
      "Epoch 31, batch 26 / 76, loss: 1.176301121711731\n",
      "Epoch 31, batch 27 / 76, loss: 1.148932933807373\n",
      "Epoch 31, batch 28 / 76, loss: 1.1898598670959473\n",
      "Epoch 31, batch 29 / 76, loss: 1.1736785173416138\n",
      "Epoch 31, batch 30 / 76, loss: 1.177790641784668\n",
      "Epoch 31, batch 31 / 76, loss: 1.1724727153778076\n",
      "Epoch 31, batch 32 / 76, loss: 1.1735868453979492\n",
      "Epoch 31, batch 33 / 76, loss: 1.1237597465515137\n",
      "Epoch 31, batch 34 / 76, loss: 1.2523524761199951\n",
      "Epoch 31, batch 35 / 76, loss: 1.1723564863204956\n",
      "Epoch 31, batch 36 / 76, loss: 1.1466946601867676\n",
      "Epoch 31, batch 37 / 76, loss: 1.1155281066894531\n",
      "Epoch 31, batch 38 / 76, loss: 1.2478148937225342\n",
      "Epoch 31, batch 39 / 76, loss: 1.1036230325698853\n",
      "Epoch 31, batch 40 / 76, loss: 1.1624507904052734\n",
      "Epoch 31, batch 41 / 76, loss: 1.117053508758545\n",
      "Epoch 31, batch 42 / 76, loss: 1.1440939903259277\n",
      "Epoch 31, batch 43 / 76, loss: 1.1035836935043335\n",
      "Epoch 31, batch 44 / 76, loss: 1.1784930229187012\n",
      "Epoch 31, batch 45 / 76, loss: 1.1734867095947266\n",
      "Epoch 31, batch 46 / 76, loss: 1.139331340789795\n",
      "Epoch 31, batch 47 / 76, loss: 1.1399198770523071\n",
      "Epoch 31, batch 48 / 76, loss: 1.0933821201324463\n",
      "Epoch 31, batch 49 / 76, loss: 1.1588969230651855\n",
      "Epoch 31, batch 50 / 76, loss: 1.1563842296600342\n",
      "Epoch 31, batch 51 / 76, loss: 1.1907060146331787\n",
      "Epoch 31, batch 52 / 76, loss: 1.1429221630096436\n",
      "Epoch 31, batch 53 / 76, loss: 1.1591213941574097\n",
      "Epoch 31, batch 54 / 76, loss: 1.1911303997039795\n",
      "Epoch 31, batch 55 / 76, loss: 1.245110273361206\n",
      "Epoch 31, batch 56 / 76, loss: 1.1546484231948853\n",
      "Epoch 31, batch 57 / 76, loss: 1.2371692657470703\n",
      "Epoch 31, batch 58 / 76, loss: 1.1603419780731201\n",
      "Epoch 31, batch 59 / 76, loss: 1.1270874738693237\n",
      "Epoch 31, batch 60 / 76, loss: 1.1287589073181152\n",
      "Epoch 31, batch 61 / 76, loss: 1.1659305095672607\n",
      "Epoch 31, batch 62 / 76, loss: 1.2220765352249146\n",
      "Epoch 31, batch 63 / 76, loss: 1.159995675086975\n",
      "Epoch 31, batch 64 / 76, loss: 1.2176260948181152\n",
      "Epoch 31, batch 65 / 76, loss: 1.1464600563049316\n",
      "Epoch 31, batch 66 / 76, loss: 1.1680576801300049\n",
      "Epoch 31, batch 67 / 76, loss: 1.1510941982269287\n",
      "Epoch 31, batch 68 / 76, loss: 1.1692014932632446\n",
      "Epoch 31, batch 69 / 76, loss: 1.1323742866516113\n",
      "Epoch 31, batch 70 / 76, loss: 1.1711509227752686\n",
      "Epoch 31, batch 71 / 76, loss: 1.162610411643982\n",
      "Epoch 31, batch 72 / 76, loss: 1.1858371496200562\n",
      "Epoch 31, batch 73 / 76, loss: 1.184211015701294\n",
      "Epoch 31, batch 74 / 76, loss: 1.1823489665985107\n",
      "Epoch 31, batch 75 / 76, loss: 1.0976645946502686\n",
      "Epoch 31, batch 76 / 76, loss: 1.1934274435043335\n",
      "Epoch 31, batch 77 / 76, loss: 1.1683921813964844\n",
      "Epoch 32, batch 1 / 76, loss: 1.153122901916504\n",
      "Epoch 32, batch 2 / 76, loss: 1.128969430923462\n",
      "Epoch 32, batch 3 / 76, loss: 1.173485517501831\n",
      "Epoch 32, batch 4 / 76, loss: 1.155808687210083\n",
      "Epoch 32, batch 5 / 76, loss: 1.1989221572875977\n",
      "Epoch 32, batch 6 / 76, loss: 1.1687095165252686\n",
      "Epoch 32, batch 7 / 76, loss: 1.1408882141113281\n",
      "Epoch 32, batch 8 / 76, loss: 1.1485838890075684\n",
      "Epoch 32, batch 9 / 76, loss: 1.1729631423950195\n",
      "Epoch 32, batch 10 / 76, loss: 1.239396572113037\n",
      "Epoch 32, batch 11 / 76, loss: 1.1935791969299316\n",
      "Epoch 32, batch 12 / 76, loss: 1.154924988746643\n",
      "Epoch 32, batch 13 / 76, loss: 1.1920490264892578\n",
      "Epoch 32, batch 14 / 76, loss: 1.1798250675201416\n",
      "Epoch 32, batch 15 / 76, loss: 1.1745550632476807\n",
      "Epoch 32, batch 16 / 76, loss: 1.2107350826263428\n",
      "Epoch 32, batch 17 / 76, loss: 1.1583524942398071\n",
      "Epoch 32, batch 18 / 76, loss: 1.1378698348999023\n",
      "Epoch 32, batch 19 / 76, loss: 1.1871870756149292\n",
      "Epoch 32, batch 20 / 76, loss: 1.2952041625976562\n",
      "Epoch 32, batch 21 / 76, loss: 1.1915032863616943\n",
      "Epoch 32, batch 22 / 76, loss: 1.1175072193145752\n",
      "Epoch 32, batch 23 / 76, loss: 1.1975525617599487\n",
      "Epoch 32, batch 24 / 76, loss: 1.1984957456588745\n",
      "Epoch 32, batch 25 / 76, loss: 1.1655956506729126\n",
      "Epoch 32, batch 26 / 76, loss: 1.196348786354065\n",
      "Epoch 32, batch 27 / 76, loss: 1.10709547996521\n",
      "Epoch 32, batch 28 / 76, loss: 1.073990821838379\n",
      "Epoch 32, batch 29 / 76, loss: 1.155268907546997\n",
      "Epoch 32, batch 30 / 76, loss: 1.1874758005142212\n",
      "Epoch 32, batch 31 / 76, loss: 1.108376145362854\n",
      "Epoch 32, batch 32 / 76, loss: 1.224607229232788\n",
      "Epoch 32, batch 33 / 76, loss: 1.18650221824646\n",
      "Epoch 32, batch 34 / 76, loss: 1.1555324792861938\n",
      "Epoch 32, batch 35 / 76, loss: 1.1766104698181152\n",
      "Epoch 32, batch 36 / 76, loss: 1.1758149862289429\n",
      "Epoch 32, batch 37 / 76, loss: 1.143031358718872\n",
      "Epoch 32, batch 38 / 76, loss: 1.1155667304992676\n",
      "Epoch 32, batch 39 / 76, loss: 1.104195237159729\n",
      "Epoch 32, batch 40 / 76, loss: 1.1715646982192993\n",
      "Epoch 32, batch 41 / 76, loss: 1.2083640098571777\n",
      "Epoch 32, batch 42 / 76, loss: 1.199706792831421\n",
      "Epoch 32, batch 43 / 76, loss: 1.199837565422058\n",
      "Epoch 32, batch 44 / 76, loss: 1.1728894710540771\n",
      "Epoch 32, batch 45 / 76, loss: 1.154595136642456\n",
      "Epoch 32, batch 46 / 76, loss: 1.18280029296875\n",
      "Epoch 32, batch 47 / 76, loss: 1.1645232439041138\n",
      "Epoch 32, batch 48 / 76, loss: 1.181095838546753\n",
      "Epoch 32, batch 49 / 76, loss: 1.1959214210510254\n",
      "Epoch 32, batch 50 / 76, loss: 1.1490426063537598\n",
      "Epoch 32, batch 51 / 76, loss: 1.1637769937515259\n",
      "Epoch 32, batch 52 / 76, loss: 1.1703708171844482\n",
      "Epoch 32, batch 53 / 76, loss: 1.1327159404754639\n",
      "Epoch 32, batch 54 / 76, loss: 1.1604783535003662\n",
      "Epoch 32, batch 55 / 76, loss: 1.1389179229736328\n",
      "Epoch 32, batch 56 / 76, loss: 1.156313419342041\n",
      "Epoch 32, batch 57 / 76, loss: 1.1142244338989258\n",
      "Epoch 32, batch 58 / 76, loss: 1.1736316680908203\n",
      "Epoch 32, batch 59 / 76, loss: 1.2126145362854004\n",
      "Epoch 32, batch 60 / 76, loss: 1.1868159770965576\n",
      "Epoch 32, batch 61 / 76, loss: 1.207376480102539\n",
      "Epoch 32, batch 62 / 76, loss: 1.1889088153839111\n",
      "Epoch 32, batch 63 / 76, loss: 1.1725962162017822\n",
      "Epoch 32, batch 64 / 76, loss: 1.1348254680633545\n",
      "Epoch 32, batch 65 / 76, loss: 1.2258185148239136\n",
      "Epoch 32, batch 66 / 76, loss: 1.1796844005584717\n",
      "Epoch 32, batch 67 / 76, loss: 1.1613030433654785\n",
      "Epoch 32, batch 68 / 76, loss: 1.2110817432403564\n",
      "Epoch 32, batch 69 / 76, loss: 1.207632064819336\n",
      "Epoch 32, batch 70 / 76, loss: 1.1454596519470215\n",
      "Epoch 32, batch 71 / 76, loss: 1.1285616159439087\n",
      "Epoch 32, batch 72 / 76, loss: 1.1673575639724731\n",
      "Epoch 32, batch 73 / 76, loss: 1.1233510971069336\n",
      "Epoch 32, batch 74 / 76, loss: 1.1557375192642212\n",
      "Epoch 32, batch 75 / 76, loss: 1.2084163427352905\n",
      "Epoch 32, batch 76 / 76, loss: 1.186177372932434\n",
      "Epoch 32, batch 77 / 76, loss: 1.1430635452270508\n",
      "Epoch 33, batch 1 / 76, loss: 1.1711773872375488\n",
      "Epoch 33, batch 2 / 76, loss: 1.2430651187896729\n",
      "Epoch 33, batch 3 / 76, loss: 1.1630504131317139\n",
      "Epoch 33, batch 4 / 76, loss: 1.2313413619995117\n",
      "Epoch 33, batch 5 / 76, loss: 1.2548184394836426\n",
      "Epoch 33, batch 6 / 76, loss: 1.138136625289917\n",
      "Epoch 33, batch 7 / 76, loss: 1.178961992263794\n",
      "Epoch 33, batch 8 / 76, loss: 1.1590865850448608\n",
      "Epoch 33, batch 9 / 76, loss: 1.2136024236679077\n",
      "Epoch 33, batch 10 / 76, loss: 1.170828104019165\n",
      "Epoch 33, batch 11 / 76, loss: 1.132758617401123\n",
      "Epoch 33, batch 12 / 76, loss: 1.1685949563980103\n",
      "Epoch 33, batch 13 / 76, loss: 1.175766110420227\n",
      "Epoch 33, batch 14 / 76, loss: 1.2400779724121094\n",
      "Epoch 33, batch 15 / 76, loss: 1.1578245162963867\n",
      "Epoch 33, batch 16 / 76, loss: 1.1300063133239746\n",
      "Epoch 33, batch 17 / 76, loss: 1.110313057899475\n",
      "Epoch 33, batch 18 / 76, loss: 1.262158751487732\n",
      "Epoch 33, batch 19 / 76, loss: 1.161035180091858\n",
      "Epoch 33, batch 20 / 76, loss: 1.1594864130020142\n",
      "Epoch 33, batch 21 / 76, loss: 1.141568660736084\n",
      "Epoch 33, batch 22 / 76, loss: 1.202831506729126\n",
      "Epoch 33, batch 23 / 76, loss: 1.1700212955474854\n",
      "Epoch 33, batch 24 / 76, loss: 1.1799689531326294\n",
      "Epoch 33, batch 25 / 76, loss: 1.1766350269317627\n",
      "Epoch 33, batch 26 / 76, loss: 1.1817426681518555\n",
      "Epoch 33, batch 27 / 76, loss: 1.1691386699676514\n",
      "Epoch 33, batch 28 / 76, loss: 1.1692674160003662\n",
      "Epoch 33, batch 29 / 76, loss: 1.162737250328064\n",
      "Epoch 33, batch 30 / 76, loss: 1.1626901626586914\n",
      "Epoch 33, batch 31 / 76, loss: 1.1983001232147217\n",
      "Epoch 33, batch 32 / 76, loss: 1.19983971118927\n",
      "Epoch 33, batch 33 / 76, loss: 1.1514935493469238\n",
      "Epoch 33, batch 34 / 76, loss: 1.1583328247070312\n",
      "Epoch 33, batch 35 / 76, loss: 1.1634595394134521\n",
      "Epoch 33, batch 36 / 76, loss: 1.1896076202392578\n",
      "Epoch 33, batch 37 / 76, loss: 1.228813886642456\n",
      "Epoch 33, batch 38 / 76, loss: 1.1566243171691895\n",
      "Epoch 33, batch 39 / 76, loss: 1.1752123832702637\n",
      "Epoch 33, batch 40 / 76, loss: 1.15968656539917\n",
      "Epoch 33, batch 41 / 76, loss: 1.1692020893096924\n",
      "Epoch 33, batch 42 / 76, loss: 1.2019163370132446\n",
      "Epoch 33, batch 43 / 76, loss: 1.1665937900543213\n",
      "Epoch 33, batch 44 / 76, loss: 1.1527913808822632\n",
      "Epoch 33, batch 45 / 76, loss: 1.1160495281219482\n",
      "Epoch 33, batch 46 / 76, loss: 1.2315678596496582\n",
      "Epoch 33, batch 47 / 76, loss: 1.1391301155090332\n",
      "Epoch 33, batch 48 / 76, loss: 1.1657613515853882\n",
      "Epoch 33, batch 49 / 76, loss: 1.1737070083618164\n",
      "Epoch 33, batch 50 / 76, loss: 1.2208551168441772\n",
      "Epoch 33, batch 51 / 76, loss: 1.1848139762878418\n",
      "Epoch 33, batch 52 / 76, loss: 1.19931161403656\n",
      "Epoch 33, batch 53 / 76, loss: 1.1501697301864624\n",
      "Epoch 33, batch 54 / 76, loss: 1.2180712223052979\n",
      "Epoch 33, batch 55 / 76, loss: 1.1686540842056274\n",
      "Epoch 33, batch 56 / 76, loss: 1.0991907119750977\n",
      "Epoch 33, batch 57 / 76, loss: 1.1309887170791626\n",
      "Epoch 33, batch 58 / 76, loss: 1.155092716217041\n",
      "Epoch 33, batch 59 / 76, loss: 1.1967145204544067\n",
      "Epoch 33, batch 60 / 76, loss: 1.1761192083358765\n",
      "Epoch 33, batch 61 / 76, loss: 1.118363618850708\n",
      "Epoch 33, batch 62 / 76, loss: 1.1416800022125244\n",
      "Epoch 33, batch 63 / 76, loss: 1.1797517538070679\n",
      "Epoch 33, batch 64 / 76, loss: 1.1774661540985107\n",
      "Epoch 33, batch 65 / 76, loss: 1.130070447921753\n",
      "Epoch 33, batch 66 / 76, loss: 1.2457799911499023\n",
      "Epoch 33, batch 67 / 76, loss: 1.1481728553771973\n",
      "Epoch 33, batch 68 / 76, loss: 1.1221718788146973\n",
      "Epoch 33, batch 69 / 76, loss: 1.2232577800750732\n",
      "Epoch 33, batch 70 / 76, loss: 1.1775262355804443\n",
      "Epoch 33, batch 71 / 76, loss: 1.2234020233154297\n",
      "Epoch 33, batch 72 / 76, loss: 1.1039561033248901\n",
      "Epoch 33, batch 73 / 76, loss: 1.1691110134124756\n",
      "Epoch 33, batch 74 / 76, loss: 1.163905382156372\n",
      "Epoch 33, batch 75 / 76, loss: 1.1679794788360596\n",
      "Epoch 33, batch 76 / 76, loss: 1.190300464630127\n",
      "Epoch 33, batch 77 / 76, loss: 1.2242052555084229\n",
      "Epoch 34, batch 1 / 76, loss: 1.1913269758224487\n",
      "Epoch 34, batch 2 / 76, loss: 1.2662243843078613\n",
      "Epoch 34, batch 3 / 76, loss: 1.0809025764465332\n",
      "Epoch 34, batch 4 / 76, loss: 1.151394248008728\n",
      "Epoch 34, batch 5 / 76, loss: 1.1703848838806152\n",
      "Epoch 34, batch 6 / 76, loss: 1.0605432987213135\n",
      "Epoch 34, batch 7 / 76, loss: 1.2224608659744263\n",
      "Epoch 34, batch 8 / 76, loss: 1.087161660194397\n",
      "Epoch 34, batch 9 / 76, loss: 1.2271775007247925\n",
      "Epoch 34, batch 10 / 76, loss: 1.2063846588134766\n",
      "Epoch 34, batch 11 / 76, loss: 1.1533175706863403\n",
      "Epoch 34, batch 12 / 76, loss: 1.1675797700881958\n",
      "Epoch 34, batch 13 / 76, loss: 1.181177020072937\n",
      "Epoch 34, batch 14 / 76, loss: 1.1487202644348145\n",
      "Epoch 34, batch 15 / 76, loss: 1.1492582559585571\n",
      "Epoch 34, batch 16 / 76, loss: 1.1512097120285034\n",
      "Epoch 34, batch 17 / 76, loss: 1.0808264017105103\n",
      "Epoch 34, batch 18 / 76, loss: 1.1173559427261353\n",
      "Epoch 34, batch 19 / 76, loss: 1.1729037761688232\n",
      "Epoch 34, batch 20 / 76, loss: 1.1446048021316528\n",
      "Epoch 34, batch 21 / 76, loss: 1.158371925354004\n",
      "Epoch 34, batch 22 / 76, loss: 1.16181218624115\n",
      "Epoch 34, batch 23 / 76, loss: 1.194334626197815\n",
      "Epoch 34, batch 24 / 76, loss: 1.1494410037994385\n",
      "Epoch 34, batch 25 / 76, loss: 1.1681888103485107\n",
      "Epoch 34, batch 26 / 76, loss: 1.1511176824569702\n",
      "Epoch 34, batch 27 / 76, loss: 1.1347532272338867\n",
      "Epoch 34, batch 28 / 76, loss: 1.130647897720337\n",
      "Epoch 34, batch 29 / 76, loss: 1.1363048553466797\n",
      "Epoch 34, batch 30 / 76, loss: 1.155088186264038\n",
      "Epoch 34, batch 31 / 76, loss: 1.1712324619293213\n",
      "Epoch 34, batch 32 / 76, loss: 1.123600721359253\n",
      "Epoch 34, batch 33 / 76, loss: 1.1546213626861572\n",
      "Epoch 34, batch 34 / 76, loss: 1.1763967275619507\n",
      "Epoch 34, batch 35 / 76, loss: 1.0803325176239014\n",
      "Epoch 34, batch 36 / 76, loss: 1.1303255558013916\n",
      "Epoch 34, batch 37 / 76, loss: 1.1491801738739014\n",
      "Epoch 34, batch 38 / 76, loss: 1.174884557723999\n",
      "Epoch 34, batch 39 / 76, loss: 1.091627836227417\n",
      "Epoch 34, batch 40 / 76, loss: 1.1271647214889526\n",
      "Epoch 34, batch 41 / 76, loss: 1.1591153144836426\n",
      "Epoch 34, batch 42 / 76, loss: 1.1107831001281738\n",
      "Epoch 34, batch 43 / 76, loss: 1.097362995147705\n",
      "Epoch 34, batch 44 / 76, loss: 1.1258760690689087\n",
      "Epoch 34, batch 45 / 76, loss: 1.1624747514724731\n",
      "Epoch 34, batch 46 / 76, loss: 1.1566935777664185\n",
      "Epoch 34, batch 47 / 76, loss: 1.1282929182052612\n",
      "Epoch 34, batch 48 / 76, loss: 1.1192951202392578\n",
      "Epoch 34, batch 49 / 76, loss: 1.1111538410186768\n",
      "Epoch 34, batch 50 / 76, loss: 1.1433770656585693\n",
      "Epoch 34, batch 51 / 76, loss: 1.1240901947021484\n",
      "Epoch 34, batch 52 / 76, loss: 1.1555153131484985\n",
      "Epoch 34, batch 53 / 76, loss: 1.106520175933838\n",
      "Epoch 34, batch 54 / 76, loss: 1.1186952590942383\n",
      "Epoch 34, batch 55 / 76, loss: 1.1682463884353638\n",
      "Epoch 34, batch 56 / 76, loss: 1.2105811834335327\n",
      "Epoch 34, batch 57 / 76, loss: 1.0970771312713623\n",
      "Epoch 34, batch 58 / 76, loss: 1.1090294122695923\n",
      "Epoch 34, batch 59 / 76, loss: 1.1948165893554688\n",
      "Epoch 34, batch 60 / 76, loss: 1.1810853481292725\n",
      "Epoch 34, batch 61 / 76, loss: 1.1232349872589111\n",
      "Epoch 34, batch 62 / 76, loss: 1.153821349143982\n",
      "Epoch 34, batch 63 / 76, loss: 1.1654791831970215\n",
      "Epoch 34, batch 64 / 76, loss: 1.1655166149139404\n",
      "Epoch 34, batch 65 / 76, loss: 1.182116985321045\n",
      "Epoch 34, batch 66 / 76, loss: 1.0984699726104736\n",
      "Epoch 34, batch 67 / 76, loss: 1.1298496723175049\n",
      "Epoch 34, batch 68 / 76, loss: 1.0643494129180908\n",
      "Epoch 34, batch 69 / 76, loss: 1.1565825939178467\n",
      "Epoch 34, batch 70 / 76, loss: 1.1014988422393799\n",
      "Epoch 34, batch 71 / 76, loss: 1.1532337665557861\n",
      "Epoch 34, batch 72 / 76, loss: 1.1843516826629639\n",
      "Epoch 34, batch 73 / 76, loss: 1.0762804746627808\n",
      "Epoch 34, batch 74 / 76, loss: 1.137100100517273\n",
      "Epoch 34, batch 75 / 76, loss: 1.1778680086135864\n",
      "Epoch 34, batch 76 / 76, loss: 1.1452515125274658\n",
      "Epoch 34, batch 77 / 76, loss: 1.0802162885665894\n",
      "Epoch 35, batch 1 / 76, loss: 1.1476632356643677\n",
      "Epoch 35, batch 2 / 76, loss: 1.1510412693023682\n",
      "Epoch 35, batch 3 / 76, loss: 1.100243330001831\n",
      "Epoch 35, batch 4 / 76, loss: 1.1947228908538818\n",
      "Epoch 35, batch 5 / 76, loss: 1.1358410120010376\n",
      "Epoch 35, batch 6 / 76, loss: 1.1094304323196411\n",
      "Epoch 35, batch 7 / 76, loss: 1.148064374923706\n",
      "Epoch 35, batch 8 / 76, loss: 1.1323325634002686\n",
      "Epoch 35, batch 9 / 76, loss: 1.139095425605774\n",
      "Epoch 35, batch 10 / 76, loss: 1.1496306657791138\n",
      "Epoch 35, batch 11 / 76, loss: 1.2236422300338745\n",
      "Epoch 35, batch 12 / 76, loss: 1.138193964958191\n",
      "Epoch 35, batch 13 / 76, loss: 1.1129882335662842\n",
      "Epoch 35, batch 14 / 76, loss: 1.23592209815979\n",
      "Epoch 35, batch 15 / 76, loss: 1.0667580366134644\n",
      "Epoch 35, batch 16 / 76, loss: 1.1093649864196777\n",
      "Epoch 35, batch 17 / 76, loss: 1.1188452243804932\n",
      "Epoch 35, batch 18 / 76, loss: 1.0575765371322632\n",
      "Epoch 35, batch 19 / 76, loss: 1.125863790512085\n",
      "Epoch 35, batch 20 / 76, loss: 1.0827133655548096\n",
      "Epoch 35, batch 21 / 76, loss: 1.1334145069122314\n",
      "Epoch 35, batch 22 / 76, loss: 1.1122493743896484\n",
      "Epoch 35, batch 23 / 76, loss: 1.1486270427703857\n",
      "Epoch 35, batch 24 / 76, loss: 1.1122167110443115\n",
      "Epoch 35, batch 25 / 76, loss: 1.1234374046325684\n",
      "Epoch 35, batch 26 / 76, loss: 1.1037688255310059\n",
      "Epoch 35, batch 27 / 76, loss: 1.1330819129943848\n",
      "Epoch 35, batch 28 / 76, loss: 1.090174674987793\n",
      "Epoch 35, batch 29 / 76, loss: 1.0838310718536377\n",
      "Epoch 35, batch 30 / 76, loss: 1.096725344657898\n",
      "Epoch 35, batch 31 / 76, loss: 1.0859699249267578\n",
      "Epoch 35, batch 32 / 76, loss: 1.1160629987716675\n",
      "Epoch 35, batch 33 / 76, loss: 1.1434317827224731\n",
      "Epoch 35, batch 34 / 76, loss: 1.1424617767333984\n",
      "Epoch 35, batch 35 / 76, loss: 1.1775015592575073\n",
      "Epoch 35, batch 36 / 76, loss: 1.1147381067276\n",
      "Epoch 35, batch 37 / 76, loss: 1.121882677078247\n",
      "Epoch 35, batch 38 / 76, loss: 1.0483282804489136\n",
      "Epoch 35, batch 39 / 76, loss: 1.1125181913375854\n",
      "Epoch 35, batch 40 / 76, loss: 1.0675030946731567\n",
      "Epoch 35, batch 41 / 76, loss: 1.1540335416793823\n",
      "Epoch 35, batch 42 / 76, loss: 1.145033359527588\n",
      "Epoch 35, batch 43 / 76, loss: 1.1346172094345093\n",
      "Epoch 35, batch 44 / 76, loss: 1.0814858675003052\n",
      "Epoch 35, batch 45 / 76, loss: 1.101760983467102\n",
      "Epoch 35, batch 46 / 76, loss: 1.168562650680542\n",
      "Epoch 35, batch 47 / 76, loss: 1.106824517250061\n",
      "Epoch 35, batch 48 / 76, loss: 1.1103713512420654\n",
      "Epoch 35, batch 49 / 76, loss: 1.1713967323303223\n",
      "Epoch 35, batch 50 / 76, loss: 1.0426697731018066\n",
      "Epoch 35, batch 51 / 76, loss: 1.2023818492889404\n",
      "Epoch 35, batch 52 / 76, loss: 1.2017019987106323\n",
      "Epoch 35, batch 53 / 76, loss: 1.1342532634735107\n",
      "Epoch 35, batch 54 / 76, loss: 1.1307260990142822\n",
      "Epoch 35, batch 55 / 76, loss: 1.104742407798767\n",
      "Epoch 35, batch 56 / 76, loss: 1.122849702835083\n",
      "Epoch 35, batch 57 / 76, loss: 1.106465458869934\n",
      "Epoch 35, batch 58 / 76, loss: 1.109546422958374\n",
      "Epoch 35, batch 59 / 76, loss: 1.1182563304901123\n",
      "Epoch 35, batch 60 / 76, loss: 1.074526309967041\n",
      "Epoch 35, batch 61 / 76, loss: 1.1049952507019043\n",
      "Epoch 35, batch 62 / 76, loss: 1.1181983947753906\n",
      "Epoch 35, batch 63 / 76, loss: 1.0859109163284302\n",
      "Epoch 35, batch 64 / 76, loss: 1.0463402271270752\n",
      "Epoch 35, batch 65 / 76, loss: 1.1980427503585815\n",
      "Epoch 35, batch 66 / 76, loss: 1.033560872077942\n",
      "Epoch 35, batch 67 / 76, loss: 1.1015872955322266\n",
      "Epoch 35, batch 68 / 76, loss: 1.0896105766296387\n",
      "Epoch 35, batch 69 / 76, loss: 1.0812333822250366\n",
      "Epoch 35, batch 70 / 76, loss: 1.1471502780914307\n",
      "Epoch 35, batch 71 / 76, loss: 1.1299163103103638\n",
      "Epoch 35, batch 72 / 76, loss: 1.176576852798462\n",
      "Epoch 35, batch 73 / 76, loss: 1.1371591091156006\n",
      "Epoch 35, batch 74 / 76, loss: 1.1351616382598877\n",
      "Epoch 35, batch 75 / 76, loss: 1.0020664930343628\n",
      "Epoch 35, batch 76 / 76, loss: 1.1836879253387451\n",
      "Epoch 35, batch 77 / 76, loss: 1.145198941230774\n",
      "Epoch 36, batch 1 / 76, loss: 1.1496778726577759\n",
      "Epoch 36, batch 2 / 76, loss: 1.0473679304122925\n",
      "Epoch 36, batch 3 / 76, loss: 1.106987476348877\n",
      "Epoch 36, batch 4 / 76, loss: 1.0757206678390503\n",
      "Epoch 36, batch 5 / 76, loss: 1.1392178535461426\n",
      "Epoch 36, batch 6 / 76, loss: 1.1264102458953857\n",
      "Epoch 36, batch 7 / 76, loss: 1.0971646308898926\n",
      "Epoch 36, batch 8 / 76, loss: 1.139605164527893\n",
      "Epoch 36, batch 9 / 76, loss: 1.1300849914550781\n",
      "Epoch 36, batch 10 / 76, loss: 1.0820531845092773\n",
      "Epoch 36, batch 11 / 76, loss: 1.108801007270813\n",
      "Epoch 36, batch 12 / 76, loss: 1.1137224435806274\n",
      "Epoch 36, batch 13 / 76, loss: 1.1461834907531738\n",
      "Epoch 36, batch 14 / 76, loss: 1.1041371822357178\n",
      "Epoch 36, batch 15 / 76, loss: 1.061501145362854\n",
      "Epoch 36, batch 16 / 76, loss: 1.1645382642745972\n",
      "Epoch 36, batch 17 / 76, loss: 1.08514404296875\n",
      "Epoch 36, batch 18 / 76, loss: 1.0607693195343018\n",
      "Epoch 36, batch 19 / 76, loss: 1.1065422296524048\n",
      "Epoch 36, batch 20 / 76, loss: 1.140123963356018\n",
      "Epoch 36, batch 21 / 76, loss: 1.1494003534317017\n",
      "Epoch 36, batch 22 / 76, loss: 1.0716900825500488\n",
      "Epoch 36, batch 23 / 76, loss: 1.1434390544891357\n",
      "Epoch 36, batch 24 / 76, loss: 1.0740337371826172\n",
      "Epoch 36, batch 25 / 76, loss: 1.1177723407745361\n",
      "Epoch 36, batch 26 / 76, loss: 1.1084028482437134\n",
      "Epoch 36, batch 27 / 76, loss: 1.1249829530715942\n",
      "Epoch 36, batch 28 / 76, loss: 1.0857064723968506\n",
      "Epoch 36, batch 29 / 76, loss: 1.0313711166381836\n",
      "Epoch 36, batch 30 / 76, loss: 1.1103363037109375\n",
      "Epoch 36, batch 31 / 76, loss: 1.1553887128829956\n",
      "Epoch 36, batch 32 / 76, loss: 1.1485763788223267\n",
      "Epoch 36, batch 33 / 76, loss: 1.1079777479171753\n",
      "Epoch 36, batch 34 / 76, loss: 1.1859102249145508\n",
      "Epoch 36, batch 35 / 76, loss: 1.146371603012085\n",
      "Epoch 36, batch 36 / 76, loss: 1.0408625602722168\n",
      "Epoch 36, batch 37 / 76, loss: 1.1341369152069092\n",
      "Epoch 36, batch 38 / 76, loss: 1.0588122606277466\n",
      "Epoch 36, batch 39 / 76, loss: 1.0922638177871704\n",
      "Epoch 36, batch 40 / 76, loss: 1.1425824165344238\n",
      "Epoch 36, batch 41 / 76, loss: 1.0963162183761597\n",
      "Epoch 36, batch 42 / 76, loss: 1.1493231058120728\n",
      "Epoch 36, batch 43 / 76, loss: 1.1630604267120361\n",
      "Epoch 36, batch 44 / 76, loss: 1.1207910776138306\n",
      "Epoch 36, batch 45 / 76, loss: 1.1081982851028442\n",
      "Epoch 36, batch 46 / 76, loss: 1.104832649230957\n",
      "Epoch 36, batch 47 / 76, loss: 1.1665480136871338\n",
      "Epoch 36, batch 48 / 76, loss: 1.0895624160766602\n",
      "Epoch 36, batch 49 / 76, loss: 1.123490333557129\n",
      "Epoch 36, batch 50 / 76, loss: 1.2100441455841064\n",
      "Epoch 36, batch 51 / 76, loss: 1.0510938167572021\n",
      "Epoch 36, batch 52 / 76, loss: 1.1569901704788208\n",
      "Epoch 36, batch 53 / 76, loss: 1.169335126876831\n",
      "Epoch 36, batch 54 / 76, loss: 1.0620514154434204\n",
      "Epoch 36, batch 55 / 76, loss: 1.0652164220809937\n",
      "Epoch 36, batch 56 / 76, loss: 1.1085556745529175\n",
      "Epoch 36, batch 57 / 76, loss: 1.0516951084136963\n",
      "Epoch 36, batch 58 / 76, loss: 1.1172195672988892\n",
      "Epoch 36, batch 59 / 76, loss: 1.0672093629837036\n",
      "Epoch 36, batch 60 / 76, loss: 1.0255874395370483\n",
      "Epoch 36, batch 61 / 76, loss: 1.0676512718200684\n",
      "Epoch 36, batch 62 / 76, loss: 1.1538598537445068\n",
      "Epoch 36, batch 63 / 76, loss: 1.0881407260894775\n",
      "Epoch 36, batch 64 / 76, loss: 1.126678466796875\n",
      "Epoch 36, batch 65 / 76, loss: 1.085997462272644\n",
      "Epoch 36, batch 66 / 76, loss: 1.1051673889160156\n",
      "Epoch 36, batch 67 / 76, loss: 1.122417688369751\n",
      "Epoch 36, batch 68 / 76, loss: 1.117321491241455\n",
      "Epoch 36, batch 69 / 76, loss: 1.0382463932037354\n",
      "Epoch 36, batch 70 / 76, loss: 1.0747079849243164\n",
      "Epoch 36, batch 71 / 76, loss: 1.0631693601608276\n",
      "Epoch 36, batch 72 / 76, loss: 1.1370530128479004\n",
      "Epoch 36, batch 73 / 76, loss: 1.1320915222167969\n",
      "Epoch 36, batch 74 / 76, loss: 1.0877548456192017\n",
      "Epoch 36, batch 75 / 76, loss: 1.1070959568023682\n",
      "Epoch 36, batch 76 / 76, loss: 1.0145084857940674\n",
      "Epoch 36, batch 77 / 76, loss: 0.9981606006622314\n",
      "Epoch 37, batch 1 / 76, loss: 1.0974222421646118\n",
      "Epoch 37, batch 2 / 76, loss: 1.058585286140442\n",
      "Epoch 37, batch 3 / 76, loss: 1.1275712251663208\n",
      "Epoch 37, batch 4 / 76, loss: 1.0689144134521484\n",
      "Epoch 37, batch 5 / 76, loss: 1.0396045446395874\n",
      "Epoch 37, batch 6 / 76, loss: 1.0788733959197998\n",
      "Epoch 37, batch 7 / 76, loss: 1.0871832370758057\n",
      "Epoch 37, batch 8 / 76, loss: 1.1597877740859985\n",
      "Epoch 37, batch 9 / 76, loss: 1.1013007164001465\n",
      "Epoch 37, batch 10 / 76, loss: 1.1152760982513428\n",
      "Epoch 37, batch 11 / 76, loss: 1.0903401374816895\n",
      "Epoch 37, batch 12 / 76, loss: 1.0417143106460571\n",
      "Epoch 37, batch 13 / 76, loss: 1.088174819946289\n",
      "Epoch 37, batch 14 / 76, loss: 1.089211106300354\n",
      "Epoch 37, batch 15 / 76, loss: 1.1180068254470825\n",
      "Epoch 37, batch 16 / 76, loss: 1.2123205661773682\n",
      "Epoch 37, batch 17 / 76, loss: 1.1230478286743164\n",
      "Epoch 37, batch 18 / 76, loss: 1.0954985618591309\n",
      "Epoch 37, batch 19 / 76, loss: 1.0301940441131592\n",
      "Epoch 37, batch 20 / 76, loss: 1.0245728492736816\n",
      "Epoch 37, batch 21 / 76, loss: 1.0240459442138672\n",
      "Epoch 37, batch 22 / 76, loss: 1.0768415927886963\n",
      "Epoch 37, batch 23 / 76, loss: 1.0879606008529663\n",
      "Epoch 37, batch 24 / 76, loss: 1.0768170356750488\n",
      "Epoch 37, batch 25 / 76, loss: 1.1312505006790161\n",
      "Epoch 37, batch 26 / 76, loss: 1.0683400630950928\n",
      "Epoch 37, batch 27 / 76, loss: 1.0840427875518799\n",
      "Epoch 37, batch 28 / 76, loss: 1.078645944595337\n",
      "Epoch 37, batch 29 / 76, loss: 1.0144431591033936\n",
      "Epoch 37, batch 30 / 76, loss: 1.0726124048233032\n",
      "Epoch 37, batch 31 / 76, loss: 1.0680464506149292\n",
      "Epoch 37, batch 32 / 76, loss: 1.1227396726608276\n",
      "Epoch 37, batch 33 / 76, loss: 1.0913183689117432\n",
      "Epoch 37, batch 34 / 76, loss: 1.0860557556152344\n",
      "Epoch 37, batch 35 / 76, loss: 1.0931535959243774\n",
      "Epoch 37, batch 36 / 76, loss: 1.1329741477966309\n",
      "Epoch 37, batch 37 / 76, loss: 1.1420314311981201\n",
      "Epoch 37, batch 38 / 76, loss: 1.004164457321167\n",
      "Epoch 37, batch 39 / 76, loss: 1.079754114151001\n",
      "Epoch 37, batch 40 / 76, loss: 1.0598628520965576\n",
      "Epoch 37, batch 41 / 76, loss: 1.084748387336731\n",
      "Epoch 37, batch 42 / 76, loss: 1.0998538732528687\n",
      "Epoch 37, batch 43 / 76, loss: 1.1341326236724854\n",
      "Epoch 37, batch 44 / 76, loss: 1.0713756084442139\n",
      "Epoch 37, batch 45 / 76, loss: 1.0009431838989258\n",
      "Epoch 37, batch 46 / 76, loss: 1.0434038639068604\n",
      "Epoch 37, batch 47 / 76, loss: 1.0711268186569214\n",
      "Epoch 37, batch 48 / 76, loss: 1.0972424745559692\n",
      "Epoch 37, batch 49 / 76, loss: 1.0730546712875366\n",
      "Epoch 37, batch 50 / 76, loss: 1.0813766717910767\n",
      "Epoch 37, batch 51 / 76, loss: 1.085982084274292\n",
      "Epoch 37, batch 52 / 76, loss: 1.0794814825057983\n",
      "Epoch 37, batch 53 / 76, loss: 0.9965342879295349\n",
      "Epoch 37, batch 54 / 76, loss: 1.1180667877197266\n",
      "Epoch 37, batch 55 / 76, loss: 1.0290436744689941\n",
      "Epoch 37, batch 56 / 76, loss: 1.1409519910812378\n",
      "Epoch 37, batch 57 / 76, loss: 1.0786187648773193\n",
      "Epoch 37, batch 58 / 76, loss: 1.0915310382843018\n",
      "Epoch 37, batch 59 / 76, loss: 1.032545804977417\n",
      "Epoch 37, batch 60 / 76, loss: 1.0505882501602173\n",
      "Epoch 37, batch 61 / 76, loss: 1.0507255792617798\n",
      "Epoch 37, batch 62 / 76, loss: 1.1081938743591309\n",
      "Epoch 37, batch 63 / 76, loss: 1.0733721256256104\n",
      "Epoch 37, batch 64 / 76, loss: 1.0231176614761353\n",
      "Epoch 37, batch 65 / 76, loss: 1.0624401569366455\n",
      "Epoch 37, batch 66 / 76, loss: 1.1056231260299683\n",
      "Epoch 37, batch 67 / 76, loss: 1.124570608139038\n",
      "Epoch 37, batch 68 / 76, loss: 1.042803406715393\n",
      "Epoch 37, batch 69 / 76, loss: 1.0354312658309937\n",
      "Epoch 37, batch 70 / 76, loss: 1.085777759552002\n",
      "Epoch 37, batch 71 / 76, loss: 1.1328620910644531\n",
      "Epoch 37, batch 72 / 76, loss: 1.0499658584594727\n",
      "Epoch 37, batch 73 / 76, loss: 1.0653977394104004\n",
      "Epoch 37, batch 74 / 76, loss: 1.0488049983978271\n",
      "Epoch 37, batch 75 / 76, loss: 1.1154345273971558\n",
      "Epoch 37, batch 76 / 76, loss: 1.063413381576538\n",
      "Epoch 37, batch 77 / 76, loss: 1.0605252981185913\n",
      "Epoch 38, batch 1 / 76, loss: 1.0596604347229004\n",
      "Epoch 38, batch 2 / 76, loss: 1.0621817111968994\n",
      "Epoch 38, batch 3 / 76, loss: 1.045748233795166\n",
      "Epoch 38, batch 4 / 76, loss: 1.0762628316879272\n",
      "Epoch 38, batch 5 / 76, loss: 1.042793869972229\n",
      "Epoch 38, batch 6 / 76, loss: 1.0386755466461182\n",
      "Epoch 38, batch 7 / 76, loss: 1.0363202095031738\n",
      "Epoch 38, batch 8 / 76, loss: 1.1084896326065063\n",
      "Epoch 38, batch 9 / 76, loss: 1.0523834228515625\n",
      "Epoch 38, batch 10 / 76, loss: 1.0509220361709595\n",
      "Epoch 38, batch 11 / 76, loss: 1.0791057348251343\n",
      "Epoch 38, batch 12 / 76, loss: 1.0938516855239868\n",
      "Epoch 38, batch 13 / 76, loss: 1.0667507648468018\n",
      "Epoch 38, batch 14 / 76, loss: 1.12087881565094\n",
      "Epoch 38, batch 15 / 76, loss: 1.0749316215515137\n",
      "Epoch 38, batch 16 / 76, loss: 1.0637980699539185\n",
      "Epoch 38, batch 17 / 76, loss: 1.1192078590393066\n",
      "Epoch 38, batch 18 / 76, loss: 1.100477695465088\n",
      "Epoch 38, batch 19 / 76, loss: 1.0598502159118652\n",
      "Epoch 38, batch 20 / 76, loss: 1.084499716758728\n",
      "Epoch 38, batch 21 / 76, loss: 1.03882896900177\n",
      "Epoch 38, batch 22 / 76, loss: 1.1051335334777832\n",
      "Epoch 38, batch 23 / 76, loss: 1.0756255388259888\n",
      "Epoch 38, batch 24 / 76, loss: 1.1116904020309448\n",
      "Epoch 38, batch 25 / 76, loss: 1.0848746299743652\n",
      "Epoch 38, batch 26 / 76, loss: 1.1236491203308105\n",
      "Epoch 38, batch 27 / 76, loss: 1.0069615840911865\n",
      "Epoch 38, batch 28 / 76, loss: 1.0621808767318726\n",
      "Epoch 38, batch 29 / 76, loss: 1.064980149269104\n",
      "Epoch 38, batch 30 / 76, loss: 1.060646414756775\n",
      "Epoch 38, batch 31 / 76, loss: 1.0920078754425049\n",
      "Epoch 38, batch 32 / 76, loss: 1.0621992349624634\n",
      "Epoch 38, batch 33 / 76, loss: 1.0719937086105347\n",
      "Epoch 38, batch 34 / 76, loss: 1.0924829244613647\n",
      "Epoch 38, batch 35 / 76, loss: 1.0433571338653564\n",
      "Epoch 38, batch 36 / 76, loss: 1.0568020343780518\n",
      "Epoch 38, batch 37 / 76, loss: 1.018629550933838\n",
      "Epoch 38, batch 38 / 76, loss: 1.0136823654174805\n",
      "Epoch 38, batch 39 / 76, loss: 1.090766429901123\n",
      "Epoch 38, batch 40 / 76, loss: 1.126098871231079\n",
      "Epoch 38, batch 41 / 76, loss: 1.1480553150177002\n",
      "Epoch 38, batch 42 / 76, loss: 0.9828585386276245\n",
      "Epoch 38, batch 43 / 76, loss: 1.0807526111602783\n",
      "Epoch 38, batch 44 / 76, loss: 1.054987907409668\n",
      "Epoch 38, batch 45 / 76, loss: 1.0644199848175049\n",
      "Epoch 38, batch 46 / 76, loss: 1.0600759983062744\n",
      "Epoch 38, batch 47 / 76, loss: 1.0849289894104004\n",
      "Epoch 38, batch 48 / 76, loss: 1.013651967048645\n",
      "Epoch 38, batch 49 / 76, loss: 1.1274441480636597\n",
      "Epoch 38, batch 50 / 76, loss: 1.1592957973480225\n",
      "Epoch 38, batch 51 / 76, loss: 1.0588409900665283\n",
      "Epoch 38, batch 52 / 76, loss: 1.077048897743225\n",
      "Epoch 38, batch 53 / 76, loss: 0.9804137945175171\n",
      "Epoch 38, batch 54 / 76, loss: 1.1081715822219849\n",
      "Epoch 38, batch 55 / 76, loss: 1.0479350090026855\n",
      "Epoch 38, batch 56 / 76, loss: 1.0704729557037354\n",
      "Epoch 38, batch 57 / 76, loss: 1.0442934036254883\n",
      "Epoch 38, batch 58 / 76, loss: 1.0684010982513428\n",
      "Epoch 38, batch 59 / 76, loss: 1.1146565675735474\n",
      "Epoch 38, batch 60 / 76, loss: 1.1040878295898438\n",
      "Epoch 38, batch 61 / 76, loss: 1.061277151107788\n",
      "Epoch 38, batch 62 / 76, loss: 1.0223007202148438\n",
      "Epoch 38, batch 63 / 76, loss: 1.0678014755249023\n",
      "Epoch 38, batch 64 / 76, loss: 1.0533086061477661\n",
      "Epoch 38, batch 65 / 76, loss: 1.0343135595321655\n",
      "Epoch 38, batch 66 / 76, loss: 1.0707753896713257\n",
      "Epoch 38, batch 67 / 76, loss: 1.1557416915893555\n",
      "Epoch 38, batch 68 / 76, loss: 1.033575177192688\n",
      "Epoch 38, batch 69 / 76, loss: 1.096590518951416\n",
      "Epoch 38, batch 70 / 76, loss: 1.0386991500854492\n",
      "Epoch 38, batch 71 / 76, loss: 1.0221123695373535\n",
      "Epoch 38, batch 72 / 76, loss: 1.0528055429458618\n",
      "Epoch 38, batch 73 / 76, loss: 1.0321495532989502\n",
      "Epoch 38, batch 74 / 76, loss: 1.1159276962280273\n",
      "Epoch 38, batch 75 / 76, loss: 0.957230806350708\n",
      "Epoch 38, batch 76 / 76, loss: 1.0567162036895752\n",
      "Epoch 38, batch 77 / 76, loss: 0.9229563474655151\n",
      "Epoch 39, batch 1 / 76, loss: 1.0482563972473145\n",
      "Epoch 39, batch 2 / 76, loss: 1.0578422546386719\n",
      "Epoch 39, batch 3 / 76, loss: 1.070415735244751\n",
      "Epoch 39, batch 4 / 76, loss: 1.0680718421936035\n",
      "Epoch 39, batch 5 / 76, loss: 1.0688109397888184\n",
      "Epoch 39, batch 6 / 76, loss: 1.0703233480453491\n",
      "Epoch 39, batch 7 / 76, loss: 1.0459039211273193\n",
      "Epoch 39, batch 8 / 76, loss: 1.0337755680084229\n",
      "Epoch 39, batch 9 / 76, loss: 1.0276590585708618\n",
      "Epoch 39, batch 10 / 76, loss: 1.0235140323638916\n",
      "Epoch 39, batch 11 / 76, loss: 1.0220330953598022\n",
      "Epoch 39, batch 12 / 76, loss: 1.0621929168701172\n",
      "Epoch 39, batch 13 / 76, loss: 0.9905282855033875\n",
      "Epoch 39, batch 14 / 76, loss: 1.106814980506897\n",
      "Epoch 39, batch 15 / 76, loss: 1.0277193784713745\n",
      "Epoch 39, batch 16 / 76, loss: 1.0179755687713623\n",
      "Epoch 39, batch 17 / 76, loss: 1.0520613193511963\n",
      "Epoch 39, batch 18 / 76, loss: 1.0414608716964722\n",
      "Epoch 39, batch 19 / 76, loss: 0.9996585845947266\n",
      "Epoch 39, batch 20 / 76, loss: 1.0407735109329224\n",
      "Epoch 39, batch 21 / 76, loss: 1.0461934804916382\n",
      "Epoch 39, batch 22 / 76, loss: 1.008068561553955\n",
      "Epoch 39, batch 23 / 76, loss: 1.0831241607666016\n",
      "Epoch 39, batch 24 / 76, loss: 1.0845990180969238\n",
      "Epoch 39, batch 25 / 76, loss: 1.0034089088439941\n",
      "Epoch 39, batch 26 / 76, loss: 1.026025652885437\n",
      "Epoch 39, batch 27 / 76, loss: 1.0230724811553955\n",
      "Epoch 39, batch 28 / 76, loss: 1.0447160005569458\n",
      "Epoch 39, batch 29 / 76, loss: 0.967564582824707\n",
      "Epoch 39, batch 30 / 76, loss: 1.0438551902770996\n",
      "Epoch 39, batch 31 / 76, loss: 1.0317833423614502\n",
      "Epoch 39, batch 32 / 76, loss: 1.0760352611541748\n",
      "Epoch 39, batch 33 / 76, loss: 1.0604252815246582\n",
      "Epoch 39, batch 34 / 76, loss: 1.0675394535064697\n",
      "Epoch 39, batch 35 / 76, loss: 1.0512570142745972\n",
      "Epoch 39, batch 36 / 76, loss: 1.0763909816741943\n",
      "Epoch 39, batch 37 / 76, loss: 1.0940418243408203\n",
      "Epoch 39, batch 38 / 76, loss: 1.0479844808578491\n",
      "Epoch 39, batch 39 / 76, loss: 1.0510993003845215\n",
      "Epoch 39, batch 40 / 76, loss: 1.0376266241073608\n",
      "Epoch 39, batch 41 / 76, loss: 1.0653892755508423\n",
      "Epoch 39, batch 42 / 76, loss: 1.0791603326797485\n",
      "Epoch 39, batch 43 / 76, loss: 1.046884298324585\n",
      "Epoch 39, batch 44 / 76, loss: 1.092032790184021\n",
      "Epoch 39, batch 45 / 76, loss: 1.0858150720596313\n",
      "Epoch 39, batch 46 / 76, loss: 1.0629582405090332\n",
      "Epoch 39, batch 47 / 76, loss: 1.0743192434310913\n",
      "Epoch 39, batch 48 / 76, loss: 1.0058785676956177\n",
      "Epoch 39, batch 49 / 76, loss: 1.020182490348816\n",
      "Epoch 39, batch 50 / 76, loss: 0.9947839379310608\n",
      "Epoch 39, batch 51 / 76, loss: 1.0739887952804565\n",
      "Epoch 39, batch 52 / 76, loss: 1.0512993335723877\n",
      "Epoch 39, batch 53 / 76, loss: 1.1016432046890259\n",
      "Epoch 39, batch 54 / 76, loss: 1.0552990436553955\n",
      "Epoch 39, batch 55 / 76, loss: 1.0887988805770874\n",
      "Epoch 39, batch 56 / 76, loss: 1.0543553829193115\n",
      "Epoch 39, batch 57 / 76, loss: 1.0090020895004272\n",
      "Epoch 39, batch 58 / 76, loss: 1.0918090343475342\n",
      "Epoch 39, batch 59 / 76, loss: 1.0332551002502441\n",
      "Epoch 39, batch 60 / 76, loss: 1.016252040863037\n",
      "Epoch 39, batch 61 / 76, loss: 1.007659912109375\n",
      "Epoch 39, batch 62 / 76, loss: 1.0395770072937012\n",
      "Epoch 39, batch 63 / 76, loss: 1.0455325841903687\n",
      "Epoch 39, batch 64 / 76, loss: 1.0059270858764648\n",
      "Epoch 39, batch 65 / 76, loss: 1.0979968309402466\n",
      "Epoch 39, batch 66 / 76, loss: 1.1374375820159912\n",
      "Epoch 39, batch 67 / 76, loss: 0.991960883140564\n",
      "Epoch 39, batch 68 / 76, loss: 1.08954918384552\n",
      "Epoch 39, batch 69 / 76, loss: 1.0007423162460327\n",
      "Epoch 39, batch 70 / 76, loss: 1.0425941944122314\n",
      "Epoch 39, batch 71 / 76, loss: 1.0364453792572021\n",
      "Epoch 39, batch 72 / 76, loss: 1.0368573665618896\n",
      "Epoch 39, batch 73 / 76, loss: 1.0083017349243164\n",
      "Epoch 39, batch 74 / 76, loss: 1.0742191076278687\n",
      "Epoch 39, batch 75 / 76, loss: 1.0452642440795898\n",
      "Epoch 39, batch 76 / 76, loss: 1.01725435256958\n",
      "Epoch 39, batch 77 / 76, loss: 1.0264850854873657\n",
      "Epoch 40, batch 1 / 76, loss: 1.0255117416381836\n",
      "Epoch 40, batch 2 / 76, loss: 1.0525944232940674\n",
      "Epoch 40, batch 3 / 76, loss: 1.0232841968536377\n",
      "Epoch 40, batch 4 / 76, loss: 1.014328956604004\n",
      "Epoch 40, batch 5 / 76, loss: 1.065806269645691\n",
      "Epoch 40, batch 6 / 76, loss: 1.0426090955734253\n",
      "Epoch 40, batch 7 / 76, loss: 1.01499342918396\n",
      "Epoch 40, batch 8 / 76, loss: 1.0320019721984863\n",
      "Epoch 40, batch 9 / 76, loss: 1.029812216758728\n",
      "Epoch 40, batch 10 / 76, loss: 1.0651094913482666\n",
      "Epoch 40, batch 11 / 76, loss: 0.9719065427780151\n",
      "Epoch 40, batch 12 / 76, loss: 1.0630476474761963\n",
      "Epoch 40, batch 13 / 76, loss: 1.0089019536972046\n",
      "Epoch 40, batch 14 / 76, loss: 1.0449129343032837\n",
      "Epoch 40, batch 15 / 76, loss: 0.9577596187591553\n",
      "Epoch 40, batch 16 / 76, loss: 1.095810055732727\n",
      "Epoch 40, batch 17 / 76, loss: 1.0081431865692139\n",
      "Epoch 40, batch 18 / 76, loss: 1.0490516424179077\n",
      "Epoch 40, batch 19 / 76, loss: 1.015980839729309\n",
      "Epoch 40, batch 20 / 76, loss: 1.054594874382019\n",
      "Epoch 40, batch 21 / 76, loss: 0.988319993019104\n",
      "Epoch 40, batch 22 / 76, loss: 1.0589780807495117\n",
      "Epoch 40, batch 23 / 76, loss: 1.1591553688049316\n",
      "Epoch 40, batch 24 / 76, loss: 1.0002634525299072\n",
      "Epoch 40, batch 25 / 76, loss: 1.0157158374786377\n",
      "Epoch 40, batch 26 / 76, loss: 1.1334514617919922\n",
      "Epoch 40, batch 27 / 76, loss: 1.00990629196167\n",
      "Epoch 40, batch 28 / 76, loss: 0.9913920164108276\n",
      "Epoch 40, batch 29 / 76, loss: 0.9721059203147888\n",
      "Epoch 40, batch 30 / 76, loss: 1.0107017755508423\n",
      "Epoch 40, batch 31 / 76, loss: 0.9356908798217773\n",
      "Epoch 40, batch 32 / 76, loss: 1.007197380065918\n",
      "Epoch 40, batch 33 / 76, loss: 0.981216549873352\n",
      "Epoch 40, batch 34 / 76, loss: 1.0251507759094238\n",
      "Epoch 40, batch 35 / 76, loss: 1.0747182369232178\n",
      "Epoch 40, batch 36 / 76, loss: 1.0393377542495728\n",
      "Epoch 40, batch 37 / 76, loss: 1.0224214792251587\n",
      "Epoch 40, batch 38 / 76, loss: 0.9412680864334106\n",
      "Epoch 40, batch 39 / 76, loss: 1.0914967060089111\n",
      "Epoch 40, batch 40 / 76, loss: 0.9814013838768005\n",
      "Epoch 40, batch 41 / 76, loss: 1.10051691532135\n",
      "Epoch 40, batch 42 / 76, loss: 1.0468982458114624\n",
      "Epoch 40, batch 43 / 76, loss: 1.0112422704696655\n",
      "Epoch 40, batch 44 / 76, loss: 1.034477949142456\n",
      "Epoch 40, batch 45 / 76, loss: 1.111520528793335\n",
      "Epoch 40, batch 46 / 76, loss: 1.0151649713516235\n",
      "Epoch 40, batch 47 / 76, loss: 1.0646030902862549\n",
      "Epoch 40, batch 48 / 76, loss: 1.0751545429229736\n",
      "Epoch 40, batch 49 / 76, loss: 1.0543925762176514\n",
      "Epoch 40, batch 50 / 76, loss: 0.9483248591423035\n",
      "Epoch 40, batch 51 / 76, loss: 1.0830711126327515\n",
      "Epoch 40, batch 52 / 76, loss: 1.00908625125885\n",
      "Epoch 40, batch 53 / 76, loss: 0.9588174819946289\n",
      "Epoch 40, batch 54 / 76, loss: 1.005704402923584\n",
      "Epoch 40, batch 55 / 76, loss: 0.9541157484054565\n",
      "Epoch 40, batch 56 / 76, loss: 0.9865295886993408\n",
      "Epoch 40, batch 57 / 76, loss: 0.9849895238876343\n",
      "Epoch 40, batch 58 / 76, loss: 1.0341349840164185\n",
      "Epoch 40, batch 59 / 76, loss: 1.0854058265686035\n",
      "Epoch 40, batch 60 / 76, loss: 1.0087294578552246\n",
      "Epoch 40, batch 61 / 76, loss: 0.9833795428276062\n",
      "Epoch 40, batch 62 / 76, loss: 1.1072008609771729\n",
      "Epoch 40, batch 63 / 76, loss: 1.029865026473999\n",
      "Epoch 40, batch 64 / 76, loss: 1.061347246170044\n",
      "Epoch 40, batch 65 / 76, loss: 1.0573742389678955\n",
      "Epoch 40, batch 66 / 76, loss: 1.0549671649932861\n",
      "Epoch 40, batch 67 / 76, loss: 1.0687544345855713\n",
      "Epoch 40, batch 68 / 76, loss: 0.9818044900894165\n",
      "Epoch 40, batch 69 / 76, loss: 1.047120213508606\n",
      "Epoch 40, batch 70 / 76, loss: 1.0673052072525024\n",
      "Epoch 40, batch 71 / 76, loss: 0.9876975417137146\n",
      "Epoch 40, batch 72 / 76, loss: 1.0054584741592407\n",
      "Epoch 40, batch 73 / 76, loss: 0.9970836639404297\n",
      "Epoch 40, batch 74 / 76, loss: 1.007987380027771\n",
      "Epoch 40, batch 75 / 76, loss: 1.0206573009490967\n",
      "Epoch 40, batch 76 / 76, loss: 0.9942816495895386\n",
      "Epoch 40, batch 77 / 76, loss: 1.1207607984542847\n",
      "Epoch 41, batch 1 / 76, loss: 1.0569777488708496\n",
      "Epoch 41, batch 2 / 76, loss: 1.0222606658935547\n",
      "Epoch 41, batch 3 / 76, loss: 0.9794940948486328\n",
      "Epoch 41, batch 4 / 76, loss: 0.9780884981155396\n",
      "Epoch 41, batch 5 / 76, loss: 1.009145736694336\n",
      "Epoch 41, batch 6 / 76, loss: 1.0014894008636475\n",
      "Epoch 41, batch 7 / 76, loss: 0.9738669991493225\n",
      "Epoch 41, batch 8 / 76, loss: 1.0640395879745483\n",
      "Epoch 41, batch 9 / 76, loss: 0.9986488819122314\n",
      "Epoch 41, batch 10 / 76, loss: 1.0406100749969482\n",
      "Epoch 41, batch 11 / 76, loss: 0.9676929712295532\n",
      "Epoch 41, batch 12 / 76, loss: 1.038659691810608\n",
      "Epoch 41, batch 13 / 76, loss: 1.099314570426941\n",
      "Epoch 41, batch 14 / 76, loss: 1.0143769979476929\n",
      "Epoch 41, batch 15 / 76, loss: 1.0006356239318848\n",
      "Epoch 41, batch 16 / 76, loss: 1.1132854223251343\n",
      "Epoch 41, batch 17 / 76, loss: 0.9766455888748169\n",
      "Epoch 41, batch 18 / 76, loss: 0.974234938621521\n",
      "Epoch 41, batch 19 / 76, loss: 0.959736704826355\n",
      "Epoch 41, batch 20 / 76, loss: 1.0046725273132324\n",
      "Epoch 41, batch 21 / 76, loss: 0.9698060154914856\n",
      "Epoch 41, batch 22 / 76, loss: 1.0278596878051758\n",
      "Epoch 41, batch 23 / 76, loss: 0.942211389541626\n",
      "Epoch 41, batch 24 / 76, loss: 1.0298776626586914\n",
      "Epoch 41, batch 25 / 76, loss: 1.0265344381332397\n",
      "Epoch 41, batch 26 / 76, loss: 1.0196315050125122\n",
      "Epoch 41, batch 27 / 76, loss: 0.9744912385940552\n",
      "Epoch 41, batch 28 / 76, loss: 1.0542874336242676\n",
      "Epoch 41, batch 29 / 76, loss: 1.012556791305542\n",
      "Epoch 41, batch 30 / 76, loss: 1.0681167840957642\n",
      "Epoch 41, batch 31 / 76, loss: 0.9816111326217651\n",
      "Epoch 41, batch 32 / 76, loss: 1.0597914457321167\n",
      "Epoch 41, batch 33 / 76, loss: 1.0840073823928833\n",
      "Epoch 41, batch 34 / 76, loss: 1.022472620010376\n",
      "Epoch 41, batch 35 / 76, loss: 0.990752100944519\n",
      "Epoch 41, batch 36 / 76, loss: 0.9294962882995605\n",
      "Epoch 41, batch 37 / 76, loss: 1.0154863595962524\n",
      "Epoch 41, batch 38 / 76, loss: 0.9990615844726562\n",
      "Epoch 41, batch 39 / 76, loss: 0.9247455596923828\n",
      "Epoch 41, batch 40 / 76, loss: 0.9377772212028503\n",
      "Epoch 41, batch 41 / 76, loss: 0.9800536632537842\n",
      "Epoch 41, batch 42 / 76, loss: 0.9305110573768616\n",
      "Epoch 41, batch 43 / 76, loss: 0.9290052652359009\n",
      "Epoch 41, batch 44 / 76, loss: 1.0058971643447876\n",
      "Epoch 41, batch 45 / 76, loss: 1.045516014099121\n",
      "Epoch 41, batch 46 / 76, loss: 0.9916385412216187\n",
      "Epoch 41, batch 47 / 76, loss: 0.9991309642791748\n",
      "Epoch 41, batch 48 / 76, loss: 1.0086266994476318\n",
      "Epoch 41, batch 49 / 76, loss: 1.010474681854248\n",
      "Epoch 41, batch 50 / 76, loss: 0.9865162372589111\n",
      "Epoch 41, batch 51 / 76, loss: 1.001845121383667\n",
      "Epoch 41, batch 52 / 76, loss: 1.0118552446365356\n",
      "Epoch 41, batch 53 / 76, loss: 0.9768706560134888\n",
      "Epoch 41, batch 54 / 76, loss: 1.0188214778900146\n",
      "Epoch 41, batch 55 / 76, loss: 1.0404072999954224\n",
      "Epoch 41, batch 56 / 76, loss: 0.9996278285980225\n",
      "Epoch 41, batch 57 / 76, loss: 0.9933277368545532\n",
      "Epoch 41, batch 58 / 76, loss: 0.9626182913780212\n",
      "Epoch 41, batch 59 / 76, loss: 0.9930182695388794\n",
      "Epoch 41, batch 60 / 76, loss: 1.05448579788208\n",
      "Epoch 41, batch 61 / 76, loss: 1.1156283617019653\n",
      "Epoch 41, batch 62 / 76, loss: 1.053783893585205\n",
      "Epoch 41, batch 63 / 76, loss: 1.0095523595809937\n",
      "Epoch 41, batch 64 / 76, loss: 1.0194313526153564\n",
      "Epoch 41, batch 65 / 76, loss: 0.9923800230026245\n",
      "Epoch 41, batch 66 / 76, loss: 0.9798544645309448\n",
      "Epoch 41, batch 67 / 76, loss: 1.0104002952575684\n",
      "Epoch 41, batch 68 / 76, loss: 1.011305809020996\n",
      "Epoch 41, batch 69 / 76, loss: 1.0613971948623657\n",
      "Epoch 41, batch 70 / 76, loss: 0.9766979217529297\n",
      "Epoch 41, batch 71 / 76, loss: 1.0051660537719727\n",
      "Epoch 41, batch 72 / 76, loss: 0.9898744821548462\n",
      "Epoch 41, batch 73 / 76, loss: 1.0494968891143799\n",
      "Epoch 41, batch 74 / 76, loss: 1.037301778793335\n",
      "Epoch 41, batch 75 / 76, loss: 0.9825613498687744\n",
      "Epoch 41, batch 76 / 76, loss: 1.0126357078552246\n",
      "Epoch 41, batch 77 / 76, loss: 0.8733938932418823\n",
      "Epoch 42, batch 1 / 76, loss: 1.0197737216949463\n",
      "Epoch 42, batch 2 / 76, loss: 1.0075767040252686\n",
      "Epoch 42, batch 3 / 76, loss: 1.0271451473236084\n",
      "Epoch 42, batch 4 / 76, loss: 1.0541529655456543\n",
      "Epoch 42, batch 5 / 76, loss: 0.928848147392273\n",
      "Epoch 42, batch 6 / 76, loss: 0.9551783800125122\n",
      "Epoch 42, batch 7 / 76, loss: 1.0331897735595703\n",
      "Epoch 42, batch 8 / 76, loss: 0.9425296783447266\n",
      "Epoch 42, batch 9 / 76, loss: 1.045271635055542\n",
      "Epoch 42, batch 10 / 76, loss: 1.001285195350647\n",
      "Epoch 42, batch 11 / 76, loss: 0.9704667329788208\n",
      "Epoch 42, batch 12 / 76, loss: 0.987487256526947\n",
      "Epoch 42, batch 13 / 76, loss: 1.0406349897384644\n",
      "Epoch 42, batch 14 / 76, loss: 1.06209397315979\n",
      "Epoch 42, batch 15 / 76, loss: 0.9725620150566101\n",
      "Epoch 42, batch 16 / 76, loss: 1.0459486246109009\n",
      "Epoch 42, batch 17 / 76, loss: 0.9947596788406372\n",
      "Epoch 42, batch 18 / 76, loss: 0.9585793614387512\n",
      "Epoch 42, batch 19 / 76, loss: 1.0325825214385986\n",
      "Epoch 42, batch 20 / 76, loss: 1.0316288471221924\n",
      "Epoch 42, batch 21 / 76, loss: 1.0294321775436401\n",
      "Epoch 42, batch 22 / 76, loss: 0.9763929843902588\n",
      "Epoch 42, batch 23 / 76, loss: 1.0969372987747192\n",
      "Epoch 42, batch 24 / 76, loss: 1.0071775913238525\n",
      "Epoch 42, batch 25 / 76, loss: 1.0588876008987427\n",
      "Epoch 42, batch 26 / 76, loss: 1.1000639200210571\n",
      "Epoch 42, batch 27 / 76, loss: 0.9989092350006104\n",
      "Epoch 42, batch 28 / 76, loss: 0.9876171350479126\n",
      "Epoch 42, batch 29 / 76, loss: 1.0532405376434326\n",
      "Epoch 42, batch 30 / 76, loss: 1.0301622152328491\n",
      "Epoch 42, batch 31 / 76, loss: 1.0078576803207397\n",
      "Epoch 42, batch 32 / 76, loss: 1.0103614330291748\n",
      "Epoch 42, batch 33 / 76, loss: 0.9462465047836304\n",
      "Epoch 42, batch 34 / 76, loss: 0.9809547662734985\n",
      "Epoch 42, batch 35 / 76, loss: 1.0294427871704102\n",
      "Epoch 42, batch 36 / 76, loss: 0.9685341119766235\n",
      "Epoch 42, batch 37 / 76, loss: 1.0387554168701172\n",
      "Epoch 42, batch 38 / 76, loss: 1.0066078901290894\n",
      "Epoch 42, batch 39 / 76, loss: 1.0658559799194336\n",
      "Epoch 42, batch 40 / 76, loss: 0.9720876216888428\n",
      "Epoch 42, batch 41 / 76, loss: 0.9723038673400879\n",
      "Epoch 42, batch 42 / 76, loss: 1.041819453239441\n",
      "Epoch 42, batch 43 / 76, loss: 1.032617449760437\n",
      "Epoch 42, batch 44 / 76, loss: 1.0137869119644165\n",
      "Epoch 42, batch 45 / 76, loss: 0.9981369972229004\n",
      "Epoch 42, batch 46 / 76, loss: 1.0331677198410034\n",
      "Epoch 42, batch 47 / 76, loss: 1.0217657089233398\n",
      "Epoch 42, batch 48 / 76, loss: 0.9911928176879883\n",
      "Epoch 42, batch 49 / 76, loss: 0.967609167098999\n",
      "Epoch 42, batch 50 / 76, loss: 1.0185415744781494\n",
      "Epoch 42, batch 51 / 76, loss: 0.9769046306610107\n",
      "Epoch 42, batch 52 / 76, loss: 1.000390887260437\n",
      "Epoch 42, batch 53 / 76, loss: 0.9418606162071228\n",
      "Epoch 42, batch 54 / 76, loss: 0.9658684730529785\n",
      "Epoch 42, batch 55 / 76, loss: 0.9812954664230347\n",
      "Epoch 42, batch 56 / 76, loss: 0.9823362827301025\n",
      "Epoch 42, batch 57 / 76, loss: 1.0187859535217285\n",
      "Epoch 42, batch 58 / 76, loss: 1.0463223457336426\n",
      "Epoch 42, batch 59 / 76, loss: 1.084265112876892\n",
      "Epoch 42, batch 60 / 76, loss: 1.0403344631195068\n",
      "Epoch 42, batch 61 / 76, loss: 1.002023696899414\n",
      "Epoch 42, batch 62 / 76, loss: 0.9951440691947937\n",
      "Epoch 42, batch 63 / 76, loss: 1.0148197412490845\n",
      "Epoch 42, batch 64 / 76, loss: 1.0253615379333496\n",
      "Epoch 42, batch 65 / 76, loss: 0.9953979253768921\n",
      "Epoch 42, batch 66 / 76, loss: 1.0497227907180786\n",
      "Epoch 42, batch 67 / 76, loss: 1.0019211769104004\n",
      "Epoch 42, batch 68 / 76, loss: 1.0096335411071777\n",
      "Epoch 42, batch 69 / 76, loss: 1.03635573387146\n",
      "Epoch 42, batch 70 / 76, loss: 0.9963629245758057\n",
      "Epoch 42, batch 71 / 76, loss: 1.024919867515564\n",
      "Epoch 42, batch 72 / 76, loss: 0.991431474685669\n",
      "Epoch 42, batch 73 / 76, loss: 0.9404867887496948\n",
      "Epoch 42, batch 74 / 76, loss: 0.9509855508804321\n",
      "Epoch 42, batch 75 / 76, loss: 0.99540114402771\n",
      "Epoch 42, batch 76 / 76, loss: 0.9947388172149658\n",
      "Epoch 42, batch 77 / 76, loss: 0.9941390752792358\n",
      "Epoch 43, batch 1 / 76, loss: 1.1117298603057861\n",
      "Epoch 43, batch 2 / 76, loss: 1.0274120569229126\n",
      "Epoch 43, batch 3 / 76, loss: 0.989798903465271\n",
      "Epoch 43, batch 4 / 76, loss: 0.9696817994117737\n",
      "Epoch 43, batch 5 / 76, loss: 0.9910145401954651\n",
      "Epoch 43, batch 6 / 76, loss: 1.0114487409591675\n",
      "Epoch 43, batch 7 / 76, loss: 0.9514999389648438\n",
      "Epoch 43, batch 8 / 76, loss: 0.9551364183425903\n",
      "Epoch 43, batch 9 / 76, loss: 1.0409739017486572\n",
      "Epoch 43, batch 10 / 76, loss: 0.9333686232566833\n",
      "Epoch 43, batch 11 / 76, loss: 0.986102819442749\n",
      "Epoch 43, batch 12 / 76, loss: 0.949764609336853\n",
      "Epoch 43, batch 13 / 76, loss: 1.0057624578475952\n",
      "Epoch 43, batch 14 / 76, loss: 0.9880996942520142\n",
      "Epoch 43, batch 15 / 76, loss: 1.0290703773498535\n",
      "Epoch 43, batch 16 / 76, loss: 0.9961782693862915\n",
      "Epoch 43, batch 17 / 76, loss: 1.0204883813858032\n",
      "Epoch 43, batch 18 / 76, loss: 1.0334694385528564\n",
      "Epoch 43, batch 19 / 76, loss: 1.0194517374038696\n",
      "Epoch 43, batch 20 / 76, loss: 1.0441932678222656\n",
      "Epoch 43, batch 21 / 76, loss: 1.0295312404632568\n",
      "Epoch 43, batch 22 / 76, loss: 0.9942282438278198\n",
      "Epoch 43, batch 23 / 76, loss: 1.0154945850372314\n",
      "Epoch 43, batch 24 / 76, loss: 0.9905482530593872\n",
      "Epoch 43, batch 25 / 76, loss: 1.0254216194152832\n",
      "Epoch 43, batch 26 / 76, loss: 0.989111065864563\n",
      "Epoch 43, batch 27 / 76, loss: 0.9523791074752808\n",
      "Epoch 43, batch 28 / 76, loss: 1.010819673538208\n",
      "Epoch 43, batch 29 / 76, loss: 1.015950322151184\n",
      "Epoch 43, batch 30 / 76, loss: 0.9869328141212463\n",
      "Epoch 43, batch 31 / 76, loss: 1.0105087757110596\n",
      "Epoch 43, batch 32 / 76, loss: 0.9891386032104492\n",
      "Epoch 43, batch 33 / 76, loss: 0.9514545798301697\n",
      "Epoch 43, batch 34 / 76, loss: 0.9659508466720581\n",
      "Epoch 43, batch 35 / 76, loss: 1.0215785503387451\n",
      "Epoch 43, batch 36 / 76, loss: 0.9789444208145142\n",
      "Epoch 43, batch 37 / 76, loss: 1.098581075668335\n",
      "Epoch 43, batch 38 / 76, loss: 0.9587757587432861\n",
      "Epoch 43, batch 39 / 76, loss: 1.0210254192352295\n",
      "Epoch 43, batch 40 / 76, loss: 0.9883830547332764\n",
      "Epoch 43, batch 41 / 76, loss: 0.9063401222229004\n",
      "Epoch 43, batch 42 / 76, loss: 0.941618800163269\n",
      "Epoch 43, batch 43 / 76, loss: 1.0150222778320312\n",
      "Epoch 43, batch 44 / 76, loss: 0.9165666103363037\n",
      "Epoch 43, batch 45 / 76, loss: 0.959852933883667\n",
      "Epoch 43, batch 46 / 76, loss: 0.9740759134292603\n",
      "Epoch 43, batch 47 / 76, loss: 1.0061790943145752\n",
      "Epoch 43, batch 48 / 76, loss: 1.0100867748260498\n",
      "Epoch 43, batch 49 / 76, loss: 1.03413724899292\n",
      "Epoch 43, batch 50 / 76, loss: 0.9626078605651855\n",
      "Epoch 43, batch 51 / 76, loss: 0.9020280838012695\n",
      "Epoch 43, batch 52 / 76, loss: 0.9821281433105469\n",
      "Epoch 43, batch 53 / 76, loss: 1.059279203414917\n",
      "Epoch 43, batch 54 / 76, loss: 0.9804021120071411\n",
      "Epoch 43, batch 55 / 76, loss: 0.9667530655860901\n",
      "Epoch 43, batch 56 / 76, loss: 1.042132019996643\n",
      "Epoch 43, batch 57 / 76, loss: 1.0223383903503418\n",
      "Epoch 43, batch 58 / 76, loss: 1.0388898849487305\n",
      "Epoch 43, batch 59 / 76, loss: 0.9757888317108154\n",
      "Epoch 43, batch 60 / 76, loss: 1.0061832666397095\n",
      "Epoch 43, batch 61 / 76, loss: 1.0095239877700806\n",
      "Epoch 43, batch 62 / 76, loss: 1.0860141515731812\n",
      "Epoch 43, batch 63 / 76, loss: 1.011474370956421\n",
      "Epoch 43, batch 64 / 76, loss: 0.9914261698722839\n",
      "Epoch 43, batch 65 / 76, loss: 1.0073542594909668\n",
      "Epoch 43, batch 66 / 76, loss: 1.0039396286010742\n",
      "Epoch 43, batch 67 / 76, loss: 0.9516181945800781\n",
      "Epoch 43, batch 68 / 76, loss: 0.9252657294273376\n",
      "Epoch 43, batch 69 / 76, loss: 1.0065407752990723\n",
      "Epoch 43, batch 70 / 76, loss: 0.9690626263618469\n",
      "Epoch 43, batch 71 / 76, loss: 1.0232306718826294\n",
      "Epoch 43, batch 72 / 76, loss: 1.0021071434020996\n",
      "Epoch 43, batch 73 / 76, loss: 0.9665246605873108\n",
      "Epoch 43, batch 74 / 76, loss: 0.9740724563598633\n",
      "Epoch 43, batch 75 / 76, loss: 1.0086371898651123\n",
      "Epoch 43, batch 76 / 76, loss: 0.9333842992782593\n",
      "Epoch 43, batch 77 / 76, loss: 0.9769788980484009\n",
      "Epoch 44, batch 1 / 76, loss: 0.9579496383666992\n",
      "Epoch 44, batch 2 / 76, loss: 0.9688466787338257\n",
      "Epoch 44, batch 3 / 76, loss: 0.956210196018219\n",
      "Epoch 44, batch 4 / 76, loss: 1.0041146278381348\n",
      "Epoch 44, batch 5 / 76, loss: 0.9884999394416809\n",
      "Epoch 44, batch 6 / 76, loss: 0.9599794745445251\n",
      "Epoch 44, batch 7 / 76, loss: 0.9497227668762207\n",
      "Epoch 44, batch 8 / 76, loss: 0.9711259603500366\n",
      "Epoch 44, batch 9 / 76, loss: 0.9494229555130005\n",
      "Epoch 44, batch 10 / 76, loss: 0.9952965378761292\n",
      "Epoch 44, batch 11 / 76, loss: 0.9304407238960266\n",
      "Epoch 44, batch 12 / 76, loss: 1.037365198135376\n",
      "Epoch 44, batch 13 / 76, loss: 0.9823911190032959\n",
      "Epoch 44, batch 14 / 76, loss: 0.9653381109237671\n",
      "Epoch 44, batch 15 / 76, loss: 0.9668909311294556\n",
      "Epoch 44, batch 16 / 76, loss: 0.966808557510376\n",
      "Epoch 44, batch 17 / 76, loss: 0.8848894834518433\n",
      "Epoch 44, batch 18 / 76, loss: 0.984399676322937\n",
      "Epoch 44, batch 19 / 76, loss: 1.0189647674560547\n",
      "Epoch 44, batch 20 / 76, loss: 1.0063564777374268\n",
      "Epoch 44, batch 21 / 76, loss: 0.993583083152771\n",
      "Epoch 44, batch 22 / 76, loss: 1.0102720260620117\n",
      "Epoch 44, batch 23 / 76, loss: 0.9107323884963989\n",
      "Epoch 44, batch 24 / 76, loss: 0.9196102619171143\n",
      "Epoch 44, batch 25 / 76, loss: 0.9439756274223328\n",
      "Epoch 44, batch 26 / 76, loss: 1.0383248329162598\n",
      "Epoch 44, batch 27 / 76, loss: 0.9692659378051758\n",
      "Epoch 44, batch 28 / 76, loss: 0.9667471647262573\n",
      "Epoch 44, batch 29 / 76, loss: 1.0005192756652832\n",
      "Epoch 44, batch 30 / 76, loss: 0.9615999460220337\n",
      "Epoch 44, batch 31 / 76, loss: 0.9682930707931519\n",
      "Epoch 44, batch 32 / 76, loss: 0.9486968517303467\n",
      "Epoch 44, batch 33 / 76, loss: 0.8644932508468628\n",
      "Epoch 44, batch 34 / 76, loss: 1.0381159782409668\n",
      "Epoch 44, batch 35 / 76, loss: 0.9237154722213745\n",
      "Epoch 44, batch 36 / 76, loss: 0.9594635963439941\n",
      "Epoch 44, batch 37 / 76, loss: 0.9693189859390259\n",
      "Epoch 44, batch 38 / 76, loss: 0.9778359532356262\n",
      "Epoch 44, batch 39 / 76, loss: 1.003404974937439\n",
      "Epoch 44, batch 40 / 76, loss: 0.9587907195091248\n",
      "Epoch 44, batch 41 / 76, loss: 0.9125864505767822\n",
      "Epoch 44, batch 42 / 76, loss: 1.0257139205932617\n",
      "Epoch 44, batch 43 / 76, loss: 0.965799868106842\n",
      "Epoch 44, batch 44 / 76, loss: 0.9697337746620178\n",
      "Epoch 44, batch 45 / 76, loss: 1.0332475900650024\n",
      "Epoch 44, batch 46 / 76, loss: 0.922757625579834\n",
      "Epoch 44, batch 47 / 76, loss: 0.9373804330825806\n",
      "Epoch 44, batch 48 / 76, loss: 0.9267722368240356\n",
      "Epoch 44, batch 49 / 76, loss: 1.010449767112732\n",
      "Epoch 44, batch 50 / 76, loss: 0.9853249788284302\n",
      "Epoch 44, batch 51 / 76, loss: 1.0287957191467285\n",
      "Epoch 44, batch 52 / 76, loss: 1.0244430303573608\n",
      "Epoch 44, batch 53 / 76, loss: 0.9795050621032715\n",
      "Epoch 44, batch 54 / 76, loss: 0.927420973777771\n",
      "Epoch 44, batch 55 / 76, loss: 0.9809670448303223\n",
      "Epoch 44, batch 56 / 76, loss: 0.9603374004364014\n",
      "Epoch 44, batch 57 / 76, loss: 0.9545858502388\n",
      "Epoch 44, batch 58 / 76, loss: 0.8996168375015259\n",
      "Epoch 44, batch 59 / 76, loss: 0.9390020966529846\n",
      "Epoch 44, batch 60 / 76, loss: 0.9771169424057007\n",
      "Epoch 44, batch 61 / 76, loss: 1.0071370601654053\n",
      "Epoch 44, batch 62 / 76, loss: 0.9684872627258301\n",
      "Epoch 44, batch 63 / 76, loss: 0.9758716225624084\n",
      "Epoch 44, batch 64 / 76, loss: 0.9633792638778687\n",
      "Epoch 44, batch 65 / 76, loss: 0.9541653394699097\n",
      "Epoch 44, batch 66 / 76, loss: 0.966254472732544\n",
      "Epoch 44, batch 67 / 76, loss: 0.9851480722427368\n",
      "Epoch 44, batch 68 / 76, loss: 0.9813072085380554\n",
      "Epoch 44, batch 69 / 76, loss: 1.0359188318252563\n",
      "Epoch 44, batch 70 / 76, loss: 0.9662848114967346\n",
      "Epoch 44, batch 71 / 76, loss: 0.953961968421936\n",
      "Epoch 44, batch 72 / 76, loss: 0.9863792657852173\n",
      "Epoch 44, batch 73 / 76, loss: 0.9515693187713623\n",
      "Epoch 44, batch 74 / 76, loss: 0.918984055519104\n",
      "Epoch 44, batch 75 / 76, loss: 0.9664469957351685\n",
      "Epoch 44, batch 76 / 76, loss: 0.94990473985672\n",
      "Epoch 44, batch 77 / 76, loss: 0.9041504859924316\n",
      "Epoch 45, batch 1 / 76, loss: 0.9539455771446228\n",
      "Epoch 45, batch 2 / 76, loss: 0.9713681936264038\n",
      "Epoch 45, batch 3 / 76, loss: 0.953276515007019\n",
      "Epoch 45, batch 4 / 76, loss: 0.9646106958389282\n",
      "Epoch 45, batch 5 / 76, loss: 0.9580913782119751\n",
      "Epoch 45, batch 6 / 76, loss: 0.9838876724243164\n",
      "Epoch 45, batch 7 / 76, loss: 0.9059582352638245\n",
      "Epoch 45, batch 8 / 76, loss: 0.962415874004364\n",
      "Epoch 45, batch 9 / 76, loss: 0.925261378288269\n",
      "Epoch 45, batch 10 / 76, loss: 1.0170999765396118\n",
      "Epoch 45, batch 11 / 76, loss: 0.9859287142753601\n",
      "Epoch 45, batch 12 / 76, loss: 0.9257676601409912\n",
      "Epoch 45, batch 13 / 76, loss: 1.001952886581421\n",
      "Epoch 45, batch 14 / 76, loss: 0.9248449802398682\n",
      "Epoch 45, batch 15 / 76, loss: 0.9720672369003296\n",
      "Epoch 45, batch 16 / 76, loss: 0.9493923187255859\n",
      "Epoch 45, batch 17 / 76, loss: 0.9524502158164978\n",
      "Epoch 45, batch 18 / 76, loss: 0.9606481194496155\n",
      "Epoch 45, batch 19 / 76, loss: 0.9301691651344299\n",
      "Epoch 45, batch 20 / 76, loss: 0.9621614813804626\n",
      "Epoch 45, batch 21 / 76, loss: 0.9647612571716309\n",
      "Epoch 45, batch 22 / 76, loss: 0.9756338000297546\n",
      "Epoch 45, batch 23 / 76, loss: 0.8959664106369019\n",
      "Epoch 45, batch 24 / 76, loss: 0.910179853439331\n",
      "Epoch 45, batch 25 / 76, loss: 0.9555054306983948\n",
      "Epoch 45, batch 26 / 76, loss: 0.9681507349014282\n",
      "Epoch 45, batch 27 / 76, loss: 1.0139975547790527\n",
      "Epoch 45, batch 28 / 76, loss: 1.0119514465332031\n",
      "Epoch 45, batch 29 / 76, loss: 1.002462387084961\n",
      "Epoch 45, batch 30 / 76, loss: 0.9032171964645386\n",
      "Epoch 45, batch 31 / 76, loss: 0.9964960217475891\n",
      "Epoch 45, batch 32 / 76, loss: 1.0216715335845947\n",
      "Epoch 45, batch 33 / 76, loss: 0.9407286643981934\n",
      "Epoch 45, batch 34 / 76, loss: 1.0431047677993774\n",
      "Epoch 45, batch 35 / 76, loss: 0.9613813757896423\n",
      "Epoch 45, batch 36 / 76, loss: 0.9759918451309204\n",
      "Epoch 45, batch 37 / 76, loss: 0.9063695669174194\n",
      "Epoch 45, batch 38 / 76, loss: 0.9669017791748047\n",
      "Epoch 45, batch 39 / 76, loss: 0.884724497795105\n",
      "Epoch 45, batch 40 / 76, loss: 0.9595910906791687\n",
      "Epoch 45, batch 41 / 76, loss: 1.0009273290634155\n",
      "Epoch 45, batch 42 / 76, loss: 0.9634963870048523\n",
      "Epoch 45, batch 43 / 76, loss: 0.994112491607666\n",
      "Epoch 45, batch 44 / 76, loss: 0.9729491472244263\n",
      "Epoch 45, batch 45 / 76, loss: 0.9985442161560059\n",
      "Epoch 45, batch 46 / 76, loss: 0.9305189251899719\n",
      "Epoch 45, batch 47 / 76, loss: 0.9939749836921692\n",
      "Epoch 45, batch 48 / 76, loss: 0.9779525995254517\n",
      "Epoch 45, batch 49 / 76, loss: 0.9876230955123901\n",
      "Epoch 45, batch 50 / 76, loss: 0.9767274260520935\n",
      "Epoch 45, batch 51 / 76, loss: 0.9800338745117188\n",
      "Epoch 45, batch 52 / 76, loss: 0.9459095001220703\n",
      "Epoch 45, batch 53 / 76, loss: 0.9565993547439575\n",
      "Epoch 45, batch 54 / 76, loss: 0.9518336653709412\n",
      "Epoch 45, batch 55 / 76, loss: 0.9899826645851135\n",
      "Epoch 45, batch 56 / 76, loss: 1.0191705226898193\n",
      "Epoch 45, batch 57 / 76, loss: 0.9973515272140503\n",
      "Epoch 45, batch 58 / 76, loss: 0.9790819883346558\n",
      "Epoch 45, batch 59 / 76, loss: 0.9738301038742065\n",
      "Epoch 45, batch 60 / 76, loss: 0.898684024810791\n",
      "Epoch 45, batch 61 / 76, loss: 1.0206809043884277\n",
      "Epoch 45, batch 62 / 76, loss: 0.9484158754348755\n",
      "Epoch 45, batch 63 / 76, loss: 0.9792163372039795\n",
      "Epoch 45, batch 64 / 76, loss: 0.9107328057289124\n",
      "Epoch 45, batch 65 / 76, loss: 0.9251775741577148\n",
      "Epoch 45, batch 66 / 76, loss: 0.9062033295631409\n",
      "Epoch 45, batch 67 / 76, loss: 0.8893564939498901\n",
      "Epoch 45, batch 68 / 76, loss: 1.0374735593795776\n",
      "Epoch 45, batch 69 / 76, loss: 0.9535452127456665\n",
      "Epoch 45, batch 70 / 76, loss: 0.9216374158859253\n",
      "Epoch 45, batch 71 / 76, loss: 0.9033440351486206\n",
      "Epoch 45, batch 72 / 76, loss: 0.9846252202987671\n",
      "Epoch 45, batch 73 / 76, loss: 0.9304541349411011\n",
      "Epoch 45, batch 74 / 76, loss: 0.9863041043281555\n",
      "Epoch 45, batch 75 / 76, loss: 0.9455125331878662\n",
      "Epoch 45, batch 76 / 76, loss: 0.9665632247924805\n",
      "Epoch 45, batch 77 / 76, loss: 0.9125493168830872\n",
      "Epoch 46, batch 1 / 76, loss: 0.9438813924789429\n",
      "Epoch 46, batch 2 / 76, loss: 0.9453519582748413\n",
      "Epoch 46, batch 3 / 76, loss: 0.9008541107177734\n",
      "Epoch 46, batch 4 / 76, loss: 0.9659875631332397\n",
      "Epoch 46, batch 5 / 76, loss: 0.9350655674934387\n",
      "Epoch 46, batch 6 / 76, loss: 0.9314415454864502\n",
      "Epoch 46, batch 7 / 76, loss: 0.9073270559310913\n",
      "Epoch 46, batch 8 / 76, loss: 0.9869999885559082\n",
      "Epoch 46, batch 9 / 76, loss: 0.9780669212341309\n",
      "Epoch 46, batch 10 / 76, loss: 0.9428873062133789\n",
      "Epoch 46, batch 11 / 76, loss: 0.9319065809249878\n",
      "Epoch 46, batch 12 / 76, loss: 0.9876832962036133\n",
      "Epoch 46, batch 13 / 76, loss: 0.9543731212615967\n",
      "Epoch 46, batch 14 / 76, loss: 0.9333661794662476\n",
      "Epoch 46, batch 15 / 76, loss: 0.9136638045310974\n",
      "Epoch 46, batch 16 / 76, loss: 0.9745142459869385\n",
      "Epoch 46, batch 17 / 76, loss: 0.9395538568496704\n",
      "Epoch 46, batch 18 / 76, loss: 1.0067834854125977\n",
      "Epoch 46, batch 19 / 76, loss: 0.9635769724845886\n",
      "Epoch 46, batch 20 / 76, loss: 0.9247002601623535\n",
      "Epoch 46, batch 21 / 76, loss: 0.9183092713356018\n",
      "Epoch 46, batch 22 / 76, loss: 1.037192940711975\n",
      "Epoch 46, batch 23 / 76, loss: 0.9332975149154663\n",
      "Epoch 46, batch 24 / 76, loss: 0.9438076615333557\n",
      "Epoch 46, batch 25 / 76, loss: 0.9715151786804199\n",
      "Epoch 46, batch 26 / 76, loss: 0.9456811547279358\n",
      "Epoch 46, batch 27 / 76, loss: 0.9367132186889648\n",
      "Epoch 46, batch 28 / 76, loss: 0.9936061501502991\n",
      "Epoch 46, batch 29 / 76, loss: 0.983162522315979\n",
      "Epoch 46, batch 30 / 76, loss: 0.9725196361541748\n",
      "Epoch 46, batch 31 / 76, loss: 0.9426711201667786\n",
      "Epoch 46, batch 32 / 76, loss: 0.9914722442626953\n",
      "Epoch 46, batch 33 / 76, loss: 0.9396916627883911\n",
      "Epoch 46, batch 34 / 76, loss: 0.9682974219322205\n",
      "Epoch 46, batch 35 / 76, loss: 0.9372373819351196\n",
      "Epoch 46, batch 36 / 76, loss: 0.9661593437194824\n",
      "Epoch 46, batch 37 / 76, loss: 0.9687584042549133\n",
      "Epoch 46, batch 38 / 76, loss: 0.8772706985473633\n",
      "Epoch 46, batch 39 / 76, loss: 0.9117989540100098\n",
      "Epoch 46, batch 40 / 76, loss: 0.9993993043899536\n",
      "Epoch 46, batch 41 / 76, loss: 0.9366074204444885\n",
      "Epoch 46, batch 42 / 76, loss: 0.9317100048065186\n",
      "Epoch 46, batch 43 / 76, loss: 0.957086443901062\n",
      "Epoch 46, batch 44 / 76, loss: 0.9256289005279541\n",
      "Epoch 46, batch 45 / 76, loss: 0.9768618941307068\n",
      "Epoch 46, batch 46 / 76, loss: 0.8909011483192444\n",
      "Epoch 46, batch 47 / 76, loss: 0.938069224357605\n",
      "Epoch 46, batch 48 / 76, loss: 0.9702258706092834\n",
      "Epoch 46, batch 49 / 76, loss: 0.9321430921554565\n",
      "Epoch 46, batch 50 / 76, loss: 0.9342917203903198\n",
      "Epoch 46, batch 51 / 76, loss: 0.9959360361099243\n",
      "Epoch 46, batch 52 / 76, loss: 0.9421855211257935\n",
      "Epoch 46, batch 53 / 76, loss: 0.9309135675430298\n",
      "Epoch 46, batch 54 / 76, loss: 0.9553941488265991\n",
      "Epoch 46, batch 55 / 76, loss: 0.9083846807479858\n",
      "Epoch 46, batch 56 / 76, loss: 0.9572103023529053\n",
      "Epoch 46, batch 57 / 76, loss: 0.9572875499725342\n",
      "Epoch 46, batch 58 / 76, loss: 1.0170828104019165\n",
      "Epoch 46, batch 59 / 76, loss: 1.0849883556365967\n",
      "Epoch 46, batch 60 / 76, loss: 0.9530997276306152\n",
      "Epoch 46, batch 61 / 76, loss: 0.9444530010223389\n",
      "Epoch 46, batch 62 / 76, loss: 0.9791429042816162\n",
      "Epoch 46, batch 63 / 76, loss: 0.9555754661560059\n",
      "Epoch 46, batch 64 / 76, loss: 0.9434076547622681\n",
      "Epoch 46, batch 65 / 76, loss: 0.9489117860794067\n",
      "Epoch 46, batch 66 / 76, loss: 0.9126312136650085\n",
      "Epoch 46, batch 67 / 76, loss: 0.9314121603965759\n",
      "Epoch 46, batch 68 / 76, loss: 0.968114972114563\n",
      "Epoch 46, batch 69 / 76, loss: 0.9052479267120361\n",
      "Epoch 46, batch 70 / 76, loss: 0.9205739498138428\n",
      "Epoch 46, batch 71 / 76, loss: 0.9600493907928467\n",
      "Epoch 46, batch 72 / 76, loss: 1.0009377002716064\n",
      "Epoch 46, batch 73 / 76, loss: 0.9069488048553467\n",
      "Epoch 46, batch 74 / 76, loss: 0.9529204368591309\n",
      "Epoch 46, batch 75 / 76, loss: 0.946857750415802\n",
      "Epoch 46, batch 76 / 76, loss: 0.9336637854576111\n",
      "Epoch 46, batch 77 / 76, loss: 0.9683564901351929\n",
      "Epoch 47, batch 1 / 76, loss: 0.9996757507324219\n",
      "Epoch 47, batch 2 / 76, loss: 0.991478681564331\n",
      "Epoch 47, batch 3 / 76, loss: 0.9613097906112671\n",
      "Epoch 47, batch 4 / 76, loss: 0.9776463508605957\n",
      "Epoch 47, batch 5 / 76, loss: 0.9158014059066772\n",
      "Epoch 47, batch 6 / 76, loss: 1.0191755294799805\n",
      "Epoch 47, batch 7 / 76, loss: 0.9718286991119385\n",
      "Epoch 47, batch 8 / 76, loss: 0.9139499068260193\n",
      "Epoch 47, batch 9 / 76, loss: 0.9616440534591675\n",
      "Epoch 47, batch 10 / 76, loss: 0.9608634114265442\n",
      "Epoch 47, batch 11 / 76, loss: 0.9711505770683289\n",
      "Epoch 47, batch 12 / 76, loss: 0.8747196197509766\n",
      "Epoch 47, batch 13 / 76, loss: 0.9756475687026978\n",
      "Epoch 47, batch 14 / 76, loss: 0.9242780804634094\n",
      "Epoch 47, batch 15 / 76, loss: 0.9665540456771851\n",
      "Epoch 47, batch 16 / 76, loss: 0.906996488571167\n",
      "Epoch 47, batch 17 / 76, loss: 1.004737138748169\n",
      "Epoch 47, batch 18 / 76, loss: 0.9818679094314575\n",
      "Epoch 47, batch 19 / 76, loss: 0.9266054034233093\n",
      "Epoch 47, batch 20 / 76, loss: 0.8976985216140747\n",
      "Epoch 47, batch 21 / 76, loss: 0.9550219774246216\n",
      "Epoch 47, batch 22 / 76, loss: 0.8897627592086792\n",
      "Epoch 47, batch 23 / 76, loss: 1.0050567388534546\n",
      "Epoch 47, batch 24 / 76, loss: 0.9486063718795776\n",
      "Epoch 47, batch 25 / 76, loss: 0.8954569101333618\n",
      "Epoch 47, batch 26 / 76, loss: 0.9633220434188843\n",
      "Epoch 47, batch 27 / 76, loss: 0.9420366883277893\n",
      "Epoch 47, batch 28 / 76, loss: 0.9552720189094543\n",
      "Epoch 47, batch 29 / 76, loss: 0.9296413660049438\n",
      "Epoch 47, batch 30 / 76, loss: 0.9613925218582153\n",
      "Epoch 47, batch 31 / 76, loss: 0.908674955368042\n",
      "Epoch 47, batch 32 / 76, loss: 0.9470700025558472\n",
      "Epoch 47, batch 33 / 76, loss: 0.8785014152526855\n",
      "Epoch 47, batch 34 / 76, loss: 0.9492120742797852\n",
      "Epoch 47, batch 35 / 76, loss: 0.946731686592102\n",
      "Epoch 47, batch 36 / 76, loss: 0.9369293451309204\n",
      "Epoch 47, batch 37 / 76, loss: 0.9687761068344116\n",
      "Epoch 47, batch 38 / 76, loss: 0.95281982421875\n",
      "Epoch 47, batch 39 / 76, loss: 0.8958594799041748\n",
      "Epoch 47, batch 40 / 76, loss: 1.0096096992492676\n",
      "Epoch 47, batch 41 / 76, loss: 0.9305068850517273\n",
      "Epoch 47, batch 42 / 76, loss: 0.9779731035232544\n",
      "Epoch 47, batch 43 / 76, loss: 1.0023750066757202\n",
      "Epoch 47, batch 44 / 76, loss: 0.9387775659561157\n",
      "Epoch 47, batch 45 / 76, loss: 0.9867449998855591\n",
      "Epoch 47, batch 46 / 76, loss: 0.9243361353874207\n",
      "Epoch 47, batch 47 / 76, loss: 0.9059054851531982\n",
      "Epoch 47, batch 48 / 76, loss: 0.9676471948623657\n",
      "Epoch 47, batch 49 / 76, loss: 0.9525642395019531\n",
      "Epoch 47, batch 50 / 76, loss: 1.0191020965576172\n",
      "Epoch 47, batch 51 / 76, loss: 0.9514230489730835\n",
      "Epoch 47, batch 52 / 76, loss: 0.9755469560623169\n",
      "Epoch 47, batch 53 / 76, loss: 0.914350688457489\n",
      "Epoch 47, batch 54 / 76, loss: 0.9577860832214355\n",
      "Epoch 47, batch 55 / 76, loss: 0.9422531127929688\n",
      "Epoch 47, batch 56 / 76, loss: 0.9969069957733154\n",
      "Epoch 47, batch 57 / 76, loss: 1.021222472190857\n",
      "Epoch 47, batch 58 / 76, loss: 0.9263451099395752\n",
      "Epoch 47, batch 59 / 76, loss: 0.9728810787200928\n",
      "Epoch 47, batch 60 / 76, loss: 0.9253429174423218\n",
      "Epoch 47, batch 61 / 76, loss: 0.8922110795974731\n",
      "Epoch 47, batch 62 / 76, loss: 0.951923131942749\n",
      "Epoch 47, batch 63 / 76, loss: 0.9674798250198364\n",
      "Epoch 47, batch 64 / 76, loss: 0.9540978670120239\n",
      "Epoch 47, batch 65 / 76, loss: 0.9443170428276062\n",
      "Epoch 47, batch 66 / 76, loss: 0.9902032613754272\n",
      "Epoch 47, batch 67 / 76, loss: 0.9392736554145813\n",
      "Epoch 47, batch 68 / 76, loss: 0.9651507139205933\n",
      "Epoch 47, batch 69 / 76, loss: 0.9391672611236572\n",
      "Epoch 47, batch 70 / 76, loss: 0.9872348308563232\n",
      "Epoch 47, batch 71 / 76, loss: 0.9782244563102722\n",
      "Epoch 47, batch 72 / 76, loss: 0.8839436173439026\n",
      "Epoch 47, batch 73 / 76, loss: 0.9812711477279663\n",
      "Epoch 47, batch 74 / 76, loss: 0.9145867228507996\n",
      "Epoch 47, batch 75 / 76, loss: 0.9623820781707764\n",
      "Epoch 47, batch 76 / 76, loss: 0.9474449157714844\n",
      "Epoch 47, batch 77 / 76, loss: 0.8798036575317383\n",
      "Epoch 48, batch 1 / 76, loss: 0.9511300921440125\n",
      "Epoch 48, batch 2 / 76, loss: 1.0211619138717651\n",
      "Epoch 48, batch 3 / 76, loss: 0.9140549898147583\n",
      "Epoch 48, batch 4 / 76, loss: 0.9194176197052002\n",
      "Epoch 48, batch 5 / 76, loss: 0.9618589878082275\n",
      "Epoch 48, batch 6 / 76, loss: 0.9329580068588257\n",
      "Epoch 48, batch 7 / 76, loss: 0.8495968580245972\n",
      "Epoch 48, batch 8 / 76, loss: 0.9708073139190674\n",
      "Epoch 48, batch 9 / 76, loss: 0.9261897206306458\n",
      "Epoch 48, batch 10 / 76, loss: 0.8704162836074829\n",
      "Epoch 48, batch 11 / 76, loss: 1.0168582201004028\n",
      "Epoch 48, batch 12 / 76, loss: 0.9694890975952148\n",
      "Epoch 48, batch 13 / 76, loss: 0.963824987411499\n",
      "Epoch 48, batch 14 / 76, loss: 0.941206693649292\n",
      "Epoch 48, batch 15 / 76, loss: 0.9645172357559204\n",
      "Epoch 48, batch 16 / 76, loss: 1.0099477767944336\n",
      "Epoch 48, batch 17 / 76, loss: 0.9745409488677979\n",
      "Epoch 48, batch 18 / 76, loss: 0.9142521619796753\n",
      "Epoch 48, batch 19 / 76, loss: 0.9832228422164917\n",
      "Epoch 48, batch 20 / 76, loss: 0.9217863082885742\n",
      "Epoch 48, batch 21 / 76, loss: 0.9981864094734192\n",
      "Epoch 48, batch 22 / 76, loss: 0.8786097168922424\n",
      "Epoch 48, batch 23 / 76, loss: 0.9658644795417786\n",
      "Epoch 48, batch 24 / 76, loss: 0.9264876842498779\n",
      "Epoch 48, batch 25 / 76, loss: 1.0060057640075684\n",
      "Epoch 48, batch 26 / 76, loss: 1.006239414215088\n",
      "Epoch 48, batch 27 / 76, loss: 0.9809067249298096\n",
      "Epoch 48, batch 28 / 76, loss: 0.9566097259521484\n",
      "Epoch 48, batch 29 / 76, loss: 0.9735382795333862\n",
      "Epoch 48, batch 30 / 76, loss: 1.022986650466919\n",
      "Epoch 48, batch 31 / 76, loss: 0.8916211724281311\n",
      "Epoch 48, batch 32 / 76, loss: 0.9751126766204834\n",
      "Epoch 48, batch 33 / 76, loss: 0.9871374368667603\n",
      "Epoch 48, batch 34 / 76, loss: 0.9726225137710571\n",
      "Epoch 48, batch 35 / 76, loss: 1.003481388092041\n",
      "Epoch 48, batch 36 / 76, loss: 0.9701372981071472\n",
      "Epoch 48, batch 37 / 76, loss: 0.9870965480804443\n",
      "Epoch 48, batch 38 / 76, loss: 0.9440421462059021\n",
      "Epoch 48, batch 39 / 76, loss: 0.9266785383224487\n",
      "Epoch 48, batch 40 / 76, loss: 1.0581127405166626\n",
      "Epoch 48, batch 41 / 76, loss: 1.0030455589294434\n",
      "Epoch 48, batch 42 / 76, loss: 0.981677234172821\n",
      "Epoch 48, batch 43 / 76, loss: 0.8781546354293823\n",
      "Epoch 48, batch 44 / 76, loss: 1.0045151710510254\n",
      "Epoch 48, batch 45 / 76, loss: 0.970977783203125\n",
      "Epoch 48, batch 46 / 76, loss: 0.9950578212738037\n",
      "Epoch 48, batch 47 / 76, loss: 0.9894206523895264\n",
      "Epoch 48, batch 48 / 76, loss: 0.9104892611503601\n",
      "Epoch 48, batch 49 / 76, loss: 1.0268393754959106\n",
      "Epoch 48, batch 50 / 76, loss: 0.979775607585907\n",
      "Epoch 48, batch 51 / 76, loss: 1.008998990058899\n",
      "Epoch 48, batch 52 / 76, loss: 1.0122220516204834\n",
      "Epoch 48, batch 53 / 76, loss: 0.9996111392974854\n",
      "Epoch 48, batch 54 / 76, loss: 1.0002079010009766\n",
      "Epoch 48, batch 55 / 76, loss: 0.9612016677856445\n",
      "Epoch 48, batch 56 / 76, loss: 1.0408488512039185\n",
      "Epoch 48, batch 57 / 76, loss: 1.0142128467559814\n",
      "Epoch 48, batch 58 / 76, loss: 0.9421687722206116\n",
      "Epoch 48, batch 59 / 76, loss: 0.9716686010360718\n",
      "Epoch 48, batch 60 / 76, loss: 0.9384361505508423\n",
      "Epoch 48, batch 61 / 76, loss: 0.979291558265686\n",
      "Epoch 48, batch 62 / 76, loss: 0.9720867872238159\n",
      "Epoch 48, batch 63 / 76, loss: 1.0496065616607666\n",
      "Epoch 48, batch 64 / 76, loss: 0.984889566898346\n",
      "Epoch 48, batch 65 / 76, loss: 0.9602777361869812\n",
      "Epoch 48, batch 66 / 76, loss: 0.9888284206390381\n",
      "Epoch 48, batch 67 / 76, loss: 0.9811962842941284\n",
      "Epoch 48, batch 68 / 76, loss: 0.9751297235488892\n",
      "Epoch 48, batch 69 / 76, loss: 0.9891315698623657\n",
      "Epoch 48, batch 70 / 76, loss: 0.9854503870010376\n",
      "Epoch 48, batch 71 / 76, loss: 0.9329314231872559\n",
      "Epoch 48, batch 72 / 76, loss: 0.9684749841690063\n",
      "Epoch 48, batch 73 / 76, loss: 0.9508385062217712\n",
      "Epoch 48, batch 74 / 76, loss: 0.9460059404373169\n",
      "Epoch 48, batch 75 / 76, loss: 1.0581175088882446\n",
      "Epoch 48, batch 76 / 76, loss: 0.9448981285095215\n",
      "Epoch 48, batch 77 / 76, loss: 1.0848453044891357\n",
      "Epoch 49, batch 1 / 76, loss: 1.0640836954116821\n",
      "Epoch 49, batch 2 / 76, loss: 0.9954210519790649\n",
      "Epoch 49, batch 3 / 76, loss: 0.9898363351821899\n",
      "Epoch 49, batch 4 / 76, loss: 0.9542893171310425\n",
      "Epoch 49, batch 5 / 76, loss: 1.002556562423706\n",
      "Epoch 49, batch 6 / 76, loss: 1.0175858736038208\n",
      "Epoch 49, batch 7 / 76, loss: 1.0057287216186523\n",
      "Epoch 49, batch 8 / 76, loss: 0.9287896156311035\n",
      "Epoch 49, batch 9 / 76, loss: 0.9229699969291687\n",
      "Epoch 49, batch 10 / 76, loss: 1.010285496711731\n",
      "Epoch 49, batch 11 / 76, loss: 1.0430375337600708\n",
      "Epoch 49, batch 12 / 76, loss: 0.9858705401420593\n",
      "Epoch 49, batch 13 / 76, loss: 0.9618990421295166\n",
      "Epoch 49, batch 14 / 76, loss: 1.0200064182281494\n",
      "Epoch 49, batch 15 / 76, loss: 0.9516555666923523\n",
      "Epoch 49, batch 16 / 76, loss: 0.9277803301811218\n",
      "Epoch 49, batch 17 / 76, loss: 0.9856940507888794\n",
      "Epoch 49, batch 18 / 76, loss: 0.9682692885398865\n",
      "Epoch 49, batch 19 / 76, loss: 1.0072060823440552\n",
      "Epoch 49, batch 20 / 76, loss: 0.9731082916259766\n",
      "Epoch 49, batch 21 / 76, loss: 0.9552825689315796\n",
      "Epoch 49, batch 22 / 76, loss: 1.0393171310424805\n",
      "Epoch 49, batch 23 / 76, loss: 0.9572708606719971\n",
      "Epoch 49, batch 24 / 76, loss: 0.9906671643257141\n",
      "Epoch 49, batch 25 / 76, loss: 0.9657541513442993\n",
      "Epoch 49, batch 26 / 76, loss: 0.9031935930252075\n",
      "Epoch 49, batch 27 / 76, loss: 0.9673982858657837\n",
      "Epoch 49, batch 28 / 76, loss: 0.9749218225479126\n",
      "Epoch 49, batch 29 / 76, loss: 1.0034635066986084\n",
      "Epoch 49, batch 30 / 76, loss: 1.0379246473312378\n",
      "Epoch 49, batch 31 / 76, loss: 0.9656953811645508\n",
      "Epoch 49, batch 32 / 76, loss: 0.9751803278923035\n",
      "Epoch 49, batch 33 / 76, loss: 0.9398024678230286\n",
      "Epoch 49, batch 34 / 76, loss: 0.9239333271980286\n",
      "Epoch 49, batch 35 / 76, loss: 0.928409218788147\n",
      "Epoch 49, batch 36 / 76, loss: 1.0192745923995972\n",
      "Epoch 49, batch 37 / 76, loss: 0.9384240508079529\n",
      "Epoch 49, batch 38 / 76, loss: 0.9907100200653076\n",
      "Epoch 49, batch 39 / 76, loss: 0.9307027459144592\n",
      "Epoch 49, batch 40 / 76, loss: 0.9562232494354248\n",
      "Epoch 49, batch 41 / 76, loss: 0.9548137187957764\n",
      "Epoch 49, batch 42 / 76, loss: 0.9852359890937805\n",
      "Epoch 49, batch 43 / 76, loss: 0.9820274114608765\n",
      "Epoch 49, batch 44 / 76, loss: 1.0532572269439697\n",
      "Epoch 49, batch 45 / 76, loss: 0.9952285885810852\n",
      "Epoch 49, batch 46 / 76, loss: 0.9852957725524902\n",
      "Epoch 49, batch 47 / 76, loss: 0.9949129819869995\n",
      "Epoch 49, batch 48 / 76, loss: 0.962854266166687\n",
      "Epoch 49, batch 49 / 76, loss: 1.0106176137924194\n",
      "Epoch 49, batch 50 / 76, loss: 0.9536574482917786\n",
      "Epoch 49, batch 51 / 76, loss: 0.981978178024292\n",
      "Epoch 49, batch 52 / 76, loss: 0.9944233894348145\n",
      "Epoch 49, batch 53 / 76, loss: 0.9953906536102295\n",
      "Epoch 49, batch 54 / 76, loss: 1.0068546533584595\n",
      "Epoch 49, batch 55 / 76, loss: 1.0017036199569702\n",
      "Epoch 49, batch 56 / 76, loss: 1.005279779434204\n",
      "Epoch 49, batch 57 / 76, loss: 0.9120166301727295\n",
      "Epoch 49, batch 58 / 76, loss: 0.9699001312255859\n",
      "Epoch 49, batch 59 / 76, loss: 0.9801227450370789\n",
      "Epoch 49, batch 60 / 76, loss: 0.9649639129638672\n",
      "Epoch 49, batch 61 / 76, loss: 0.9632144570350647\n",
      "Epoch 49, batch 62 / 76, loss: 0.9578843116760254\n",
      "Epoch 49, batch 63 / 76, loss: 0.9868049621582031\n",
      "Epoch 49, batch 64 / 76, loss: 0.9573194980621338\n",
      "Epoch 49, batch 65 / 76, loss: 0.9985924959182739\n",
      "Epoch 49, batch 66 / 76, loss: 0.9233555793762207\n",
      "Epoch 49, batch 67 / 76, loss: 1.062237024307251\n",
      "Epoch 49, batch 68 / 76, loss: 1.0240987539291382\n",
      "Epoch 49, batch 69 / 76, loss: 1.003504991531372\n",
      "Epoch 49, batch 70 / 76, loss: 0.972133457660675\n",
      "Epoch 49, batch 71 / 76, loss: 0.9583191275596619\n",
      "Epoch 49, batch 72 / 76, loss: 0.9776257872581482\n",
      "Epoch 49, batch 73 / 76, loss: 0.9631859064102173\n",
      "Epoch 49, batch 74 / 76, loss: 1.0387765169143677\n",
      "Epoch 49, batch 75 / 76, loss: 0.9952399730682373\n",
      "Epoch 49, batch 76 / 76, loss: 0.9665619134902954\n",
      "Epoch 49, batch 77 / 76, loss: 1.1149840354919434\n",
      "Epoch 50, batch 1 / 76, loss: 0.8842611312866211\n",
      "Epoch 50, batch 2 / 76, loss: 0.9839667081832886\n",
      "Epoch 50, batch 3 / 76, loss: 0.9738964438438416\n",
      "Epoch 50, batch 4 / 76, loss: 1.0468635559082031\n",
      "Epoch 50, batch 5 / 76, loss: 0.9745264649391174\n",
      "Epoch 50, batch 6 / 76, loss: 0.9253261089324951\n",
      "Epoch 50, batch 7 / 76, loss: 1.00795316696167\n",
      "Epoch 50, batch 8 / 76, loss: 1.03773033618927\n",
      "Epoch 50, batch 9 / 76, loss: 1.028853178024292\n",
      "Epoch 50, batch 10 / 76, loss: 0.9803520441055298\n",
      "Epoch 50, batch 11 / 76, loss: 0.980097770690918\n",
      "Epoch 50, batch 12 / 76, loss: 0.9708440899848938\n",
      "Epoch 50, batch 13 / 76, loss: 0.9722851514816284\n",
      "Epoch 50, batch 14 / 76, loss: 0.9659945964813232\n",
      "Epoch 50, batch 15 / 76, loss: 0.9956085681915283\n",
      "Epoch 50, batch 16 / 76, loss: 1.0107483863830566\n",
      "Epoch 50, batch 17 / 76, loss: 0.9643523693084717\n",
      "Epoch 50, batch 18 / 76, loss: 1.0223215818405151\n",
      "Epoch 50, batch 19 / 76, loss: 0.953413724899292\n",
      "Epoch 50, batch 20 / 76, loss: 1.04718017578125\n",
      "Epoch 50, batch 21 / 76, loss: 0.9013732671737671\n",
      "Epoch 50, batch 22 / 76, loss: 0.8991141319274902\n",
      "Epoch 50, batch 23 / 76, loss: 0.9689140915870667\n",
      "Epoch 50, batch 24 / 76, loss: 1.0031261444091797\n",
      "Epoch 50, batch 25 / 76, loss: 1.0170831680297852\n",
      "Epoch 50, batch 26 / 76, loss: 0.9273853302001953\n",
      "Epoch 50, batch 27 / 76, loss: 1.0006422996520996\n",
      "Epoch 50, batch 28 / 76, loss: 0.953568696975708\n",
      "Epoch 50, batch 29 / 76, loss: 0.9922170639038086\n",
      "Epoch 50, batch 30 / 76, loss: 1.027119755744934\n",
      "Epoch 50, batch 31 / 76, loss: 0.9792101383209229\n",
      "Epoch 50, batch 32 / 76, loss: 0.9013122320175171\n",
      "Epoch 50, batch 33 / 76, loss: 1.0117461681365967\n",
      "Epoch 50, batch 34 / 76, loss: 0.9752269983291626\n",
      "Epoch 50, batch 35 / 76, loss: 0.9577112197875977\n",
      "Epoch 50, batch 36 / 76, loss: 1.0102365016937256\n",
      "Epoch 50, batch 37 / 76, loss: 1.0137608051300049\n",
      "Epoch 50, batch 38 / 76, loss: 0.9826052188873291\n",
      "Epoch 50, batch 39 / 76, loss: 0.9759150743484497\n",
      "Epoch 50, batch 40 / 76, loss: 1.0475609302520752\n",
      "Epoch 50, batch 41 / 76, loss: 1.04142165184021\n",
      "Epoch 50, batch 42 / 76, loss: 0.9671529531478882\n",
      "Epoch 50, batch 43 / 76, loss: 1.0175501108169556\n",
      "Epoch 50, batch 44 / 76, loss: 0.928286075592041\n",
      "Epoch 50, batch 45 / 76, loss: 0.9935897588729858\n",
      "Epoch 50, batch 46 / 76, loss: 0.9303277134895325\n",
      "Epoch 50, batch 47 / 76, loss: 0.9813048839569092\n",
      "Epoch 50, batch 48 / 76, loss: 1.0557632446289062\n",
      "Epoch 50, batch 49 / 76, loss: 0.9763965606689453\n",
      "Epoch 50, batch 50 / 76, loss: 0.9512847065925598\n",
      "Epoch 50, batch 51 / 76, loss: 0.9725334644317627\n",
      "Epoch 50, batch 52 / 76, loss: 0.9015277624130249\n",
      "Epoch 50, batch 53 / 76, loss: 0.9977476596832275\n",
      "Epoch 50, batch 54 / 76, loss: 1.0035802125930786\n",
      "Epoch 50, batch 55 / 76, loss: 0.9339261054992676\n",
      "Epoch 50, batch 56 / 76, loss: 0.8902789354324341\n",
      "Epoch 50, batch 57 / 76, loss: 1.041546106338501\n",
      "Epoch 50, batch 58 / 76, loss: 0.9590474367141724\n",
      "Epoch 50, batch 59 / 76, loss: 0.9786756634712219\n",
      "Epoch 50, batch 60 / 76, loss: 0.9847548007965088\n",
      "Epoch 50, batch 61 / 76, loss: 1.0357575416564941\n",
      "Epoch 50, batch 62 / 76, loss: 0.9632117748260498\n",
      "Epoch 50, batch 63 / 76, loss: 0.9215508699417114\n",
      "Epoch 50, batch 64 / 76, loss: 0.997125506401062\n",
      "Epoch 50, batch 65 / 76, loss: 1.0116312503814697\n",
      "Epoch 50, batch 66 / 76, loss: 0.9362261295318604\n",
      "Epoch 50, batch 67 / 76, loss: 0.9533848762512207\n",
      "Epoch 50, batch 68 / 76, loss: 0.9678341150283813\n",
      "Epoch 50, batch 69 / 76, loss: 0.9782775044441223\n",
      "Epoch 50, batch 70 / 76, loss: 0.9952259659767151\n",
      "Epoch 50, batch 71 / 76, loss: 0.9418022632598877\n",
      "Epoch 50, batch 72 / 76, loss: 0.9660477638244629\n",
      "Epoch 50, batch 73 / 76, loss: 1.0010603666305542\n",
      "Epoch 50, batch 74 / 76, loss: 0.9813622236251831\n",
      "Epoch 50, batch 75 / 76, loss: 0.986335039138794\n",
      "Epoch 50, batch 76 / 76, loss: 0.9493893384933472\n",
      "Epoch 50, batch 77 / 76, loss: 1.05193030834198\n"
     ]
    }
   ],
   "source": [
    "loss_history = parent.train_model(lstm_model, cnn_model, X_sequence_train, X_expressions_train, y_train)\n",
    "lstm_model.save(f'../Models/{name}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 12 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "lstm_model = parent.load_model(f'../Models/{name}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Mean Squared Error on Test Data: 0.0193\n"
     ]
    }
   ],
   "source": [
    "mse, predicted_expression = parent.evaluate_model(lstm_model, cnn_model, X_sequence_test, X_expressions_test)\n",
    "print(f'Mean Squared Error on Test Data: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "TTTTCTATCTACGTACTTGACACTATTTC______________ATT__________ACCTTAGTTTGTACGTT\n",
      "TTTTTTCTACGTACTTTACACTATTTTTTTTTTTAAAAAAAATTTTTTTTTAAAACCTTAGTTTTTTTGTT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sequence = 'TTTTCTATCTACGTACTTGACACTATTTC______________ATT__________ACCTTAGTTTGTACGTT'\n",
    "generated_sequence = parent.predict_with_lstm(lstm_model, sequence, 0.5, scaler, 150)\n",
    "generated_sequence_onehot = parent.predict_with_lstm(lstm_model, sequence, 0.5, scaler, 150, decode_output=False)\n",
    "\n",
    "print(sequence)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.14118679 0.14755061 0.16863506 0.15042593 0.3922016 ]\n",
      "  [0.09740667 0.10475921 0.13353467 0.10917311 0.55512637]\n",
      "  [0.06488813 0.07130995 0.10151958 0.07686499 0.6854173 ]\n",
      "  [0.04375513 0.04900131 0.07730516 0.05474461 0.77519375]\n",
      "  [0.03077868 0.03508341 0.06060063 0.04050682 0.8330304 ]\n",
      "  [0.02287718 0.02651631 0.04949296 0.0314834  0.8696302 ]\n",
      "  [0.01799966 0.02118978 0.04220132 0.0257338  0.8928755 ]\n",
      "  [0.01493306 0.01782905 0.03746092 0.02203655 0.9077404 ]\n",
      "  [0.01298186 0.0156928  0.03444713 0.01965541 0.91722274]\n",
      "  [0.01174474 0.01434847 0.0326338  0.01814708 0.92312586]\n",
      "  [0.01098476 0.01353814 0.03168284 0.01723984 0.92655444]\n",
      "  [0.01055779 0.01310349 0.03137256 0.01676266 0.9282035 ]\n",
      "  [0.01037312 0.01294399 0.03155313 0.01660436 0.9285253 ]\n",
      "  [0.01037162 0.01299348 0.03211972 0.01669018 0.92782515]\n",
      "  [0.0105131  0.01320667 0.03299578 0.01696782 0.9263167 ]\n",
      "  [0.01076913 0.01355128 0.03412297 0.0173992  0.9241574 ]\n",
      "  [0.01111857 0.01400313 0.03545472 0.01795535 0.9214682 ]\n",
      "  [0.01154493 0.01454334 0.0369524  0.0186133  0.9183459 ]\n",
      "  [0.01203476 0.01515645 0.03858274 0.01935408 0.91487193]\n",
      "  [0.01257659 0.01582928 0.04031646 0.02016146 0.91111624]\n",
      "  [0.01316037 0.01655032 0.04212746 0.02102137 0.90714043]\n",
      "  [0.01377705 0.01730918 0.04399208 0.02192116 0.9030006 ]\n",
      "  [0.01441845 0.01809652 0.04588918 0.02284959 0.89874625]\n",
      "  [0.01507707 0.01890371 0.04779967 0.02379646 0.89442307]\n",
      "  [0.01574611 0.01972293 0.04970666 0.02475267 0.8900718 ]\n",
      "  [0.01641936 0.02054707 0.05159532 0.02571006 0.88572824]\n",
      "  [0.01709132 0.02136973 0.0534528  0.02666146 0.8814246 ]\n",
      "  [0.01775712 0.02218525 0.05526841 0.02760061 0.8771887 ]\n",
      "  [0.01841246 0.0229886  0.05703295 0.02852208 0.87304395]\n",
      "  [0.01905372 0.02377549 0.05873929 0.02942134 0.86901015]\n",
      "  [0.01967786 0.02454228 0.06038174 0.03029462 0.86510354]\n",
      "  [0.02028238 0.02528596 0.06195618 0.03113893 0.8613366 ]\n",
      "  [0.0208653  0.02600412 0.06345965 0.03195187 0.8577191 ]\n",
      "  [0.02142514 0.02669485 0.06489042 0.03273174 0.8542579 ]\n",
      "  [0.02196083 0.02735684 0.06624781 0.03347737 0.85095716]\n",
      "  [0.0224717  0.02798914 0.06753191 0.03418801 0.8478192 ]\n",
      "  [0.02295741 0.02859124 0.06874354 0.03486341 0.8448444 ]\n",
      "  [0.02341791 0.02916298 0.06988414 0.03550367 0.8420313 ]\n",
      "  [0.02385337 0.02970447 0.07095554 0.03610912 0.83937746]\n",
      "  [0.02426422 0.03021611 0.07196    0.03668042 0.83687925]\n",
      "  [0.024651   0.03069849 0.07290008 0.03721844 0.834532  ]\n",
      "  [0.02501441 0.03115233 0.07377843 0.03772412 0.83233064]\n",
      "  [0.02535526 0.0315786  0.07459796 0.03819865 0.8302695 ]\n",
      "  [0.0256744  0.03197823 0.07536156 0.03864322 0.8283426 ]\n",
      "  [0.02597281 0.03235236 0.07607224 0.03905915 0.8265434 ]\n",
      "  [0.02625141 0.03270205 0.07673288 0.03944772 0.8248659 ]\n",
      "  [0.0265112  0.03302851 0.07734638 0.03981031 0.8233036 ]\n",
      "  [0.02675316 0.03333288 0.07791562 0.04014826 0.82185006]\n",
      "  [0.0269783  0.03361636 0.07844334 0.04046294 0.8204991 ]\n",
      "  [0.02718755 0.03388009 0.07893217 0.04075562 0.8192446 ]\n",
      "  [0.02738188 0.03412522 0.07938468 0.04102764 0.81808066]\n",
      "  [0.02756219 0.03435287 0.0798033  0.04128023 0.81700146]\n",
      "  [0.02772936 0.03456409 0.08019032 0.04151458 0.81600165]\n",
      "  [0.02788425 0.03475993 0.08054795 0.04173186 0.81507605]\n",
      "  [0.02802765 0.03494139 0.08087821 0.04193316 0.81421953]\n",
      "  [0.02816036 0.03510942 0.08118318 0.04211961 0.8134275 ]\n",
      "  [0.02828308 0.03526489 0.08146455 0.04229213 0.8126953 ]\n",
      "  [0.02839652 0.0354087  0.08172411 0.04245171 0.8120189 ]\n",
      "  [0.02850133 0.03554164 0.08196345 0.04259925 0.8113944 ]\n",
      "  [0.02859813 0.03566447 0.08218411 0.04273557 0.8108178 ]\n",
      "  [0.02868748 0.03577789 0.08238741 0.04286148 0.81028575]\n",
      "  [0.02876993 0.03588264 0.08257475 0.04297776 0.8097949 ]\n",
      "  [0.02884599 0.03597928 0.08274727 0.04308507 0.8093423 ]\n",
      "  [0.02891613 0.03606844 0.08290614 0.04318407 0.8089252 ]\n",
      "  [0.02898078 0.03615066 0.08305241 0.04327539 0.8085408 ]\n",
      "  [0.02904037 0.03622646 0.08318704 0.04335959 0.8081866 ]\n",
      "  [0.02909526 0.03629632 0.08331089 0.04343719 0.8078604 ]\n",
      "  [0.02914583 0.0363607  0.08342491 0.04350872 0.80755997]\n",
      "  [0.0291924  0.03642001 0.0835298  0.04357462 0.80728316]\n",
      "  [0.02923528 0.03647463 0.08362626 0.04363531 0.80702853]\n",
      "  [0.02927476 0.03652493 0.08371498 0.04369122 0.8067941 ]\n",
      "  [0.0293111  0.03657123 0.08379658 0.04374271 0.80657846]\n",
      "  [0.02934454 0.03661386 0.08387163 0.0437901  0.8063799 ]\n",
      "  [0.0293753  0.03665308 0.08394057 0.0438337  0.8061973 ]\n",
      "  [0.0294036  0.03668918 0.08400399 0.04387384 0.8060294 ]\n",
      "  [0.02942964 0.03672239 0.0840623  0.04391077 0.80587494]\n",
      "  [0.02945358 0.03675294 0.08411589 0.04394476 0.8057328 ]\n",
      "  [0.04611101 0.12286469 0.12957501 0.07681303 0.62463623]\n",
      "  [0.06084691 0.28423616 0.1642415  0.10388841 0.38678706]\n",
      "  [0.06596053 0.46209413 0.16770664 0.11057388 0.19366483]\n",
      "  [0.06368775 0.5929169  0.15146303 0.10253143 0.08940088]\n",
      "  [0.08975909 0.3995509  0.33277187 0.12052187 0.05739633]\n",
      "  [0.0787626  0.5640167  0.22322237 0.1058602  0.02813816]\n",
      "  [0.24762595 0.35143015 0.25356248 0.12754196 0.0198395 ]\n",
      "  [0.14046894 0.54354626 0.192183   0.11254239 0.01125934]\n",
      "  [0.15226486 0.34786728 0.37151697 0.11954408 0.00880676]\n",
      "  [0.11531696 0.5416417  0.23140317 0.10566265 0.00597547]\n",
      "  [0.32121292 0.31382576 0.2400117  0.11981007 0.00513954]\n",
      "  [0.22092465 0.25023004 0.40386492 0.12076367 0.00421668]\n",
      "  [0.20888458 0.21943077 0.2742125  0.29381698 0.0036552 ]\n",
      "  [0.15508339 0.4359025  0.20948197 0.19634166 0.00319046]\n",
      "  [0.37727597 0.24666746 0.20182474 0.17140561 0.00282624]\n",
      "  [0.25428182 0.22052756 0.36228675 0.16018507 0.00271885]\n",
      "  [0.17748767 0.44045877 0.23714308 0.14232357 0.00258693]\n",
      "  [0.12801185 0.57481194 0.17554021 0.11928657 0.00234941]\n",
      "  [0.15379281 0.36764666 0.1722602  0.3037579  0.00254239]\n",
      "  [0.3753777  0.24325857 0.16930917 0.20974243 0.00231209]\n",
      "  [0.25987047 0.22426094 0.3250919  0.18838012 0.00239659]\n",
      "  [0.47399232 0.15409051 0.22039708 0.14953394 0.00198616]\n",
      "  [0.29873493 0.16216943 0.38674304 0.15019459 0.00215807]\n",
      "  [0.20987798 0.37930778 0.26215276 0.14631894 0.00234249]\n",
      "  [0.43664223 0.20809363 0.21992251 0.13321508 0.0021266 ]\n",
      "  [0.23041834 0.44760346 0.18529208 0.13442037 0.00226567]\n",
      "  [0.15367132 0.58256125 0.14587586 0.11574062 0.00215094]\n",
      "  [0.11216415 0.6683252  0.11848827 0.09900546 0.00201686]\n",
      "  [0.14874393 0.44412678 0.28459504 0.11996148 0.00257281]\n",
      "  [0.19072556 0.39273334 0.26067567 0.15290731 0.00295808]\n",
      "  [0.21451207 0.3493134  0.25612527 0.17675531 0.00329391]\n",
      "  [0.23161712 0.3193132  0.25045505 0.19502066 0.00359399]\n",
      "  [0.24346425 0.2985805  0.24523593 0.20884742 0.00387186]\n",
      "  [0.25161746 0.28419033 0.24085839 0.21920048 0.00413335]\n",
      "  [0.25720006 0.27413362 0.23736127 0.22692476 0.00438034]\n",
      "  [0.26100084 0.26704848 0.2346502  0.23268759 0.00461285]\n",
      "  [0.2635667  0.26201352 0.2325955  0.23699419 0.00483008]\n",
      "  [0.2652759  0.25840253 0.23107094 0.24021952 0.00503113]\n",
      "  [0.26639137 0.2557877  0.2299661  0.24263954 0.00521531]\n",
      "  [0.26709634 0.25387514 0.22918907 0.24445722 0.00538227]\n",
      "  [0.26751944 0.25246152 0.2286648  0.24582219 0.0055321 ]\n",
      "  [0.26775092 0.2514055  0.22833315 0.24684522 0.00566524]\n",
      "  [0.26785433 0.25060835 0.22814587 0.24760905 0.00578245]\n",
      "  [0.46695346 0.1631357  0.18344362 0.18211904 0.00434815]\n",
      "  [0.25446996 0.3950015  0.1702244  0.17588525 0.0044189 ]\n",
      "  [0.16556072 0.5489125  0.13930872 0.1423942  0.00382383]\n",
      "  [0.21631211 0.42337406 0.17625211 0.17958127 0.00448046]\n",
      "  [0.23672447 0.36717477 0.19347607 0.19789556 0.00472911]\n",
      "  [0.24942091 0.32973626 0.2050875  0.21083336 0.00492198]\n",
      "  [0.25727156 0.30465043 0.21291517 0.22007471 0.00508807]\n",
      "  [0.2620841  0.2876797  0.21818893 0.22680855 0.00523874]\n",
      "  [0.2650004  0.2760679  0.22173722 0.23181635 0.00537818]\n",
      "  [0.26672846 0.26803398 0.22411664 0.23561345 0.00550743]\n",
      "  [0.2677099  0.2624174  0.22570442 0.23854184 0.00562643]\n",
      "  [0.26822302 0.25845212 0.22675858 0.24083133 0.0057349 ]\n",
      "  [0.26844513 0.25562638 0.22745617 0.24263965 0.00583269]\n",
      "  [0.46776116 0.165461   0.18323207 0.17917262 0.00437304]\n",
      "  [0.305415   0.17045291 0.3454285  0.17431648 0.00438711]\n",
      "  [0.23269503 0.15597945 0.4553682  0.1519918  0.00396555]\n",
      "  [0.1768802  0.37072277 0.29823866 0.15015131 0.00400709]\n",
      "  [0.12976095 0.5273921  0.20998316 0.12931898 0.00354476]\n",
      "  [0.34898418 0.2971614  0.21321833 0.13715851 0.00347762]\n",
      "  [0.24464059 0.24438405 0.17923148 0.32857284 0.00317103]\n",
      "  [0.16998921 0.46612534 0.1501006  0.21083494 0.00294992]\n",
      "  [0.1229437  0.5953406  0.12433126 0.15475205 0.0026324 ]\n",
      "  [0.09556928 0.6766852  0.10554223 0.119835   0.00236826]\n",
      "  [0.13082515 0.42847025 0.12398152 0.31398085 0.00274224]\n",
      "  [0.10033057 0.6139268  0.10452016 0.1788529  0.00236959]\n",
      "  [0.32007346 0.35714546 0.13897946 0.18112874 0.00267287]\n",
      "  [0.2398466  0.29677144 0.28948212 0.17112236 0.00277752]\n",
      "  [0.20986935 0.2314874  0.19965671 0.35646015 0.00252642]\n",
      "  [0.15473594 0.45577163 0.16431744 0.22270513 0.00246979]\n",
      "  [0.11602817 0.58854717 0.133299   0.15984614 0.00227956]]]\n"
     ]
    }
   ],
   "source": [
    "print(generated_sequence_onehot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
