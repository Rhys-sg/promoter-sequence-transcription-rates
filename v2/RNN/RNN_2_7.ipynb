{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Implement loss calculation based on deviation unmasked nucleotides. Removes mask as final layer.\n",
    "\n",
    "TODO:\n",
    "1. Change Loss to compare CNN(predicted_sequence) to CNN(actual_sequence) instead of CNN(predicted_sequence) to actual_expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RNN_2_7 as parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'RNN_2_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Data/combined/LaFleur_supp.csv'\n",
    "\n",
    "df, scaler = parent.load_and_preprocess_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequence, X_expressions, y = parent.preprocess_X_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequence_train, X_sequence_test, X_expressions_train, X_expressions_test, y_train, y_test = parent.train_test_split(\n",
    "        X_sequence, X_expressions, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = parent.load_model('../Models/CNN_5_0.keras')\n",
    "lstm_model = parent.build_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Sequence 0/39013\n",
      "Weighted Mask Loss: 0.8050437569618225\n",
      "Weighted One-Hot Loss: 6.336150545394048e-05\n",
      "Weighted Expression Loss: 0.6295397281646729\n",
      "Epoch 1, Sequence 512/39013\n",
      "Weighted Mask Loss: 0.804634690284729\n",
      "Weighted One-Hot Loss: 6.337168451864272e-05\n",
      "Weighted Expression Loss: 0.7782192826271057\n",
      "Epoch 1, Sequence 1024/39013\n",
      "Weighted Mask Loss: 0.8043426275253296\n",
      "Weighted One-Hot Loss: 6.336524529615417e-05\n",
      "Weighted Expression Loss: 0.6714464426040649\n",
      "Epoch 1, Sequence 1536/39013\n",
      "Weighted Mask Loss: 0.8038469552993774\n",
      "Weighted One-Hot Loss: 6.336059595923871e-05\n",
      "Weighted Expression Loss: 0.6751692891120911\n",
      "Epoch 1, Sequence 2048/39013\n",
      "Weighted Mask Loss: 0.8035848140716553\n",
      "Weighted One-Hot Loss: 6.336257501970977e-05\n",
      "Weighted Expression Loss: 0.6761363744735718\n",
      "Epoch 1, Sequence 2560/39013\n",
      "Weighted Mask Loss: 0.8031780123710632\n",
      "Weighted One-Hot Loss: 6.336154910968617e-05\n",
      "Weighted Expression Loss: 0.6583179831504822\n",
      "Epoch 1, Sequence 3072/39013\n",
      "Weighted Mask Loss: 0.8027639985084534\n",
      "Weighted One-Hot Loss: 6.336765363812447e-05\n",
      "Weighted Expression Loss: 0.8293617367744446\n",
      "Epoch 1, Sequence 3584/39013\n",
      "Weighted Mask Loss: 0.8022937774658203\n",
      "Weighted One-Hot Loss: 6.335223588394001e-05\n",
      "Weighted Expression Loss: 0.802255392074585\n",
      "Epoch 1, Sequence 4096/39013\n",
      "Weighted Mask Loss: 0.8019851446151733\n",
      "Weighted One-Hot Loss: 6.336650403682142e-05\n",
      "Weighted Expression Loss: 0.7290297746658325\n",
      "Epoch 1, Sequence 4608/39013\n",
      "Weighted Mask Loss: 0.8014883399009705\n",
      "Weighted One-Hot Loss: 6.33509989711456e-05\n",
      "Weighted Expression Loss: 0.8489277362823486\n",
      "Epoch 1, Sequence 5120/39013\n",
      "Weighted Mask Loss: 0.8014407157897949\n",
      "Weighted One-Hot Loss: 6.337587547022849e-05\n",
      "Weighted Expression Loss: 0.8361232876777649\n",
      "Epoch 1, Sequence 5632/39013\n",
      "Weighted Mask Loss: 0.8011590838432312\n",
      "Weighted One-Hot Loss: 6.334621866699308e-05\n",
      "Weighted Expression Loss: 0.8154981732368469\n",
      "Epoch 1, Sequence 6144/39013\n",
      "Weighted Mask Loss: 0.8006505370140076\n",
      "Weighted One-Hot Loss: 6.334255886031315e-05\n",
      "Weighted Expression Loss: 0.8244568109512329\n",
      "Epoch 1, Sequence 6656/39013\n",
      "Weighted Mask Loss: 0.8003490567207336\n",
      "Weighted One-Hot Loss: 6.334226782200858e-05\n",
      "Weighted Expression Loss: 0.8237088322639465\n",
      "Epoch 1, Sequence 7168/39013\n",
      "Weighted Mask Loss: 0.8002553582191467\n",
      "Weighted One-Hot Loss: 6.334455247269943e-05\n",
      "Weighted Expression Loss: 0.8450748324394226\n",
      "Epoch 1, Sequence 7680/39013\n",
      "Weighted Mask Loss: 0.7996456623077393\n",
      "Weighted One-Hot Loss: 6.332910561468452e-05\n",
      "Weighted Expression Loss: 0.9249491691589355\n",
      "Epoch 1, Sequence 8192/39013\n",
      "Weighted Mask Loss: 0.7994635105133057\n",
      "Weighted One-Hot Loss: 6.333358032861724e-05\n",
      "Weighted Expression Loss: 0.7830194234848022\n",
      "Epoch 1, Sequence 8704/39013\n",
      "Weighted Mask Loss: 0.7990292310714722\n",
      "Weighted One-Hot Loss: 6.332823249977082e-05\n",
      "Weighted Expression Loss: 0.9397655725479126\n",
      "Epoch 1, Sequence 9216/39013\n",
      "Weighted Mask Loss: 0.7985494136810303\n",
      "Weighted One-Hot Loss: 6.333062628982589e-05\n",
      "Weighted Expression Loss: 0.92364102602005\n",
      "Epoch 1, Sequence 9728/39013\n",
      "Weighted Mask Loss: 0.7982046604156494\n",
      "Weighted One-Hot Loss: 6.331386248348281e-05\n",
      "Weighted Expression Loss: 0.9930008053779602\n",
      "Epoch 1, Sequence 10240/39013\n",
      "Weighted Mask Loss: 0.7980911731719971\n",
      "Weighted One-Hot Loss: 6.329712778097019e-05\n",
      "Weighted Expression Loss: 0.8283444046974182\n",
      "Epoch 1, Sequence 10752/39013\n",
      "Weighted Mask Loss: 0.7975726127624512\n",
      "Weighted One-Hot Loss: 6.329876487143338e-05\n",
      "Weighted Expression Loss: 0.8677204251289368\n",
      "Epoch 1, Sequence 11264/39013\n",
      "Weighted Mask Loss: 0.7974777221679688\n",
      "Weighted One-Hot Loss: 6.330548785626888e-05\n",
      "Weighted Expression Loss: 0.9248806238174438\n",
      "Epoch 1, Sequence 11776/39013\n",
      "Weighted Mask Loss: 0.7970078587532043\n",
      "Weighted One-Hot Loss: 6.329875031951815e-05\n",
      "Weighted Expression Loss: 0.8970858454704285\n",
      "Epoch 1, Sequence 12288/39013\n",
      "Weighted Mask Loss: 0.7966183423995972\n",
      "Weighted One-Hot Loss: 6.32876981399022e-05\n",
      "Weighted Expression Loss: 0.9285497665405273\n",
      "Epoch 1, Sequence 12800/39013\n",
      "Weighted Mask Loss: 0.7962464690208435\n",
      "Weighted One-Hot Loss: 6.327679147943854e-05\n",
      "Weighted Expression Loss: 0.9247881174087524\n",
      "Epoch 1, Sequence 13312/39013\n",
      "Weighted Mask Loss: 0.7959626317024231\n",
      "Weighted One-Hot Loss: 6.325675349216908e-05\n",
      "Weighted Expression Loss: 0.9141833186149597\n",
      "Epoch 1, Sequence 13824/39013\n",
      "Weighted Mask Loss: 0.7958015203475952\n",
      "Weighted One-Hot Loss: 6.327709706965834e-05\n",
      "Weighted Expression Loss: 0.8653361201286316\n",
      "Epoch 1, Sequence 14336/39013\n",
      "Weighted Mask Loss: 0.795203447341919\n",
      "Weighted One-Hot Loss: 6.325548747554421e-05\n",
      "Weighted Expression Loss: 0.7892230749130249\n",
      "Epoch 1, Sequence 14848/39013\n",
      "Weighted Mask Loss: 0.7952373027801514\n",
      "Weighted One-Hot Loss: 6.326130824163556e-05\n",
      "Weighted Expression Loss: 0.7886543273925781\n",
      "Epoch 1, Sequence 15360/39013\n",
      "Weighted Mask Loss: 0.7948617339134216\n",
      "Weighted One-Hot Loss: 6.324268906610087e-05\n",
      "Weighted Expression Loss: 0.7897207736968994\n",
      "Epoch 1, Sequence 15872/39013\n",
      "Weighted Mask Loss: 0.7943287491798401\n",
      "Weighted One-Hot Loss: 6.323640263872221e-05\n",
      "Weighted Expression Loss: 0.7732358574867249\n",
      "Epoch 1, Sequence 16384/39013\n",
      "Weighted Mask Loss: 0.7942092418670654\n",
      "Weighted One-Hot Loss: 6.323367415461689e-05\n",
      "Weighted Expression Loss: 0.7678735852241516\n",
      "Epoch 1, Sequence 16896/39013\n",
      "Weighted Mask Loss: 0.7937761545181274\n",
      "Weighted One-Hot Loss: 6.321703403955325e-05\n",
      "Weighted Expression Loss: 0.7941802740097046\n",
      "Epoch 1, Sequence 17408/39013\n",
      "Weighted Mask Loss: 0.7932105660438538\n",
      "Weighted One-Hot Loss: 6.319220119621605e-05\n",
      "Weighted Expression Loss: 0.8063940405845642\n",
      "Epoch 1, Sequence 17920/39013\n",
      "Weighted Mask Loss: 0.7930885553359985\n",
      "Weighted One-Hot Loss: 6.320737884379923e-05\n",
      "Weighted Expression Loss: 0.8703272342681885\n",
      "Epoch 1, Sequence 18432/39013\n",
      "Weighted Mask Loss: 0.792593777179718\n",
      "Weighted One-Hot Loss: 6.319976091617718e-05\n",
      "Weighted Expression Loss: 0.8050403594970703\n",
      "Epoch 1, Sequence 18944/39013\n",
      "Weighted Mask Loss: 0.7924886345863342\n",
      "Weighted One-Hot Loss: 6.318344094324857e-05\n",
      "Weighted Expression Loss: 0.8284463882446289\n",
      "Epoch 1, Sequence 19456/39013\n",
      "Weighted Mask Loss: 0.792278528213501\n",
      "Weighted One-Hot Loss: 6.317972292890772e-05\n",
      "Weighted Expression Loss: 0.8208854794502258\n",
      "Epoch 1, Sequence 19968/39013\n",
      "Weighted Mask Loss: 0.7916562557220459\n",
      "Weighted One-Hot Loss: 6.3172890804708e-05\n",
      "Weighted Expression Loss: 0.8590367436408997\n",
      "Epoch 1, Sequence 20480/39013\n",
      "Weighted Mask Loss: 0.7913500070571899\n",
      "Weighted One-Hot Loss: 6.317212682915851e-05\n",
      "Weighted Expression Loss: 0.848904013633728\n",
      "Epoch 1, Sequence 20992/39013\n",
      "Weighted Mask Loss: 0.7910938262939453\n",
      "Weighted One-Hot Loss: 6.316353392321616e-05\n",
      "Weighted Expression Loss: 0.8117603063583374\n",
      "Epoch 1, Sequence 21504/39013\n",
      "Weighted Mask Loss: 0.7907874584197998\n",
      "Weighted One-Hot Loss: 6.315875361906365e-05\n",
      "Weighted Expression Loss: 0.8499057292938232\n",
      "Epoch 1, Sequence 22016/39013\n",
      "Weighted Mask Loss: 0.7904884219169617\n",
      "Weighted One-Hot Loss: 6.313576886896044e-05\n",
      "Weighted Expression Loss: 0.7756205797195435\n",
      "Epoch 1, Sequence 22528/39013\n",
      "Weighted Mask Loss: 0.7902215123176575\n",
      "Weighted One-Hot Loss: 6.313900667009875e-05\n",
      "Weighted Expression Loss: 0.7335341572761536\n",
      "Epoch 1, Sequence 23040/39013\n",
      "Weighted Mask Loss: 0.789626955986023\n",
      "Weighted One-Hot Loss: 6.312008190434426e-05\n",
      "Weighted Expression Loss: 0.7987226247787476\n",
      "Epoch 1, Sequence 23552/39013\n",
      "Weighted Mask Loss: 0.789601743221283\n",
      "Weighted One-Hot Loss: 6.310057506198063e-05\n",
      "Weighted Expression Loss: 0.7434164881706238\n",
      "Epoch 1, Sequence 24064/39013\n",
      "Weighted Mask Loss: 0.7890161871910095\n",
      "Weighted One-Hot Loss: 6.30982976872474e-05\n",
      "Weighted Expression Loss: 0.8059046864509583\n",
      "Epoch 1, Sequence 24576/39013\n",
      "Weighted Mask Loss: 0.7889318466186523\n",
      "Weighted One-Hot Loss: 6.309776654234156e-05\n",
      "Weighted Expression Loss: 0.7211281657218933\n",
      "Epoch 1, Sequence 25088/39013\n",
      "Weighted Mask Loss: 0.7885063290596008\n",
      "Weighted One-Hot Loss: 6.308753654593602e-05\n",
      "Weighted Expression Loss: 0.797444224357605\n",
      "Epoch 1, Sequence 25600/39013\n",
      "Weighted Mask Loss: 0.7882253527641296\n",
      "Weighted One-Hot Loss: 6.307595322141424e-05\n",
      "Weighted Expression Loss: 0.8240180611610413\n",
      "Epoch 1, Sequence 26112/39013\n",
      "Weighted Mask Loss: 0.7877524495124817\n",
      "Weighted One-Hot Loss: 6.308006413746625e-05\n",
      "Weighted Expression Loss: 0.7614694237709045\n",
      "Epoch 1, Sequence 26624/39013\n",
      "Weighted Mask Loss: 0.7874782681465149\n",
      "Weighted One-Hot Loss: 6.306504656095058e-05\n",
      "Weighted Expression Loss: 0.7507546544075012\n",
      "Epoch 1, Sequence 27136/39013\n",
      "Weighted Mask Loss: 0.7873855829238892\n",
      "Weighted One-Hot Loss: 6.30329959676601e-05\n",
      "Weighted Expression Loss: 0.6967395544052124\n",
      "Epoch 1, Sequence 27648/39013\n",
      "Weighted Mask Loss: 0.786794126033783\n",
      "Weighted One-Hot Loss: 6.30278154858388e-05\n",
      "Weighted Expression Loss: 0.6939663290977478\n",
      "Epoch 1, Sequence 28160/39013\n",
      "Weighted Mask Loss: 0.7863594889640808\n",
      "Weighted One-Hot Loss: 6.302419205894694e-05\n",
      "Weighted Expression Loss: 0.7056624293327332\n",
      "Epoch 1, Sequence 28672/39013\n",
      "Weighted Mask Loss: 0.7862325310707092\n",
      "Weighted One-Hot Loss: 6.302787369349971e-05\n",
      "Weighted Expression Loss: 0.7035659551620483\n",
      "Epoch 1, Sequence 29184/39013\n",
      "Weighted Mask Loss: 0.7857552170753479\n",
      "Weighted One-Hot Loss: 6.299116648733616e-05\n",
      "Weighted Expression Loss: 0.6394296884536743\n",
      "Epoch 1, Sequence 29696/39013\n",
      "Weighted Mask Loss: 0.7855613827705383\n",
      "Weighted One-Hot Loss: 6.300643144641072e-05\n",
      "Weighted Expression Loss: 0.7260668873786926\n",
      "Epoch 1, Sequence 30208/39013\n",
      "Weighted Mask Loss: 0.7856248021125793\n",
      "Weighted One-Hot Loss: 6.300469976849854e-05\n",
      "Weighted Expression Loss: 0.7042422294616699\n",
      "Epoch 1, Sequence 30720/39013\n",
      "Weighted Mask Loss: 0.784977912902832\n",
      "Weighted One-Hot Loss: 6.297473009908572e-05\n",
      "Weighted Expression Loss: 0.6700528860092163\n",
      "Epoch 1, Sequence 31232/39013\n",
      "Weighted Mask Loss: 0.7848460078239441\n",
      "Weighted One-Hot Loss: 6.296319043030962e-05\n",
      "Weighted Expression Loss: 0.6188375353813171\n",
      "Epoch 1, Sequence 31744/39013\n",
      "Weighted Mask Loss: 0.784050464630127\n",
      "Weighted One-Hot Loss: 6.295584898907691e-05\n",
      "Weighted Expression Loss: 0.6672017574310303\n",
      "Epoch 1, Sequence 32256/39013\n",
      "Weighted Mask Loss: 0.7840096950531006\n",
      "Weighted One-Hot Loss: 6.294725608313456e-05\n",
      "Weighted Expression Loss: 0.5768164992332458\n",
      "Epoch 1, Sequence 32768/39013\n",
      "Weighted Mask Loss: 0.783502995967865\n",
      "Weighted One-Hot Loss: 6.293925252975896e-05\n",
      "Weighted Expression Loss: 0.6286637783050537\n",
      "Epoch 1, Sequence 33280/39013\n",
      "Weighted Mask Loss: 0.7834222316741943\n",
      "Weighted One-Hot Loss: 6.291296449489892e-05\n",
      "Weighted Expression Loss: 0.580677330493927\n",
      "Epoch 1, Sequence 33792/39013\n",
      "Weighted Mask Loss: 0.7831134796142578\n",
      "Weighted One-Hot Loss: 6.291231693467125e-05\n",
      "Weighted Expression Loss: 0.6636945605278015\n",
      "Epoch 1, Sequence 34304/39013\n",
      "Weighted Mask Loss: 0.7827355861663818\n",
      "Weighted One-Hot Loss: 6.289549492066726e-05\n",
      "Weighted Expression Loss: 0.6212954521179199\n",
      "Epoch 1, Sequence 34816/39013\n",
      "Weighted Mask Loss: 0.7829492092132568\n",
      "Weighted One-Hot Loss: 6.289835437200963e-05\n",
      "Weighted Expression Loss: 0.6739557385444641\n",
      "Epoch 1, Sequence 35328/39013\n",
      "Weighted Mask Loss: 0.7824018597602844\n",
      "Weighted One-Hot Loss: 6.289011798799038e-05\n",
      "Weighted Expression Loss: 0.7085082530975342\n",
      "Epoch 1, Sequence 35840/39013\n",
      "Weighted Mask Loss: 0.7817311882972717\n",
      "Weighted One-Hot Loss: 6.286903226282448e-05\n",
      "Weighted Expression Loss: 0.6866469383239746\n",
      "Epoch 1, Sequence 36352/39013\n",
      "Weighted Mask Loss: 0.7813217043876648\n",
      "Weighted One-Hot Loss: 6.285202834988013e-05\n",
      "Weighted Expression Loss: 0.6653330326080322\n",
      "Epoch 1, Sequence 36864/39013\n",
      "Weighted Mask Loss: 0.7814107537269592\n",
      "Weighted One-Hot Loss: 6.283601396717131e-05\n",
      "Weighted Expression Loss: 0.68142169713974\n",
      "Epoch 1, Sequence 37376/39013\n",
      "Weighted Mask Loss: 0.7810682058334351\n",
      "Weighted One-Hot Loss: 6.284200208028778e-05\n",
      "Weighted Expression Loss: 0.6265131831169128\n",
      "Epoch 1, Sequence 37888/39013\n",
      "Weighted Mask Loss: 0.7803537249565125\n",
      "Weighted One-Hot Loss: 6.283559196162969e-05\n",
      "Weighted Expression Loss: 0.764545738697052\n",
      "Epoch 1, Sequence 38400/39013\n",
      "Weighted Mask Loss: 0.7802921533584595\n",
      "Weighted One-Hot Loss: 6.281788955675438e-05\n",
      "Weighted Expression Loss: 0.6903777122497559\n",
      "Epoch 1, Sequence 38912/39013\n",
      "Weighted Mask Loss: 0.7796860933303833\n",
      "Weighted One-Hot Loss: 6.277801730902866e-05\n",
      "Weighted Expression Loss: 0.685213565826416\n"
     ]
    }
   ],
   "source": [
    "loss_history = parent.train_model(lstm_model, cnn_model, X_sequence_train, X_expressions_train, y_train)\n",
    "lstm_model.save(f'../Models/{name}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 12 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "lstm_model = parent.load_model(f'../Models/{name}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step\n",
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Mean Squared Error on Test Data: 0.0246\n"
     ]
    }
   ],
   "source": [
    "mse, predicted_expression = parent.evaluate_model(lstm_model, cnn_model, X_sequence_test, X_expressions_test)\n",
    "print(f'Mean Squared Error on Test Data: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "ATCG\n",
      "TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\rsore\\anaconda3\\envs\\TX_prediction\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sequence = 'TTTTCTATCTACGTACTTGACACTATTTCCT____________ATT__________ACCTTAGTTTGTACGTT'\n",
    "generated_sequence = parent.predict_with_lstm(lstm_model, sequence, 0.5, scaler, 150)\n",
    "generated_sequence_onehot = parent.predict_with_lstm(lstm_model, sequence, 0.5, scaler, 150, decode_output=False)\n",
    "\n",
    "print(sequence)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.2010004  0.21219712 0.19421923 0.1915412  0.20104209]\n",
      "  [0.19985245 0.22024241 0.19112661 0.18697846 0.20180006]\n",
      "  [0.19898014 0.22544841 0.18881726 0.18437837 0.2023758 ]\n",
      "  [0.19839866 0.22850251 0.18708704 0.18303925 0.20297246]\n",
      "  [0.19804518 0.23002148 0.18578976 0.18248987 0.20365372]\n",
      "  [0.19784304 0.23050366 0.1848198  0.18241553 0.20441797]\n",
      "  [0.19772738 0.23032705 0.18409933 0.18260844 0.20523776]\n",
      "  [0.19765176 0.2297652  0.18356973 0.18293412 0.20607917]\n",
      "  [0.19758724 0.22900805 0.1831859  0.18330806 0.20691073]\n",
      "  [0.1975185  0.22818197 0.18291292 0.18367949 0.20770718]\n",
      "  [0.19743969 0.22736673 0.18272339 0.18401992 0.2084503 ]\n",
      "  [0.19735076 0.22660916 0.18259604 0.18431553 0.20912851]\n",
      "  [0.19725469 0.2259334  0.18251431 0.18456161 0.20973599]\n",
      "  [0.19715565 0.22534868 0.18246543 0.184759   0.21027131]\n",
      "  [0.19705766 0.22485466 0.18243963 0.18491167 0.21073633]\n",
      "  [0.19696432 0.22444561 0.18242963 0.18502514 0.21113531]\n",
      "  [0.19687814 0.22411269 0.18243    0.18510543 0.2114738 ]\n",
      "  [0.19680078 0.22384597 0.18243676 0.1851585  0.21175805]\n",
      "  [0.19673303 0.22363539 0.18244708 0.18518984 0.21199462]\n",
      "  [0.19667508 0.22347157 0.18245906 0.18520437 0.21218994]\n",
      "  [0.19662651 0.22334594 0.18247136 0.18520626 0.21234995]\n",
      "  [0.19658662 0.22325104 0.18248317 0.18519899 0.21248013]\n",
      "  [0.19655454 0.22318062 0.18249398 0.18518545 0.21258537]\n",
      "  [0.19652924 0.22312933 0.18250361 0.18516791 0.21266992]\n",
      "  [0.19650969 0.22309287 0.18251191 0.18514813 0.21273744]\n",
      "  [0.19649495 0.22306767 0.18251891 0.18512739 0.21279104]\n",
      "  [0.19648412 0.22305101 0.18252473 0.18510675 0.21283337]\n",
      "  [0.19647643 0.22304067 0.18252946 0.18508686 0.2128666 ]\n",
      "  [0.19647115 0.22303489 0.18253326 0.18506815 0.2128925 ]\n",
      "  [0.19646782 0.22303241 0.18253626 0.18505095 0.21291256]\n",
      "  [0.19646586 0.22303224 0.1825386  0.18503538 0.212928  ]\n",
      "  [0.19646496 0.22303356 0.18254036 0.18502143 0.21293974]\n",
      "  [0.19646475 0.22303586 0.18254168 0.18500912 0.21294856]\n",
      "  [0.19646508 0.22303875 0.18254264 0.18499838 0.21295513]\n",
      "  [0.19646573 0.22304194 0.18254334 0.18498908 0.21295993]\n",
      "  [0.19646654 0.2230452  0.18254378 0.1849811  0.21296333]\n",
      "  [0.19646747 0.22304843 0.18254408 0.18497428 0.2129657 ]\n",
      "  [0.19646843 0.22305155 0.18254428 0.18496853 0.21296725]\n",
      "  [0.19646934 0.22305444 0.18254432 0.18496367 0.21296819]\n",
      "  [0.19647022 0.22305714 0.18254435 0.18495962 0.21296865]\n",
      "  [0.19647104 0.22305961 0.18254432 0.18495627 0.21296877]\n",
      "  [0.19647177 0.22306184 0.18254423 0.18495347 0.21296866]\n",
      "  [0.19647244 0.22306387 0.18254416 0.18495117 0.21296838]\n",
      "  [0.19647302 0.22306567 0.18254405 0.18494928 0.21296798]\n",
      "  [0.19647354 0.2230673  0.18254396 0.18494774 0.2129675 ]\n",
      "  [0.19647397 0.22306871 0.18254383 0.18494649 0.21296696]\n",
      "  [0.19647434 0.22307    0.18254375 0.1849455  0.21296644]\n",
      "  [0.19647469 0.22307111 0.18254364 0.18494466 0.21296589]\n",
      "  [0.19647498 0.22307211 0.18254356 0.18494402 0.21296537]\n",
      "  [0.19647521 0.22307299 0.18254346 0.1849435  0.21296488]\n",
      "  [0.19647542 0.22307375 0.18254337 0.18494307 0.21296442]\n",
      "  [0.19647557 0.2230744  0.1825433  0.1849427  0.21296395]\n",
      "  [0.19647571 0.223075   0.18254322 0.18494245 0.21296355]\n",
      "  [0.19647585 0.22307552 0.18254316 0.18494226 0.21296318]\n",
      "  [0.19647597 0.22307599 0.18254311 0.18494211 0.21296285]\n",
      "  [0.19647604 0.22307636 0.18254305 0.18494196 0.21296252]\n",
      "  [0.19647612 0.22307672 0.18254301 0.18494189 0.21296227]\n",
      "  [0.1964762  0.22307703 0.182543   0.18494181 0.21296203]\n",
      "  [0.19647625 0.22307727 0.18254294 0.18494174 0.2129618 ]\n",
      "  [0.19647628 0.22307749 0.1825429  0.1849417  0.2129616 ]\n",
      "  [0.19647633 0.22307768 0.18254288 0.18494165 0.21296142]\n",
      "  [0.19647636 0.22307785 0.18254285 0.18494162 0.21296127]\n",
      "  [0.1964764  0.22307801 0.18254285 0.18494162 0.21296115]\n",
      "  [0.19647641 0.22307812 0.1825428  0.1849416  0.212961  ]\n",
      "  [0.19647646 0.22307825 0.1825428  0.1849416  0.21296093]\n",
      "  [0.1964765  0.22307836 0.1825428  0.18494159 0.21296085]\n",
      "  [0.19647647 0.2230784  0.18254277 0.18494156 0.21296075]\n",
      "  [0.1964765  0.22307847 0.18254277 0.18494157 0.21296069]\n",
      "  [0.19647652 0.22307855 0.18254277 0.18494159 0.21296065]\n",
      "  [0.19647653 0.2230786  0.18254277 0.18494157 0.21296059]\n",
      "  [0.19647652 0.22307861 0.18254274 0.18494156 0.21296051]\n",
      "  [0.19647653 0.22307865 0.18254274 0.18494156 0.2129605 ]\n",
      "  [0.19647653 0.22307868 0.18254273 0.18494155 0.21296045]\n",
      "  [0.19647656 0.22307873 0.18254273 0.18494157 0.21296044]\n",
      "  [0.19647655 0.22307873 0.18254271 0.18494156 0.2129604 ]\n",
      "  [0.19647656 0.22307876 0.18254271 0.18494156 0.21296039]\n",
      "  [0.19647656 0.22307877 0.18254271 0.18494156 0.21296038]\n",
      "  [0.19647656 0.22307877 0.1825427  0.18494156 0.21296033]\n",
      "  [0.19647656 0.22307877 0.18254268 0.18494155 0.21296032]\n",
      "  [0.19647658 0.2230788  0.18254271 0.18494156 0.21296032]\n",
      "  [0.19647658 0.2230788  0.1825427  0.18494156 0.2129603 ]\n",
      "  [0.1964766  0.22307883 0.18254273 0.18494157 0.21296032]\n",
      "  [0.1964766  0.22307883 0.18254271 0.18494156 0.2129603 ]\n",
      "  [0.1964766  0.22307883 0.18254271 0.18494156 0.21296029]\n",
      "  [0.19647658 0.22307883 0.18254271 0.18494156 0.21296029]\n",
      "  [0.1964766  0.22307885 0.18254273 0.18494157 0.21296029]\n",
      "  [0.1964766  0.22307883 0.18254271 0.18494156 0.21296027]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494156 0.21296027]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296027]\n",
      "  [0.19647661 0.22307885 0.18254271 0.18494157 0.21296027]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.19647661 0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.19647661 0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.19647661 0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494156 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.19647661 0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.19647661 0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.19647661 0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494156 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.19647661 0.22307888 0.18254273 0.18494159 0.21296027]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494156 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494156 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.18254271 0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494156 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.19647661 0.22307888 0.18254273 0.18494159 0.21296027]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494156 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.1964766  0.22307885 0.1825427  0.18494157 0.21296026]\n",
      "  [0.19311197 0.2187679  0.18595393 0.19437778 0.20778833]\n",
      "  [0.19932228 0.21842152 0.19193102 0.19287553 0.19744958]\n",
      "  [0.19686493 0.21463582 0.195479   0.20194565 0.19107462]\n",
      "  [0.19886512 0.21888125 0.19708444 0.20475635 0.1804129 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(generated_sequence_onehot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
